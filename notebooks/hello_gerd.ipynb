{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello GERD\n",
    "\n",
    "## Getting started with GERD\n",
    "\n",
    "In this notebook, we will start with basic features. If you run this notebook on binder, all required dependencies should already be installed. If you run this notebook locally, we'd recommend to create a virtual environment, install poetry and let peotry handle the installation of dependencies:\n",
    "\n",
    "```shell\n",
    "!pip install poetry\n",
    "!poetry install --extras cpu --extras gguf\n",
    "```\n",
    "\n",
    "Before we start, we need to let Python know where to find `gerd`.\n",
    "This is not necessary if you have used poetry as mentioned above since poetry will install `gerd` 'editable' and thus `import gerd` can be used without further configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")  # the project's root directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load a `GenerationConfig` with `load_get_config`. `load_get_config` accepts a path to a config file or a config name as a string. The later will search for a config file with this name in the `config/` project directory. The code below will load `config/hello.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gerd.config import load_gen_config\n",
    "\n",
    "config = load_gen_config(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `config` contains LLM model information which `gerd` (or more precisely, the `transformers` or `llama-cpp-python` library) will use to load a large language model. If this model has been downloaded before, it will be reused. Otherwise, it will be downloaded first. Note that since binder notebooks do not cache LLMs, you need to download them whenever you restart a notebook.\n",
    "\n",
    "### Optional: Use GGUF model\n",
    "\n",
    "The used LLM for this example is already quite small.\n",
    "However, in a VM or container it still may take some time to process in- and output.\n",
    "You could opt for a quantized model by uncomment the code below BEFORE you continue.\n",
    "If you have already executed code below, just restart the Jupyter kernel and start over.\n",
    "\n",
    "A quantized model provided reduces complexity and file size, increases processing speed but reduces accuracy.\n",
    "For larger models, *some* quantization might not reduce output quality significantly but notably reduce file size and processing speed.\n",
    "However, for smaller models with reduced parameter count, event slight quantization might make a model unusable.\n",
    "Also note that `gerd` will use [llama.cpp](https://github.com/ggerganov/llama.cpp) instead of [transformers](https://github.com/huggingface/transformers) when you want to use quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment if you want to use a GGUF\n",
    "# # (quantized and thus smaller but less accurate) LLM.\n",
    "\n",
    "# config.model.name = \"Qwen/Qwen2.5-0.5B-Instruct-GGUF\"\n",
    "# config.model.file = \"qwen2.5-0.5b-instruct-q5_k_m.gguf\"  # 1 GB -> 522 MB\n",
    "# # config.model.file = \"qwen2.5-0.5b-instruct-q2_k.gguf\"  # 415 MB; smallest version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will pass the configuration to a `gerd` LLM service. We use the `ChatService` which can be used for chat-like interaction but also to generate texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gerd.gen.chat_service import ChatService\n",
    "\n",
    "chat = ChatService(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the chat service and submit a message to the LLM.\n",
    "We do not pass messages directly but provide a dictionary with parameters to `submit_user_message`.\n",
    "How and which parameters will be processed depends on the current prompt settings.\n",
    "We can have a look at the current prompt by inspecting `config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model.prompt_config.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial prompt contains one variable argument `{word}` which is replaced with 'teleportation' by the call below and then passed to the LLM.\n",
    "Note that executing this line of code may take a while on binder notebooks since the LLM is pretty demanding, especially when processed on the CPU.\n",
    "\n",
    "If you have no more patience and haven't opted for the `GGUF` version of the model above, you might want to restart the Jupyter kernel and try again with the quantized LLM, either the recommended version, ending with `q5_k_m` or the even smaller version with the `q2_k` suffix.\n",
    "On Binder, even a GGUF-quantized model may take some time to return though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat.submit_user_message({\"word\": \"teleportation\"})\n",
    "res.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think the answer feels somewhat weird, you are correct.\n",
    "`config` also provides an initial prompt with e.g. a so called 'system prompt' that adds some context and rules to the LLM interaction.\n",
    "A `gerd` system prompt consists of a list of roles and `PromptConfig` objects.\n",
    "This example provides one tuple with the role 'system' and a prompt as seen below.\n",
    "Of course, this can be adjusted, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model.prompt_setup[0][1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change prompt configurations without the need to reload an LLM.\n",
    "The code below will replace the **user** prompt with an empty string.\n",
    "`message` will be passed without modifications to the LLM.\n",
    "However, the system prompt or prompt setup is still active and will influence the answer accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gerd.models.model import PromptConfig\n",
    "\n",
    "chat.set_prompt_config(PromptConfig.model_validate({\"text\": \"{message}\"}))\n",
    "res = chat.submit_user_message({\"message\": \"Hello! What is one plus one?\"})\n",
    "res.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the previous conversation will be passed as well.\n",
    "Chat history can be reset with `chat.reset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
