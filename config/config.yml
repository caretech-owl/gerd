device: "cpu" # options: cpu, mps, gpu
logging:
  level: "debug" # options: debug, info, warn, error, none
gen:
  model:
  model: &gen_model
    name: "TheBloke/em_german_leo_mistral-GGUF"
    file: "em_german_leo_mistral.Q5_K_M.gguf"
    type: "llama"
    temperature: 0
    top_k: 100
    top_p: 0.95
    max_new_tokens: 512
    repetition_penalty: 1.1
    context_length: 2048
    prompt:
      path: "prompts/gen.txt"
  features:
    continuation:
      model:
        <<: *gen_model
        prompt:
          path: "prompts/gen_con.txt"
qa:
  model: &qa_model
    name: "TheBloke/leo-hessianai-7B-chat-GGUF"
    file: "leo-hessianai-7b-chat.Q5_K_M.gguf"
    type: "llama"
    temperature: 0.01
    max_new_tokens: 128
    context_length: 512
    prompt:
      path: "prompts/qa.txt"
  features:
    fact_checking:
      enabled: false
      model:
        <<: *qa_model
        prompt:
          path: "prompts/qa_fact_checking.txt"
    return_source: true
  embedding:
    chunk_size: 256
    chunk_overlap: 25
    vector_count: 2
    model:
      name: "sentence-transformers/all-MiniLM-L6-v2"
    db_path: "vectorstore/db_faiss"
