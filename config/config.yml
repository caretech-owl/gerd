device: "cpu" # options: cpu, mps, gpu
logging:
  level: "debug" # options: debug, info, warn, error, none
gen:
  model:
    name: "TheBloke/leo-hessianai-7B-chat-GGUF"
    file: "leo-hessianai-7b-chat.Q5_K_M.gguf"
    type: "llama"
    temperature: 0.01
    max_new_tokens: 128
    prompt:
      path: "prompts/gen.txt"
qa:
  model: &qa_model
    name: "TheBloke/leo-hessianai-7B-chat-GGUF"
    file: "leo-hessianai-7b-chat.Q5_K_M.gguf"
    type: "llama"
    temperature: 0.01
    max_new_tokens: 128
    context_length: 512
    prompt:
      path: "prompts/qa.txt"
  features:
    fact_checking:
      enabled: false
      model:
        <<: *qa_model
        prompt:
          path: "prompts/qa_fact_checking.txt"
    return_source: true
  embedding:
    chunk_size: 256
    chunk_overlap: 25
    vector_count: 2
    model:
      name: "sentence-transformers/all-MiniLM-L6-v2"
      # file:
    # db_path: "vectorstore/db_faiss"
