model:
  name: "meta-llama/Llama-3.2-1B"
  temperature: 0.01
  top_k: 100
  top_p: 0.95
  max_new_tokens: 512
  repetition_penalty: 1.1
  context_length: 2048
input_glob: "tests/data/grascco/raw/*.txt"
output_dir: "models/lora_generate"
override_existing: true
pad_token_id: 0
padding_side: right
modules:
  default: false
  q: true
  v: true
