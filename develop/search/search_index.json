{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Generating and evaluating relevant documentation (GERD)","text":"<p>GERD is developed as an experimental library to investigate how large language models (LLMs) can be used to generate and analyze (sets of) documents.</p> <p>This project was initially forked from Llama-2-Open-Source-LLM-CPU-Inference  by Kenneth Leung.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>If you just want to it try out, you can clone the project and install dependencies with <code>pip</code>:</p> <pre><code>git clone https://github.com/caretech-owl/gerd.git\ncd gerd\npip install -e \".[full]\"\npython examples/hello.py\n</code></pre> <p>For more information on development look at DEV.md. If you want to try this out in your browser, head over to binder \ud83d\udc49 .  Note that running LLMs on the CPU (and especially on limited virtual machines like binder) takes some time. If you are in a hurry you might be better off by cloning the repo and running the examples or notebooks locally.</p>"},{"location":"#question-and-answer-example","title":"Question and Answer example","text":"<p>Follow quickstart but execute <code>gradio</code> with the <code>qa_frontend</code> instead of the example file. When the server is done loading, open <code>http://127.0.0.1:7860</code> in your browser.</p> <pre><code>gradio gerd/frontends/qa_frontend.py\n# Some Llama.cpp outut\n# ...\n# * Running on local URL:  http://127.0.0.1:7860\n</code></pre> <p>Click the 'Click to Upload' button and search for a GRASCCO document named <code>Caja.txt</code> which is located in the <code>tests/data/grascoo</code> folder and upload it into the vector store. Next, you can query information from the document. For instance <code>Wie hei\u00dft der Patient?</code> (What is the patient called?).</p> <p></p>"},{"location":"#prompt-chaining","title":"Prompt Chaining","text":"<p>Prompt chaining is a prompt engineering approach to increase the 'reflection' of a large language model onto its given answer. Check examples/chaining.py for an illustration. Also, have a look at how chaining is configured and used with GERD. You can find the config at config/gen_chaining.yml</p> <pre><code>python examples/chaining.py\n# ...\n====== Resolved prompt =====\n\nsystem: You are a helpful assistant. Please answer the following question in a truthful and brief manner.\nuser: What type of mammal lays the biggest eggs?\n\n# ...\nResult: Based on the given information, the largest egg-laying mammal is the blue whale, which can lay up to 100 million eggs per year. However, the other assertions provided do not align with this information.\n</code></pre> <p>As you see, the answer does not make much sense with the default model which is rather small. Give it a try with meta-llama/Llama-3.2-3B. To use this model, you need to login with the huggingface cli and accept the Meta Community License Agreement.</p>"},{"location":"#used-tools","title":"Used Tools","text":"<ul> <li>LangChain: Framework for developing applications powered by language models</li> <li>C Transformers: Python bindings for the Transformer models implemented in C/C++ using GGML library</li> <li>FAISS: Open-source library for efficient similarity search and clustering of dense vectors.</li> <li>Sentence-Transformers (all-MiniLM-L6-v2): Open-source pre-trained transformer model for embedding text to a 384-dimensional dense vector space for tasks like</li> <li>Poetry: Tool for dependency management and Python packaging</li> </ul>"},{"location":"#files-and-content","title":"Files and Content","text":"<ul> <li><code>/assets</code>: Images relevant to the project</li> <li><code>/config</code>: Configuration files for LLM applications</li> <li><code>/examples</code>: Examples that demonstrate the different usage scenarios</li> <li><code>/gerd</code>: Code related to <code>GERD</code></li> <li><code>/images</code>: Images for the documentation</li> <li><code>/models</code>: Binary file of GGML quantized LLM model (i.e., Llama-2-7B-Chat)</li> <li><code>/prompts</code>: Plain text prompt files</li> <li><code>/templates</code>: Prompt files as jinja2 templates </li> <li><code>/tests</code>: Unit tests for <code>GERD</code></li> <li><code>/vectorstore</code>: FAISS vector store for documents</li> <li><code>pyproject.toml</code>: TOML file to specify which versions of the dependencies used (Poetry)</li> </ul>"},{"location":"#references","title":"References","text":"<ul> <li>https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference</li> <li>https://pubmed.ncbi.nlm.nih.gov/36073490</li> <li>https://huggingface.co</li> </ul>"}]}