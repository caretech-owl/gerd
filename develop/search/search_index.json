{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Generating and evaluating relevant documentation (GERD)","text":"<p>GERD is developed as an experimental library to investigate how large language models (LLMs) can be used to generate and analyze (sets of) documents.</p> <p>This project was initially forked from Llama-2-Open-Source-LLM-CPU-Inference  by Kenneth Leung.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>If you just want to it try out, you can clone the project and install dependencies with <code>pip</code>:</p> <pre><code>git clone https://github.com/caretech-owl/gerd.git\ncd gerd\npip install -e \".[full]\"\npython examples/hello.py\n</code></pre> <p>If you want to try this out in your browser, head over to binder \ud83d\udc49 .  Note that running LLMs on the CPU (and especially on limited virtual machines like binder) takes some time.</p>"},{"location":"#question-and-answer-example","title":"Question and Answer Example","text":"<p>Follow quickstart but execute <code>gradio</code> with the <code>qa_frontend</code> instead of the example file. When the server is done loading, open <code>http://127.0.0.1:7860</code> in your browser.</p> <pre><code>gradio gerd/frontends/qa_frontend.py\n# Some Llama.cpp outut\n# ...\n# * Running on local URL:  http://127.0.0.1:7860\n</code></pre> <p>Click the 'Click to Upload' button and search for a GRASCCO document named <code>Caja.txt</code> which is located in the <code>tests/data/grascoo</code> folder and upload it into the vector store. Next, you can query information from the document. For instance <code>Wie hei\u00dft der Patient?</code> (What is the patient called?).</p> <p></p>"},{"location":"#prompt-chaining","title":"Prompt Chaining","text":"<p>Prompt chaining is a prompt engineering approach to increase the 'reflection' of a large language model onto its given answer. Check examples/chaining.py for an illustration. Also, have a look at how chaining is configured and used with GERD. You can find the config at config/gen_chaining.yml</p> <pre><code>python examples/chaining.py\n# ...\n====== Resolved prompt =====\n\nsystem: You are a helpful assistant. Please answer the following question in a truthful and brief manner.\nuser: What type of mammal lays the biggest eggs?\n\n# ...\nResult: Based on the given information, the largest egg-laying mammal is the blue whale, which can lay up to 100 million eggs per year. However, the other assertions provided do not align with this information.\n</code></pre> <p>As you see, the answer does not make much sense with the default model which is rather small. Give it a try with meta-llama/Llama-3.2-3B. To use this model, you need to login with the huggingface cli and accept the Meta Community License Agreement.</p>"},{"location":"#full-documentation","title":"Full Documentation","text":"<p>A more detailled documentation can be found \ud83d\udc49 .</p>"},{"location":"#used-tools","title":"Used Tools","text":"<ul> <li>LangChain: Framework for developing applications powered by language models</li> <li>C Transformers: Python bindings for the Transformer models implemented in C/C++ using GGML library</li> <li>FAISS: Open-source library for efficient similarity search and clustering of dense vectors.</li> <li>Sentence-Transformers (all-MiniLM-L6-v2): Open-source pre-trained transformer model for embedding text to a 384-dimensional dense vector space for tasks like</li> <li>Poetry: Tool for dependency management and Python packaging</li> </ul>"},{"location":"#files-and-content","title":"Files and Content","text":"<ul> <li><code>/assets</code>: Images relevant to the project</li> <li><code>/config</code>: Configuration files for LLM applications</li> <li><code>/examples</code>: Examples that demonstrate the different usage scenarios</li> <li><code>/gerd</code>: Code related to <code>GERD</code></li> <li><code>/images</code>: Images for the documentation</li> <li><code>/models</code>: Binary file of GGML quantized LLM model (i.e., Llama-2-7B-Chat)</li> <li><code>/prompts</code>: Plain text prompt files</li> <li><code>/templates</code>: Prompt files as jinja2 templates </li> <li><code>/tests</code>: Unit tests for <code>GERD</code></li> <li><code>/vectorstore</code>: FAISS vector store for documents</li> <li><code>pyproject.toml</code>: TOML file to specify which versions of the dependencies used (Poetry)</li> </ul>"},{"location":"#references","title":"References","text":"<ul> <li>https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference</li> <li>https://pubmed.ncbi.nlm.nih.gov/36073490</li> <li>https://huggingface.co</li> </ul>"},{"location":"concepts/","title":"Concepts","text":"<p>GERD is primarly a tool for prototyping workflows for working with Large Language Models. It is meant to act as 'glue' between different tools and services and should ease the access to these tools.</p> <p>In general, there should be only be two components involved in a GERD workflow: A configuration and a service. The configuration can be assembled from different sources and should be able to be used in different services. The foundation of such a configration is a YAML file. GERD provides a set of those which can be found in the <code>config</code> directory. Configurations in this directly can be accessed by name as shown in the following example. A simple configutation file may look like this:</p> <pre><code>model:\n  name: \"Qwen/Qwen2.5-0.5B-Instruct\"\n  temperature: 0.1\n  top_p: 0.90\n  max_new_tokens: 256\n  repetition_penalty: 1.1\n  prompt_setup:\n    - [system, text: \"You are a helpful assistant. Start your answers with 'Sure thing, buddy!'.\"]\n  prompt_config:\n    text: \"Please say the word '{word}' three times.\"\n</code></pre> <p>And can be used with a <code>ChatService</code> via <code>load_gen_config</code> shown in the following example:</p> <pre><code>import logging\n\nfrom gerd.config import load_gen_config\nfrom gerd.gen.chat_service import ChatService\nfrom gerd.models.model import PromptConfig\n\nlogging.basicConfig(level=logging.WARNING)\nlogging.getLogger(\"gerd\").setLevel(logging.DEBUG)\n\nlogging.info(\n    \"Loading chat service...\"\n    \" When this is the first time you run this script, it will download the model.\"\n    \" This may take a few minutes.\"\n)\n\nchat = ChatService(load_gen_config(\"hello\"))\nres = chat.submit_user_message({\"word\": \"teleportation\"})\nlogging.info(res)\n\nchat.set_prompt_config(PromptConfig.model_validate({\"text\": \"{message}\"}))\nres = chat.submit_user_message({\"message\": \"Hello! What is one plus one?\"})\nlogging.info(res)\n</code></pre>"},{"location":"develop/","title":"Development Guide","text":""},{"location":"develop/#basics","title":"Basics","text":"<p>To get started on development you need to install uv. You can use <code>pip</code>, <code>pipx</code> or <code>conda</code> to do so:</p> <pre><code>pip install uv\n</code></pre> <p>Next install the package and all dependencies with <code>uv sync</code>.</p> <pre><code># cd &lt;gerd_project_root&gt;\nuv sync\n</code></pre> <p>After that, it should be possible to run scripts without further issues:</p> <pre><code>uv run examples/hello.py\n</code></pre> <p>To add a new runtime dependency, just run <code>uv add</code>:</p> <pre><code>uv add langchain\n</code></pre> <p>To add a new development dependency, run <code>uv add</code> with the <code>--dev</code> flag:</p> <pre><code>uv add mypy --dev\n</code></pre>"},{"location":"develop/#pre-commit-hooks-recommended","title":"Pre-commit hooks (recommended)","text":"<p>Pre-commit hooks are used to check linting and run tests before commit changes to prevent faulty commits. Thus, it is recommended to use these hooks! Hooks should not include long running actions (such as tests) since committing should be fast. To install pre-commit hooks, execute this once:</p> <pre><code>uv run pre-commit install\n</code></pre>"},{"location":"develop/#further-tools","title":"Further tools","text":""},{"location":"develop/#poe-task-runner","title":"Poe Task Runner","text":"<p>See pyproject.toml for task runner configurations. You can run most of the tools mentioned above with a (shorter) call to <code>poe</code>.</p> <pre><code>uv run poe lint  # do some linting (with mypy)\n</code></pre>"},{"location":"develop/#pytest","title":"PyTest","text":"<p>Test case are run via pytest. Tests can be found in the tests folder. Tests will not be run via pre-commit since they might be too complex to be done before commits. To run the standard set of tests use the <code>poe</code> task <code>test</code>:</p> <pre><code>uv run poe test\n</code></pre> <p>More excessive testing can be trigger with <code>test_manual</code> which will NOT mock calls to the used models:</p> <pre><code>uv run poe test_manual\n</code></pre>"},{"location":"develop/#ruff","title":"Ruff","text":"<p>Ruff is used for linting and code formatting. Ruff follows <code>black</code> styling ref. Ruff will be run automatically before commits when pre-commit hooks are installed. To run <code>ruff</code> manually, use uv:</p> <pre><code>uv run ruff check gerd\n</code></pre> <p>There is a VSCode extension that handles formatting and linting.</p>"},{"location":"develop/#mypy","title":"MyPy","text":"<p>MyPy does static type checking. It will not be run automatically. To run MyPy manually use uv with the folder to be checked:</p> <pre><code>uv run mypy gerd\n</code></pre>"},{"location":"develop/#implemented-guis","title":"Implemented GUIs","text":""},{"location":"develop/#run-frontend","title":"Run Frontend","text":"<p>Either run Generate Frontend:</p> <pre><code>uv run poe gen_dev\n</code></pre> <p>or QA Frontend:</p> <pre><code>uv run poe qa_dev\n</code></pre> <p>or the GERD Router:</p> <pre><code># add _dev for the gradio live reload version\n# omit in 'prod'\nuv run router[_dev]\n</code></pre> <p>The backend is chosen via <code>config.yaml</code>.</p>"},{"location":"develop/#cicd-and-distribution","title":"CI/CD and Distribution","text":""},{"location":"develop/#github-actions","title":"GitHub Actions","text":"<p>GitHub Actions can be found under .github/workflows. There is currently one main CI workflow called <code>pythin-ci.yml</code>:</p> <pre><code># This workflow will install Python dependencies, run tests and lint with a variety of Python versions\n# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions\n\nname: Linting/Testing Python\n\non:\n  push:\n    branches: [main, dev-gha]\n  pull_request:\n    branches: [main]\n\njobs:\n  linting:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Install uv\n        run: pipx install uv\n      - name: Set up Python 3.12\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n      - name: Install development dependencies with extras\n        run: uv sync\n      - name: Run ruff linting via pre-commit on all files and verbose\n        uses: pre-commit/action@v3.0.0\n        with:\n          extra_args: ruff --all-files -v\n      - name: Run linting\n        run: uv run poe lint\n  testing:\n    needs: linting\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest]\n        python-version: [\"3.12\", \"3.10\"]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/checkout@4\n      - name: Cache HuggingFace models and data\n        uses: actions/cache@v4\n        with:\n          path: ~/.cache/huggingface\n          key: ${{ runner.os }}-hf\n      - name: Install uv\n        run: pipx install uv\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Install development dependencies with extras\n        run: uv sync\n      - name: Run tests\n        run: uv run poe test\n    # - name: Upload coverage to Codecov\n    #   uses: codecov/codecov-action@v3\n    #   with:\n    #     env_vars: OS,PYTHON\n    #     fail_ci_if_error: true\n    #     files: ./coverage.xml\n    #     name: codecov-umbrella\n    #     verbose: true\n</code></pre> <p>In its current config it will only be executed when a PR for <code>main</code> is created or when a special <code>dev-gha</code> branch is created. It will also trigger actions when commits are pushed to <code>main</code> directly but this should be avoided.</p>"},{"location":"develop/#github-issue-templates","title":"GitHub Issue Templates","text":"<p>This project uses GitHub issue templates. Currently, there are three templates available.</p>"},{"location":"develop/#bug-report","title":"Bug Report","text":"<pre><code>name: Bug\ndescription: File a bug report\ntitle: \"[Bug]: \"\nlabels: [\"bug\"]\n# assignees:\n#   - aleneum\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        Thanks for taking the time to fill out this bug report!\n  - type: textarea\n    id: expected-behavior\n    attributes:\n      label: Expected Behavior\n      description: What should happen\n    validations:\n      required: true\n  - type: textarea\n    id: actual-behavior\n    attributes:\n      label: Actual Behavior\n      description: What happened instead\n    validations:\n      required: true\n  - type: textarea\n    id: steps-reproduce\n    attributes:\n      label: Steps to Reproduce the Problem\n      value: |\n        1.\n        2.\n        3.\n\n    validations:\n      required: true\n  - type: input\n    id: affected-version\n    attributes:\n      label: Affected version\n      description: Version number or commit hash\n      placeholder: \"0.1.0\"\n    validations:\n      required: true\n  - type: dropdown\n    id: affected-components\n    attributes:\n      label: Affected components\n      description: Which components are affected\n      multiple: true\n      options:\n        - Gen Service\n        - Qa Service\n        - Frontend\n        - Backend\n    validations:\n      required: false\n  - type: dropdown\n    id: affected-plattforms\n    attributes:\n      label: Affected plattforms\n      description: Which plattforms are affected\n      multiple: true\n      options:\n        - Windows\n        - Linux\n        - MacOS\n    validations:\n      required: false\n  - type: textarea\n    id: further-information\n    attributes:\n      label: Further contextual information and suggestions\n      description: More detailed descriptions or possible solutions\n    validations:\n      required: false\n</code></pre>"},{"location":"develop/#feature-request","title":"Feature Request","text":"<pre><code>name: Feature Request\ndescription: Suggest an idea for this project\ntitle: \"[FR]: \"\nlabels: [\"enhancement\"]\n# assignees:\n#   - aleneum\nbody:\n  - type: markdown\n    attributes:\n      value: Do you have an idea for a feature that would make this project more helpful or easier to use?\n  - type: textarea\n    id: problem-description\n    attributes:\n      label: Problem description\n      description: Is your feature request related to a problem? Please describe.\n    validations:\n      required: true\n  - type: textarea\n    id: solution\n    attributes:\n      label: Solution\n      description: A clear and concise description of what you want to happen.\n    validations:\n      required: true\n  - type: textarea\n    id: additional-context\n    attributes:\n      label: Additional context\n      description: Add any other context or screenshots about the feature request here.\n    validations:\n      required: false\n  - type: dropdown\n    id: affected-components\n    attributes:\n      label: Affected components\n      description: Which components are affected\n      multiple: true\n      options:\n        - Gen Service\n        - Qa Service\n        - Frontend\n        - Backend\n    validations:\n      required: false\n</code></pre>"},{"location":"develop/#use-case","title":"Use Case","text":"<pre><code>name: Use Case\ndescription: A description of a user-centered system requirement\ntitle: \"[UC]: \"\nlabels: [\"use case\"]\n# assignees:\n#   - aleneum\nbody:\n  - type: textarea\n    id: summary\n    attributes:\n      label: Summary\n      description: A concise description of the use case\n    validations:\n      required: true\n  - type: textarea\n    id: rationale\n    attributes:\n      label: Rationale\n      description: The motivation and value added by the described use case\n    validations:\n      required: true\n  - type: dropdown\n    id: level\n    attributes:\n      label: Level\n      multiple: false\n      options:\n        - user goal\n        - subfunction\n  - type: input\n    id: actors\n    attributes:\n      label: Actors\n      description: Someone or something with behavior, e.g. a person (identified by role), a computer system, or an organization\n    validations:\n      required: true\n  - type: textarea\n    id: preconditions\n    attributes:\n      label: Preconditions\n      description: What needs to be true on start\n      placeholder: |\n        - Use Cases: #1, #2\n        - User is authenticated\n    validations:\n      required: true\n  - type: textarea\n    id: postconditions\n    attributes:\n      label: Postconditions\n      description: What must be true on successful completion\n      placeholder: \"The form \"\n    validations:\n      required: true\n  - type: textarea\n    id: basic-flow\n    attributes:\n      label: Basic Flow\n      description: A typical, unconditional happy path\n      value: |\n        1.\n        2.\n        3.\n\n    validations:\n      required: true\n  - type: textarea\n    id: alternative-paths\n    attributes:\n      label: Alternative Paths\n      description: Alternate scenarios of success or failure\n      placeholder: |\n        2.1\n        2.2\n        3.1\n\n    validations:\n      required: false\n  - type: textarea\n    id: visualisation\n    attributes:\n      label: Visualisation\n      description: A mermaid diagram or image depiciting the action flow\n      value: |\n        ```mermaid\n        flowchart LR;\n          1--&gt;2\n          2--&gt;3\n          3--&gt;4\n          3--&gt;3.1\n          3.1--&gt;3.2\n          3.2--&gt;2\n          4--&gt;4.1\n          4.1--&gt;4\n        ```\n    validations:\n      required: false\n  - type: textarea\n    id: related-issues\n    attributes:\n      label: Other related issues, use cases, features\n      description: What must be true on successful completion\n      placeholder: \"#1 #2 #3\"\n    validations:\n      required: false\n</code></pre>"},{"location":"reference/gerd/","title":"gerd","text":""},{"location":"reference/gerd/#gerd","title":"gerd","text":"<p>GERD</p> <p>Modules:</p> Name Description <code>config</code> <p>Configuration for the application.</p> <code>features</code> <code>frontends</code> <code>gen</code> <code>qa</code> <code>training</code>"},{"location":"reference/gerd/backend/","title":"gerd.backend","text":""},{"location":"reference/gerd/backend/#gerd.backend","title":"gerd.backend","text":""},{"location":"reference/gerd/backends/","title":"gerd.backends","text":""},{"location":"reference/gerd/backends/#gerd.backends","title":"gerd.backends","text":""},{"location":"reference/gerd/backends/bridge/","title":"gerd.backends.bridge","text":""},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge","title":"gerd.backends.bridge","text":""},{"location":"reference/gerd/backends/loader/","title":"gerd.backends.loader","text":""},{"location":"reference/gerd/backends/loader/#gerd.backends.loader","title":"gerd.backends.loader","text":""},{"location":"reference/gerd/backends/rag/","title":"gerd.backends.rag","text":""},{"location":"reference/gerd/backends/rag/#gerd.backends.rag","title":"gerd.backends.rag","text":""},{"location":"reference/gerd/backends/rest_client/","title":"gerd.backends.rest_client","text":""},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client","title":"gerd.backends.rest_client","text":""},{"location":"reference/gerd/backends/rest_server/","title":"gerd.backends.rest_server","text":""},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server","title":"gerd.backends.rest_server","text":""},{"location":"reference/gerd/config/","title":"gerd.config","text":""},{"location":"reference/gerd/config/#gerd.config","title":"gerd.config","text":"<p>Configuration for the application.</p> <p>Classes:</p> Name Description <code>EnvVariables</code> <p>Environment variables.</p> <code>Settings</code> <p>Settings for the application.</p> <code>YamlConfig</code> <p>YAML configuration source.</p> <p>Functions:</p> Name Description <code>load_gen_config</code> <p>Load the LLM model configuration.</p> <code>load_qa_config</code> <p>Load the LLM model configuration.</p> <p>Attributes:</p> Name Type Description <code>CONFIG</code> <p>The global configuration object.</p>"},{"location":"reference/gerd/config/#gerd.config.CONFIG","title":"CONFIG  <code>module-attribute</code>","text":"<pre><code>CONFIG = Settings()\n</code></pre> <p>The global configuration object.</p>"},{"location":"reference/gerd/config/#gerd.config.EnvVariables","title":"EnvVariables","text":"<p>               Bases: <code>BaseModel</code></p> <p>Environment variables.</p>"},{"location":"reference/gerd/config/#gerd.config.Settings","title":"Settings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for the application.</p>"},{"location":"reference/gerd/config/#gerd.config.YamlConfig","title":"YamlConfig","text":"<p>               Bases: <code>PydanticBaseSettingsSource</code></p> <p>YAML configuration source.</p>"},{"location":"reference/gerd/config/#gerd.config.load_gen_config","title":"load_gen_config","text":"<pre><code>load_gen_config(config: str = 'gen_default') -&gt; GenerationConfig\n</code></pre> <p>Load the LLM model configuration.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the configuration.</p> <code>'gen_default'</code> <p>Returns:     The model configuration.</p> Source code in <code>gerd/config.py</code> <pre><code>def load_gen_config(config: str = \"gen_default\") -&gt; GenerationConfig:\n    \"\"\"Load the LLM model configuration.\n\n    Parameters:\n        config: The name of the configuration.\n    Returns:\n        The model configuration.\n    \"\"\"\n    config_path = (\n        Path(config)\n        if config.endswith(\"yml\")\n        else Path(PROJECT_DIR, \"config\", f\"{config}.yml\")\n    )\n    with config_path.open(\"r\", encoding=\"utf-8\") as f:\n        conf = GenerationConfig.model_validate(safe_load(f))\n    if CONFIG.env and CONFIG.env.api_token and conf.model.endpoint:\n        conf.model.endpoint.key = CONFIG.env.api_token\n    return conf\n</code></pre>"},{"location":"reference/gerd/config/#gerd.config.load_gen_config(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/config/#gerd.config.load_qa_config","title":"load_qa_config","text":"<pre><code>load_qa_config(config: str = 'qa_default') -&gt; QAConfig\n</code></pre> <p>Load the LLM model configuration.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the configuration.</p> <code>'qa_default'</code> <p>Returns:     The model configuration.</p> Source code in <code>gerd/config.py</code> <pre><code>def load_qa_config(config: str = \"qa_default\") -&gt; QAConfig:\n    \"\"\"Load the LLM model configuration.\n\n    Parameters:\n        config: The name of the configuration.\n    Returns:\n        The model configuration.\n    \"\"\"\n    config_path = (\n        Path(config)\n        if config.endswith(\"yml\")\n        else Path(PROJECT_DIR, \"config\", f\"{config}.yml\")\n    )\n    with config_path.open(\"r\", encoding=\"utf-8\") as f:\n        conf = QAConfig.model_validate(safe_load(f))\n\n    if CONFIG.env and CONFIG.env.api_token and conf.model.endpoint:\n        conf.model.endpoint.key = CONFIG.env.api_token\n    return conf\n</code></pre>"},{"location":"reference/gerd/config/#gerd.config.load_qa_config(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/features/","title":"gerd.features","text":""},{"location":"reference/gerd/features/#gerd.features","title":"gerd.features","text":"<p>Modules:</p> Name Description <code>prompt_chaining</code> <p>The prompt chaining extension</p>"},{"location":"reference/gerd/features/prompt_chaining/","title":"gerd.features.prompt_chaining","text":""},{"location":"reference/gerd/features/prompt_chaining/#gerd.features.prompt_chaining","title":"gerd.features.prompt_chaining","text":"<p>The prompt chaining extension</p> <p>Classes:</p> Name Description <code>PromptChainingConfig</code> <p>Configuration for</p>"},{"location":"reference/gerd/features/prompt_chaining/#gerd.features.prompt_chaining.PromptChainingConfig","title":"PromptChainingConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for</p>"},{"location":"reference/gerd/frontends/","title":"gerd.frontends","text":""},{"location":"reference/gerd/frontends/#gerd.frontends","title":"gerd.frontends","text":"<p>Modules:</p> Name Description <code>qa_frontend</code> <code>training</code>"},{"location":"reference/gerd/frontends/gen_frontend/","title":"gerd.frontends.gen_frontend","text":""},{"location":"reference/gerd/frontends/gen_frontend/#gerd.frontends.gen_frontend","title":"gerd.frontends.gen_frontend","text":""},{"location":"reference/gerd/frontends/generate/","title":"gerd.frontends.generate","text":""},{"location":"reference/gerd/frontends/generate/#gerd.frontends.generate","title":"gerd.frontends.generate","text":""},{"location":"reference/gerd/frontends/instruct/","title":"gerd.frontends.instruct","text":""},{"location":"reference/gerd/frontends/instruct/#gerd.frontends.instruct","title":"gerd.frontends.instruct","text":""},{"location":"reference/gerd/frontends/qa_frontend/","title":"gerd.frontends.qa_frontend","text":""},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend","title":"gerd.frontends.qa_frontend","text":"<p>Functions:</p> Name Description <code>files_changed</code> <p>Upload a document to vectorstore</p> <code>get_qa_mode</code> <p>Get QAMode from string</p> <code>handle_developer_mode_checkbox_change</code> <p>Enable/disable developermode</p> <code>handle_type_radio_selection_change</code> <p>Enable/disable gui elements depend on which mode is selected</p> <code>query</code> <p>Start the selected QA Mode</p> <code>set_prompt</code> <p>Update the prompt of the selected QA Mode</p>"},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.files_changed","title":"files_changed","text":"<pre><code>files_changed(file_paths: Optional[list[str]]) -&gt; None\n</code></pre> <p>Upload a document to vectorstore</p> Source code in <code>gerd/frontends/qa_frontend.py</code> <pre><code>def files_changed(file_paths: Optional[list[str]]) -&gt; None:\n    \"\"\"\n    Upload a document to vectorstore\n    \"\"\"\n    file_paths = file_paths or []\n    progress = gr.Progress()\n    new_set = set(file_paths)\n    new_files = new_set - store_set\n    delete_files = store_set - new_set\n    for new_file in new_files:\n        store_set.add(new_file)\n        with pathlib.Path(new_file).open(\"rb\") as file:\n            data = QAFileUpload(\n                data=file.read(),\n                name=pathlib.Path(new_file).name,\n            )\n        res = TRANSPORTER.add_file(data)\n        if res.status != 200:\n            _LOGGER.warning(\n                \"Data upload failed with error code: %d\\nReason: %s\",\n                res.status,\n                res.error_msg,\n            )\n            msg = (\n                f\"Datei konnte nicht hochgeladen werden: {res.error_msg}\"\n                \"(Error Code {res.status})\"\n            )\n            raise gr.Error(msg)\n    for delete_file in delete_files:\n        store_set.remove(delete_file)\n        res = TRANSPORTER.remove_file(pathlib.Path(delete_file).name)\n    progress(100, desc=\"Fertig!\")\n</code></pre>"},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.get_qa_mode","title":"get_qa_mode","text":"<pre><code>get_qa_mode(search_type: str) -&gt; QAModesEnum\n</code></pre> <p>Get QAMode from string</p> Source code in <code>gerd/frontends/qa_frontend.py</code> <pre><code>def get_qa_mode(search_type: str) -&gt; QAModesEnum:\n    \"\"\"\n    Get QAMode from string\n    \"\"\"\n    if search_type in qa_modes_dict:\n        return qa_modes_dict[search_type]\n    else:\n        return QAModesEnum.NONE\n</code></pre>"},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.handle_developer_mode_checkbox_change","title":"handle_developer_mode_checkbox_change","text":"<pre><code>handle_developer_mode_checkbox_change(check: bool) -&gt; List[Any]\n</code></pre> <p>Enable/disable developermode</p> Source code in <code>gerd/frontends/qa_frontend.py</code> <pre><code>def handle_developer_mode_checkbox_change(check: bool) -&gt; List[Any]:\n    \"\"\"\n    Enable/disable developermode\n    \"\"\"\n    return [\n        gr.update(visible=check),\n        gr.update(visible=check),\n        gr.update(visible=check),\n        gr.update(visible=check),\n        gr.update(\n            choices=(\n                [\"LLM\", \"Analyze\", \"Analyze mult.\", \"VectorDB\"]\n                if check\n                else [\"LLM\", \"Analyze mult.\"]\n            )\n        ),\n    ]\n</code></pre>"},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.handle_type_radio_selection_change","title":"handle_type_radio_selection_change","text":"<pre><code>handle_type_radio_selection_change(search_type: str) -&gt; List[Any]\n</code></pre> <p>Enable/disable gui elements depend on which mode is selected</p> Source code in <code>gerd/frontends/qa_frontend.py</code> <pre><code>def handle_type_radio_selection_change(search_type: str) -&gt; List[Any]:\n    \"\"\"\n    Enable/disable gui elements depend on which mode is selected\n    \"\"\"\n    if search_type == \"LLM\":\n        return [\n            gr.update(interactive=True, placeholder=\"Wie hei\u00dft der Patient?\"),\n            gr.update(value=TRANSPORTER.get_qa_prompt(get_qa_mode(search_type)).text),\n            gr.update(interactive=False),\n            gr.update(interactive=False),\n        ]\n    elif search_type == \"VectorDB\":\n        return [\n            gr.update(interactive=True, placeholder=\"Wie hei\u00dft der Patient?\"),\n            gr.update(value=TRANSPORTER.get_qa_prompt(get_qa_mode(search_type)).text),\n            gr.update(interactive=True),\n            gr.update(interactive=True),\n        ]\n\n    return [\n        gr.update(interactive=False, placeholder=\"\"),\n        gr.update(value=TRANSPORTER.get_qa_prompt(get_qa_mode(search_type)).text),\n        gr.update(interactive=False),\n        gr.update(interactive=False),\n    ]\n</code></pre>"},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.query","title":"query","text":"<pre><code>query(question: str, search_type: str, k_source: int, search_strategy: str) -&gt; str\n</code></pre> <p>Start the selected QA Mode</p> Source code in <code>gerd/frontends/qa_frontend.py</code> <pre><code>def query(question: str, search_type: str, k_source: int, search_strategy: str) -&gt; str:\n    \"\"\"\n    Start the selected QA Mode\n    \"\"\"\n    q = QAQuestion(\n        question=question, search_strategy=search_strategy, max_sources=k_source\n    )\n    # start search mode\n    if search_type == \"LLM\":\n        qa_res = TRANSPORTER.qa_query(q)\n        if qa_res.status != 200:\n            msg = (\n                f\"Query was unsuccessful: {qa_res.error_msg}\"\n                f\" (Error Code {qa_res.status})\"\n            )\n            raise gr.Error(msg)\n        return qa_res.answer\n    # start analyze mode\n    elif search_type == \"Analyze\":\n        qa_analyze_res = TRANSPORTER.analyze_query()\n        if qa_analyze_res.status != 200:\n            msg = (\n                f\"Query was unsuccessful: {qa_analyze_res.error_msg}\"\n                f\" (Error Code {qa_analyze_res.status})\"\n            )\n            raise gr.Error(msg)\n        # remove unwanted fields from answer\n        qa_res_dic = {\n            key: value\n            for key, value in vars(qa_analyze_res).items()\n            if value is not None\n            and value != \"\"\n            and key not in qa_analyze_res.__class__.__dict__\n            and key != \"sources\"\n            and key != \"status\"\n            and key != \"response\"\n            and key != \"prompt\"\n        }\n        qa_res_str = \", \".join(f\"{key}={value}\" for key, value in qa_res_dic.items())\n        return qa_res_str\n    # start analyze mult prompts mode\n    elif search_type == \"Analyze mult.\":\n        qa_analyze_mult_res = TRANSPORTER.analyze_mult_prompts_query()\n        if qa_analyze_mult_res.status != 200:\n            msg = (\n                f\"Query was unsuccessful: {qa_analyze_mult_res.error_msg}\"\n                f\" (Error Code {qa_analyze_mult_res.status})\"\n            )\n            raise gr.Error(msg)\n        # remove unwanted fields from answer\n        qa_res_dic = {\n            key: value\n            for key, value in vars(qa_analyze_mult_res).items()\n            if value is not None\n            and value != \"\"\n            and key not in qa_analyze_mult_res.__class__.__dict__\n            and key != \"sources\"\n            and key != \"status\"\n            and key != \"response\"\n            and key != \"prompt\"\n        }\n        qa_res_str = \", \".join(f\"{key}={value}\" for key, value in qa_res_dic.items())\n        return qa_res_str\n    # start db search mode\n    db_res = TRANSPORTER.db_query(q)\n    if not db_res:\n        msg = f\"Database query returned empty!\"\n        raise gr.Error(msg)\n    output = \"\"\n    for doc in db_res:\n        output += f\"{doc.content}\\n\"\n        output += f\"({doc.name} / {doc.page})\\n----------\\n\\n\"\n    return output\n</code></pre>"},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.set_prompt","title":"set_prompt","text":"<pre><code>set_prompt(prompt: str, search_type: str, progress: Optional[gr.Progress] = None) -&gt; None\n</code></pre> <p>Update the prompt of the selected QA Mode</p> Source code in <code>gerd/frontends/qa_frontend.py</code> <pre><code>def set_prompt(\n    prompt: str, search_type: str, progress: Optional[gr.Progress] = None\n) -&gt; None:\n    \"\"\"\n    Update the prompt of the selected QA Mode\n    \"\"\"\n    if progress is None:\n        progress = gr.Progress()\n    progress(0, \"Aktualisiere Prompt...\")\n\n    answer = TRANSPORTER.set_qa_prompt(\n        PromptConfig(text=prompt), get_qa_mode(search_type)\n    )\n    if answer.error_msg:\n        if answer.status != 200:\n            msg = (\n                f\"Prompt konnte nicht aktualisiert werden: {answer.error_msg}\"\n                f\" (Error Code {answer.status})\"\n            )\n            raise gr.Error(msg)\n        else:\n            msg = f\"{answer.error_msg}\"\n            gr.Warning(msg)\n    else:\n        gr.Info(\"Prompt wurde aktualisiert!\")\n    progress(100, \"Fertig!\")\n</code></pre>"},{"location":"reference/gerd/frontends/router/","title":"gerd.frontends.router","text":""},{"location":"reference/gerd/frontends/router/#gerd.frontends.router","title":"gerd.frontends.router","text":""},{"location":"reference/gerd/frontends/training/","title":"gerd.frontends.training","text":""},{"location":"reference/gerd/frontends/training/#gerd.frontends.training","title":"gerd.frontends.training","text":"<p>Functions:</p> Name Description <code>validate_files</code> <p>Upload a document to vectorstore</p>"},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.validate_files","title":"validate_files","text":"<pre><code>validate_files(file_paths: list[str] | None, mode: str) -&gt; tuple[list[str], dict[str, bool]]\n</code></pre> <p>Upload a document to vectorstore</p> Source code in <code>gerd/frontends/training.py</code> <pre><code>def validate_files(\n    file_paths: list[str] | None, mode: str\n) -&gt; tuple[list[str], dict[str, bool]]:\n    \"\"\"\n    Upload a document to vectorstore\n    \"\"\"\n    file_paths = file_paths or []\n    if mode.lower() == \"instructions\":\n\n        def validate(path: Path) -&gt; bool:\n            if path.suffix != \".json\":\n                gr.Warning(f\"{path.name} has an invalid file type\")\n                return False\n            try:\n                with path.open(\"r\") as f:\n                    InstructTrainingData.model_validate(json.load(f))\n            except Exception as e:\n                gr.Warning(f\"{path.name} could not be validated: {e}\")\n                return False\n            return True\n\n    elif mode.lower() == \"unstructured\":\n\n        def validate(path: Path) -&gt; bool:\n            if path.suffix != \".txt\":\n                gr.Warning(f\"{path.name} has an invalid file type\")\n                return False\n            return True\n\n    else:\n        gr.Error(\"Invalid training mode\")\n        return (file_paths, gr.update(interactive=False))\n\n    res = [f_path for f_path in file_paths if validate(Path(f_path).resolve())]\n    return (res, gr.update(interactive=len(res) &gt; 0))\n</code></pre>"},{"location":"reference/gerd/gen/","title":"gerd.gen","text":""},{"location":"reference/gerd/gen/#gerd.gen","title":"gerd.gen","text":"<p>Modules:</p> Name Description <code>chat_service</code> <code>generation_service</code>"},{"location":"reference/gerd/gen/chat_service/","title":"gerd.gen.chat_service","text":""},{"location":"reference/gerd/gen/chat_service/#gerd.gen.chat_service","title":"gerd.gen.chat_service","text":"<p>Classes:</p> Name Description <code>ChatService</code>"},{"location":"reference/gerd/gen/chat_service/#gerd.gen.chat_service.ChatService","title":"ChatService","text":"<pre><code>ChatService(config: GenerationConfig, parameters: Dict[str, str] | None = None)\n</code></pre> <p>Methods:</p> Name Description <code>add_message</code> <p>Add a message to the chat history.</p> <code>get_prompt_config</code> <p>Get the prompt configuration.</p> <code>reset</code> <p>Reset the chat history.</p> <code>set_prompt_config</code> <p>Set the prompt configuration.</p> Source code in <code>gerd/gen/chat_service.py</code> <pre><code>def __init__(\n    self, config: GenerationConfig, parameters: Dict[str, str] | None = None\n) -&gt; None:\n    self.config = config\n    self._model = gerd_loader.load_model_from_config(self.config.model)\n    self.messages: list[ChatMessage] = []\n    self.reset(parameters)\n</code></pre>"},{"location":"reference/gerd/gen/chat_service/#gerd.gen.chat_service.ChatService.add_message","title":"add_message","text":"<pre><code>add_message(parameters: Dict[str, str] | None = None, role: Literal['user', 'system', 'assistant'] = 'user', prompt_config: Optional[PromptConfig] = None) -&gt; None\n</code></pre> <p>Add a message to the chat history.</p> Source code in <code>gerd/gen/chat_service.py</code> <pre><code>def add_message(\n    self,\n    parameters: Dict[str, str] | None = None,\n    role: Literal[\"user\", \"system\", \"assistant\"] = \"user\",\n    prompt_config: Optional[PromptConfig] = None,\n) -&gt; None:\n    \"\"\"Add a message to the chat history.\"\"\"\n    parameters = parameters or {}\n    user_prompt: PromptConfig = prompt_config or self.config.model.prompt_config\n    self.messages.append(\n        {\n            \"role\": role,\n            \"content\": user_prompt.format(parameters),\n        }\n    )\n</code></pre>"},{"location":"reference/gerd/gen/chat_service/#gerd.gen.chat_service.ChatService.get_prompt_config","title":"get_prompt_config","text":"<pre><code>get_prompt_config() -&gt; PromptConfig\n</code></pre> <p>Get the prompt configuration.</p> Source code in <code>gerd/gen/chat_service.py</code> <pre><code>def get_prompt_config(self) -&gt; PromptConfig:\n    \"\"\"Get the prompt configuration.\"\"\"\n    return self.config.model.prompt_config\n</code></pre>"},{"location":"reference/gerd/gen/chat_service/#gerd.gen.chat_service.ChatService.reset","title":"reset","text":"<pre><code>reset(parameters: Dict[str, str] | None = None) -&gt; None\n</code></pre> <p>Reset the chat history.</p> Source code in <code>gerd/gen/chat_service.py</code> <pre><code>def reset(self, parameters: Dict[str, str] | None = None) -&gt; None:\n    \"\"\"Reset the chat history.\"\"\"\n    parameters = parameters or {}\n    self.messages.clear()\n    for role, message in self.config.model.prompt_setup:\n        self.messages.append(\n            {\n                \"role\": role,\n                \"content\": message.format(parameters),\n            }\n        )\n</code></pre>"},{"location":"reference/gerd/gen/chat_service/#gerd.gen.chat_service.ChatService.set_prompt_config","title":"set_prompt_config","text":"<pre><code>set_prompt_config(config: PromptConfig) -&gt; PromptConfig\n</code></pre> <p>Set the prompt configuration.</p> Source code in <code>gerd/gen/chat_service.py</code> <pre><code>def set_prompt_config(\n    self,\n    config: PromptConfig,\n) -&gt; PromptConfig:\n    \"\"\"Set the prompt configuration.\"\"\"\n    self.config.model.prompt_config = config\n    return self.config.model.prompt_config\n</code></pre>"},{"location":"reference/gerd/gen/generation_service/","title":"gerd.gen.generation_service","text":""},{"location":"reference/gerd/gen/generation_service/#gerd.gen.generation_service","title":"gerd.gen.generation_service","text":"<p>Classes:</p> Name Description <code>GenerationService</code>"},{"location":"reference/gerd/gen/generation_service/#gerd.gen.generation_service.GenerationService","title":"GenerationService","text":"<pre><code>GenerationService(config: GenerationConfig)\n</code></pre> <p>Methods:</p> Name Description <code>get_prompt_config</code> <p>Get the prompt configuration.</p> <code>set_prompt_config</code> <p>Set the prompt configuration.</p> Source code in <code>gerd/gen/generation_service.py</code> <pre><code>def __init__(self, config: GenerationConfig) -&gt; None:\n    self.config = config\n    self._model = gerd_loader.load_model_from_config(self.config.model)\n</code></pre>"},{"location":"reference/gerd/gen/generation_service/#gerd.gen.generation_service.GenerationService.get_prompt_config","title":"get_prompt_config","text":"<pre><code>get_prompt_config() -&gt; PromptConfig\n</code></pre> <p>Get the prompt configuration.</p> Source code in <code>gerd/gen/generation_service.py</code> <pre><code>def get_prompt_config(self) -&gt; PromptConfig:\n    \"\"\"Get the prompt configuration.\"\"\"\n    return self.config.model.prompt_config\n</code></pre>"},{"location":"reference/gerd/gen/generation_service/#gerd.gen.generation_service.GenerationService.set_prompt_config","title":"set_prompt_config","text":"<pre><code>set_prompt_config(config: PromptConfig) -&gt; PromptConfig\n</code></pre> <p>Set the prompt configuration.</p> Source code in <code>gerd/gen/generation_service.py</code> <pre><code>def set_prompt_config(\n    self,\n    config: PromptConfig,\n) -&gt; PromptConfig:\n    \"\"\"Set the prompt configuration.\"\"\"\n    self.config.model.prompt_config = config\n    return self.config.model.prompt_config\n</code></pre>"},{"location":"reference/gerd/models/","title":"gerd.models","text":""},{"location":"reference/gerd/models/#gerd.models","title":"gerd.models","text":""},{"location":"reference/gerd/models/gen/","title":"gerd.models.gen","text":""},{"location":"reference/gerd/models/gen/#gerd.models.gen","title":"gerd.models.gen","text":""},{"location":"reference/gerd/models/label/","title":"gerd.models.label","text":""},{"location":"reference/gerd/models/label/#gerd.models.label","title":"gerd.models.label","text":""},{"location":"reference/gerd/models/logging/","title":"gerd.models.logging","text":""},{"location":"reference/gerd/models/logging/#gerd.models.logging","title":"gerd.models.logging","text":""},{"location":"reference/gerd/models/model/","title":"gerd.models.model","text":""},{"location":"reference/gerd/models/model/#gerd.models.model","title":"gerd.models.model","text":""},{"location":"reference/gerd/models/qa/","title":"gerd.models.qa","text":""},{"location":"reference/gerd/models/qa/#gerd.models.qa","title":"gerd.models.qa","text":""},{"location":"reference/gerd/models/server/","title":"gerd.models.server","text":""},{"location":"reference/gerd/models/server/#gerd.models.server","title":"gerd.models.server","text":""},{"location":"reference/gerd/qa/","title":"gerd.qa","text":""},{"location":"reference/gerd/qa/#gerd.qa","title":"gerd.qa","text":"<p>Modules:</p> Name Description <code>qa_service</code>"},{"location":"reference/gerd/qa/qa_service/","title":"gerd.qa.qa_service","text":""},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service","title":"gerd.qa.qa_service","text":"<p>Classes:</p> Name Description <code>QAService</code>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService","title":"QAService","text":"<pre><code>QAService(config: QAConfig)\n</code></pre> <p>Init the llm and set default values</p> <p>Methods:</p> Name Description <code>analyze_mult_prompts_query</code> <p>Read a set of data from doc.</p> <code>analyze_query</code> <p>Read a set of data from doc</p> <code>db_embedding</code> <p>Generate embeddings</p> <code>db_query</code> <p>To pass a question directly to the vectorstore</p> <code>get_prompt_config</code> <p>Returns the prompt for the mode</p> <code>query</code> <p>Pass a single question to the llm and returns the answer</p> <code>remove_file</code> <p>Remove a document from the vectorstore</p> <code>set_prompt_config</code> <p>Set the prompt for the mode</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def __init__(self, config: QAConfig) -&gt; None:\n    \"\"\"\n    Init the llm and set default values\n    \"\"\"\n    self.config = config\n    self._llm = gerd_loader.load_model_from_config(config.model)\n    self._vectorstore: Optional[FAISS] = None\n    self._database: Optional[Rag] = None\n    if (\n        config.embedding.db_path\n        and Path(config.embedding.db_path, \"index.faiss\").exists()\n    ):\n        _LOGGER.info(\n            \"Load existing vector store from '%s'.\", config.embedding.db_path\n        )\n        self._vectorstore = load_faiss(\n            Path(config.embedding.db_path, \"index.faiss\"),\n            config.embedding.model.name,\n            config.device,\n        )\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.analyze_mult_prompts_query","title":"analyze_mult_prompts_query","text":"<pre><code>analyze_mult_prompts_query() -&gt; QAAnalyzeAnswer\n</code></pre> <p>Read a set of data from doc. Loads the data via multiple prompts Data:     patient_name     patient_date_of_birth     attending_doctors     recording_date     release_date</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def analyze_mult_prompts_query(self) -&gt; QAAnalyzeAnswer:\n    \"\"\"\n    Read a set of data from doc.\n    Loads the data via multiple prompts\n    Data:\n        patient_name\n        patient_date_of_birth\n        attending_doctors\n        recording_date\n        release_date\n    \"\"\"\n    if not self._vectorstore:\n        msg = \"No vector store initialized! Upload documents first.\"\n        _LOGGER.error(msg)\n        return QAAnalyzeAnswer(status=404, error_msg=msg)\n\n    config = self.config.features.analyze_mult_prompts\n\n    # check if prompt contains needed fields\n    qa_analyze_mult_prompts = config.model.prompt_config\n    if (\n        \"context\" not in qa_analyze_mult_prompts.parameters\n        or \"question\" not in qa_analyze_mult_prompts.parameters\n    ):\n        msg = \"Prompt does not include '{context}' or '{question}' variable.\"\n        _LOGGER.error(msg)\n        return QAAnalyzeAnswer(status=404, error_msg=msg)\n\n    # questions to search model and vectorstore\n    questions_model_dict: Dict[str, str] = {\n        \"Wie hei\u00dft der Patient?\": \"wir berichten \u00fcber unseren Patient oder Btr. oder Patient, wh, geboren oder  Patient, * 0.00.0000,\",  # noqa: E501\n        \"Wann hat der Patient Geburstag?\": \"wir berichten \u00fcber unseren Patient oder Btr. oder Patient, wh, geboren oder  Patient, * 0.00.0000,\",  # noqa: E501\n        \"Wie hei\u00dft der Arzt?\": \"Mit freundlichen kollegialen Gr\u00fc\u00dfen, Prof, Dr\",\n        \"Wann wurde der Patient bei uns aufgenommen?\": (\n            \"wir berichten \u00fcber unseren Patient oder Btr. oder Patient, wh, geboren\"\n        ),\n        \"Wann wurde der Patient bei uns entlassen?\": (\n            \"wir berichten \u00fcber unseren Patient oder Btr. oder Patient, wh, geboren\"\n        ),\n    }\n    fields: Dict[str, str] = {\n        \"Wie hei\u00dft der Patient?\": \"patient_name\",\n        \"Wann hat der Patient Geburstag?\": \"patient_date_of_birth\",\n        \"Wie hei\u00dft der Arzt?\": \"attending_doctors\",\n        \"Wann wurde der Patient bei uns aufgenommen?\": \"recording_date\",\n        \"Wann wurde der Patient bei uns entlassen?\": \"release_date\",\n    }\n    questions_dict: Dict[str, List[DocumentSource]] = {}\n    answer_dict: Dict[str, Any] = {}\n    responses: str = \"\"\n\n    # load context from vectorstore for each question\n    for question_m, question_v in questions_model_dict.items():\n        questions_dict, formatted_prompt = self._create_analyze_mult_prompt(\n            question_m, question_v, qa_analyze_mult_prompts.text\n        )\n\n        # query the model for each question\n        response = self._llm.generate(formatted_prompt)\n\n        # format the response\n        if response is not None:\n            response = self._clean_response(response)\n\n            # if enabled, collect response\n            if self.config.features.return_source:\n                responses = responses + \"; \" + question_m + \": \" + response\n        # format the response\n        answer_dict[fields[question_m]] = self._format_response_analyze_mult_prompt(\n            response, fields[question_m]\n        )\n\n        _LOGGER.info(\n            \"\\n===== Modelresult ====\\n\\n%s\\n\\n====================\", response\n        )\n\n    answer = QAAnalyzeAnswer(**answer_dict)\n\n    # if enabled, pass source data to answer\n    if self.config.features.return_source:\n        answer.response = responses\n        for question in questions_dict:\n            answer.sources = answer.sources + questions_dict[question]\n\n    _LOGGER.warning(\"\\n==== Answer ====\\n\\n%s\\n===============\", answer)\n    return answer\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.analyze_query","title":"analyze_query","text":"<pre><code>analyze_query() -&gt; QAAnalyzeAnswer\n</code></pre> <p>Read a set of data from doc Loads the data via single prompt Data:     patient_name     patient_date_of_birth     attending_doctors     recording_date     release_date</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def analyze_query(self) -&gt; QAAnalyzeAnswer:\n    \"\"\"\n    Read a set of data from doc\n    Loads the data via single prompt\n    Data:\n        patient_name\n        patient_date_of_birth\n        attending_doctors\n        recording_date\n        release_date\n    \"\"\"\n    if not self._vectorstore:\n        msg = \"No vector store initialized! Upload documents first.\"\n        _LOGGER.error(msg)\n        return QAAnalyzeAnswer(status=404, error_msg=msg)\n\n    config = self.config.features.analyze\n\n    # questions to search model and vectorstore\n    questions_model_dict: Dict[str, str] = {\n        \"Wie hei\u00dft der Patient?\": \"wir berichten \u00fcber unseren Patient oder Btr. oder Patient, wh, geboren oder  Patient, * 0.00.0000,\",  # noqa E501\n        \"Wann hat der Patient Geburstag?\": \"wir berichten \u00fcber unseren Patient oder Btr. oder Patient, wh, geboren oder  Patient, * 0.00.0000,\",  # noqa E501\n        \"Wie hei\u00dft der Arzt?\": \"Mit freundlichen kollegialen Gr\u00fc\u00dfen, Prof, Dr\",\n        \"Wann wurde der Patient bei uns aufgenommen?\": (\n            \"wir berichten \u00fcber unseren Patient oder Btr. oder Patient, wh, geboren\"\n        ),\n        \"Wann wurde der Patient bei uns entlassen?\": (\n            \"wir berichten \u00fcber unseren Patient oder Btr. oder Patient, wh, geboren\"\n        ),\n    }\n\n    # map model questions to jsonfields\n    fields: Dict[str, str] = {\n        \"Wie hei\u00dft der Patient?\": \"patient_name\",\n        \"Wann hat der Patient Geburstag?\": \"patient_date_of_birth\",\n        \"Wie hei\u00dft der Arzt?\": \"attending_doctors\",\n        \"Wann wurde der Patient bei uns aufgenommen?\": \"recording_date\",\n        \"Wann wurde der Patient bei uns entlassen?\": \"release_date\",\n    }\n\n    questions_dict: Dict[str, List[DocumentSource]] = {}\n    parameters: Dict[str, str] = {}\n    question_counter: int = 0\n\n    qa_analyze_prompt = config.model.prompt_config\n    # check if prompt contains needed fields\n    for i in range(0, len(questions_model_dict)):\n        if (\n            \"context\" + str(i) not in qa_analyze_prompt.parameters\n            or \"question\" + str(i) not in qa_analyze_prompt.parameters\n        ):\n            msg = (\n                \"Prompt does not include '{context\"\n                + str(i)\n                + \"}' or '{question\"\n                + str(i)\n                + \"}' variable.\"\n            )\n            _LOGGER.error(msg)\n            return QAAnalyzeAnswer(status=404, error_msg=msg)\n\n    # load context from vectorstore for each question\n    for question_m, question_v in questions_model_dict.items():\n        questions_dict[question_m] = list(\n            self.db_query(QAQuestion(question=question_v, max_sources=3))\n        )\n\n        parameters[\"context\" + str(question_counter)] = \" \".join(\n            doc.content for doc in questions_dict[question_m]\n        )\n        parameters[\"question\" + str(question_counter)] = question_m\n        parameters[\"field\" + str(question_counter)] = fields[question_m]\n\n        question_counter = question_counter + 1\n\n    formatted_prompt = qa_analyze_prompt.text.format(**parameters)\n\n    # query the model\n    response = self._llm.generate(formatted_prompt)\n\n    if response is not None:\n        response = self._clean_response(response)\n\n    _LOGGER.info(\"\\n===== Modelresult ====\\n\\n%s\\n\\n====================\", response)\n\n    # convert json to QAAnalyzerAnswerclass\n    answer = self._format_response_analyze(response)\n\n    # if enabled, pass source data to answer\n    if self.config.features.return_source:\n        answer.response = response\n        answer.prompt = formatted_prompt\n        for question in questions_dict:\n            answer.sources = answer.sources + questions_dict[question]\n        _LOGGER.info(\n            \"\\n===== Sources ====\\n\\n%s\\n\\n====================\", answer.sources\n        )\n\n    _LOGGER.warning(\"\\n==== Answer ====\\n\\n%s\\n===============\", answer)\n    return answer\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.db_embedding","title":"db_embedding","text":"<pre><code>db_embedding(question: QAQuestion) -&gt; List[float]\n</code></pre> <p>Generate embeddings</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def db_embedding(self, question: QAQuestion) -&gt; List[float]:\n    \"\"\"\n    Generate embeddings\n    \"\"\"\n    if self._vectorstore is None or self._vectorstore.embeddings is None:\n        return []\n    return self._vectorstore.embeddings.embed_documents([question.question])[0]\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.db_query","title":"db_query","text":"<pre><code>db_query(question: QAQuestion) -&gt; List[DocumentSource]\n</code></pre> <p>To pass a question directly to the vectorstore</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def db_query(self, question: QAQuestion) -&gt; List[DocumentSource]:\n    \"\"\"\n    To pass a question directly to the vectorstore\n    \"\"\"\n    if not self._vectorstore:\n        return []\n    return [\n        DocumentSource(\n            query=question.question,\n            content=doc.page_content,\n            name=doc.metadata.get(\"source\", \"unknown\"),\n            page=doc.metadata.get(\"page\", 1),\n        )\n        for doc in self._vectorstore.search(\n            question.question,\n            search_type=question.search_strategy,\n            k=question.max_sources,\n        )\n    ]\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.get_prompt_config","title":"get_prompt_config","text":"<pre><code>get_prompt_config(qa_mode: QAModesEnum) -&gt; PromptConfig\n</code></pre> <p>Returns the prompt for the mode</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def get_prompt_config(self, qa_mode: QAModesEnum) -&gt; PromptConfig:\n    \"\"\"\n    Returns the prompt for the mode\n    \"\"\"\n    if qa_mode == QAModesEnum.SEARCH:\n        return self.config.model.prompt_config\n    elif qa_mode == QAModesEnum.ANALYZE:\n        return self.config.features.analyze.model.prompt_config\n    elif qa_mode == QAModesEnum.ANALYZE_MULT_PROMPTS:\n        return self.config.features.analyze_mult_prompts.model.prompt_config\n    return PromptConfig()\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.query","title":"query","text":"<pre><code>query(question: QAQuestion) -&gt; QAAnswer\n</code></pre> <p>Pass a single question to the llm and returns the answer</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def query(self, question: QAQuestion) -&gt; QAAnswer:\n    \"\"\"\n    Pass a single question to the llm and returns the answer\n    \"\"\"\n\n    if not self._database:\n        if not self._vectorstore:\n            return QAAnswer(error_msg=\"No database available!\", status=404)\n        self._database = Rag(\n            self._llm,\n            self.config.model,\n            self.config.model.prompt_config,\n            self._vectorstore,\n            self.config.features.return_source,\n        )\n    return self._database.query(question)\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.remove_file","title":"remove_file","text":"<pre><code>remove_file(file_name: str) -&gt; QAAnswer\n</code></pre> <p>Remove a document from the vectorstore</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def remove_file(self, file_name: str) -&gt; QAAnswer:\n    \"\"\"\n    Remove a document from the vectorstore\n    \"\"\"\n    if not self._vectorstore:\n        return QAAnswer(error_msg=\"No vector store initialized!\", status=404)\n    self._vectorstore.delete(\n        [\n            id\n            for id in self._vectorstore.index_to_docstore_id.values()\n            if id.startswith(file_name)\n        ]\n    )\n    return QAAnswer(status=200)\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.set_prompt_config","title":"set_prompt_config","text":"<pre><code>set_prompt_config(config: PromptConfig, qa_mode: QAModesEnum) -&gt; QAAnswer\n</code></pre> <p>Set the prompt for the mode</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def set_prompt_config(self, config: PromptConfig, qa_mode: QAModesEnum) -&gt; QAAnswer:\n    \"\"\"\n    Set the prompt for the mode\n    \"\"\"\n    answer = QAAnswer()\n    if qa_mode == QAModesEnum.SEARCH:\n        self.config.model.prompt_config = config\n        if \"context\" not in config.parameters:\n            answer.error_msg = (\n                \"Prompt does not include '{context}' variable. \"\n                \"No context will be added. \"\n            )\n        if \"question\" not in config.parameters:\n            answer.error_msg += (\n                \"Prompt does not include '{question}' variable. \"\n                \"Questions will not be passed to the model.\"\n            )\n    elif qa_mode == QAModesEnum.ANALYZE:\n        self.config.features.analyze.model.prompt_config = config\n    elif qa_mode == QAModesEnum.ANALYZE_MULT_PROMPTS:\n        self.config.features.analyze_mult_prompts.model.prompt_config = config\n    return answer\n</code></pre>"},{"location":"reference/gerd/training/","title":"gerd.training","text":""},{"location":"reference/gerd/training/#gerd.training","title":"gerd.training","text":"<p>Modules:</p> Name Description <code>lora</code>"},{"location":"reference/gerd/training/data/","title":"gerd.training.data","text":""},{"location":"reference/gerd/training/data/#gerd.training.data","title":"gerd.training.data","text":""},{"location":"reference/gerd/training/instruct/","title":"gerd.training.instruct","text":""},{"location":"reference/gerd/training/instruct/#gerd.training.instruct","title":"gerd.training.instruct","text":""},{"location":"reference/gerd/training/lora/","title":"gerd.training.lora","text":""},{"location":"reference/gerd/training/lora/#gerd.training.lora","title":"gerd.training.lora","text":"<p>Functions:</p> Name Description <code>load_training_config</code> <p>Load the LLM model configuration.</p>"},{"location":"reference/gerd/training/lora/#gerd.training.lora.load_training_config","title":"load_training_config","text":"<pre><code>load_training_config(config: str) -&gt; LoraTrainingConfig\n</code></pre> <p>Load the LLM model configuration.</p> <p>:param config: The name of the configuration. :return: The model configuration.</p> Source code in <code>gerd/training/lora.py</code> <pre><code>def load_training_config(config: str) -&gt; LoraTrainingConfig:\n    \"\"\"Load the LLM model configuration.\n\n    :param config: The name of the configuration.\n    :return: The model configuration.\n    \"\"\"\n    config_path = (\n        Path(config)\n        if config.endswith(\"yml\")\n        else Path(PROJECT_DIR, \"config\", f\"{config}.yml\")\n    )\n    with config_path.open(\"r\", encoding=\"utf-8\") as f:\n        conf = LoraTrainingConfig.model_validate(safe_load(f))\n    return conf\n</code></pre>"},{"location":"reference/gerd/training/trainer/","title":"gerd.training.trainer","text":""},{"location":"reference/gerd/training/trainer/#gerd.training.trainer","title":"gerd.training.trainer","text":""},{"location":"reference/gerd/training/unstructured/","title":"gerd.training.unstructured","text":""},{"location":"reference/gerd/training/unstructured/#gerd.training.unstructured","title":"gerd.training.unstructured","text":""},{"location":"reference/gerd/transport/","title":"gerd.transport","text":""},{"location":"reference/gerd/transport/#gerd.transport","title":"gerd.transport","text":""}]}