{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Generating and evaluating relevant documentation","text":"<p>GERD is developed as an experimental library to investigate how large language models (LLMs) can be used to generate and analyze (sets of) documents.</p> <p>This project was initially forked from Llama-2-Open-Source-LLM-CPU-Inference  by Kenneth Leung.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>If you just want to it try out, you can clone the project and install dependencies with <code>pip</code>:</p> <pre><code>git clone https://github.com/caretech-owl/gerd.git\ncd gerd\npip install -e \".[full]\"\npython examples/hello.py\n</code></pre> Source: examples/hello.py <pre><code>\"\"\"A 'hello world' example for GERD.\"\"\"\n\nimport logging\n\nfrom gerd.config import load_gen_config\nfrom gerd.gen.chat_service import ChatService\nfrom gerd.models.model import PromptConfig\n\nlogging.basicConfig(level=logging.WARNING)\nlogging.getLogger(\"gerd\").setLevel(logging.DEBUG)\n\nlogging.info(\n    \"Loading chat service...\"\n    \" When this is the first time you run this script, it will download the model.\"\n    \" This may take a few minutes.\"\n)\n\nchat = ChatService(load_gen_config(\"hello\"))\nres = chat.submit_user_message({\"word\": \"teleportation\"})\nlogging.info(res)\n\nchat.set_prompt_config(PromptConfig.model_validate({\"text\": \"{message}\"}))\nres = chat.submit_user_message({\"message\": \"Hello! What is one plus one?\"})\nlogging.info(res)\n</code></pre> <p>If you want to try this out in your browser, head over to binder \ud83d\udc49 .  Note that running LLMs on the CPU (and especially on limited virtual machines like binder) takes some time.</p>"},{"location":"#question-and-answer-example","title":"Question and Answer Example","text":"<p>Follow quickstart but execute <code>gradio</code> with the <code>qa_frontend</code> instead of the example file. When the server is done loading, open <code>http://127.0.0.1:7860</code> in your browser.</p> <pre><code>gradio gerd/frontends/qa_frontend.py\n# Some Llama.cpp outut\n# ...\n# * Running on local URL:  http://127.0.0.1:7860\n</code></pre> <p>Click the 'Click to Upload' button and search for a GRASCCO document named <code>Caja.txt</code> which is located in the <code>tests/data/grascoo</code> folder and upload it into the vector store. Next, you can query information from the document. For instance <code>Wie hei\u00dft der Patient?</code> (What is the patient called?).</p> <p></p>"},{"location":"#prompt-chaining","title":"Prompt Chaining","text":"<p>Prompt chaining is a prompt engineering approach to increase the 'reflection' of a large language model onto its given answer. Check <code>examples/chaining.py</code> for an illustration.</p> <pre><code>python examples/chaining.py\n# ...\n====== Resolved prompt =====\n\nsystem: You are a helpful assistant. Please answer the following question in a truthful and brief manner.\nuser: What type of mammal lays the biggest eggs?\n\n# ...\nResult: Based on the given information, the largest egg-laying mammal is the blue whale, which can lay up to 100 million eggs per year. However, the other assertions provided do not align with this information.\n</code></pre> Source: examples/chaining.py <pre><code>\"\"\"A chaining example for the chat service.\"\"\"\n\nimport logging\n\nfrom gerd.config import load_gen_config\nfrom gerd.gen.chat_service import ChatService\n\nlogging.basicConfig(level=logging.DEBUG)\n\ngen = ChatService(load_gen_config(\"gen_chaining\"))\nres = gen.generate({\"question\": \"What type of mammal lays the biggest eggs?\"})\nprint(f\"Result: {res.text}\")  # noqa: T201\n</code></pre> Config: config/gen_chaining.yml <pre><code>model:\n  name: \"Qwen/Qwen2.5-0.5B-Instruct\"\n  temperature: 0.05\n  max_new_tokens: 512\n  context_length: 2048\n  prompt_setup:\n    - [\"system\", text: \"You are a helpful assistant. Please answer the following question in a truthful and brief manner.\"]\nfeatures:\n  prompt_chaining:\n    prompts:\n      - text: \"{question}\"\n      - text: \"Here is a statement:\\n{response_1}\\n\\nMake a bullet point list of the assumptions you made when producing the above statement.\\n\\n\"\n      - text: \"Here is a bullet point list of assertions:\\n{response_2}\\n\\nFor each assertion, determine whether it is true or false. If it is false, explain why.\\n\\n\"\n      - text: \"{response_3}\\n\\nIn light of the above facts, how would you answer the question '{question}'\"\n</code></pre> <p>As you see, the answer does not make much sense with the default model which is rather small. Give it a try with meta-llama/Llama-3.2-3B. To use this model, you need to login with the huggingface cli and accept the Meta Community License Agreement.</p>"},{"location":"#full-documentation","title":"Full Documentation","text":"<p>A more detailled documentation can be found here \ud83d\udc49 .</p>"},{"location":"concepts/","title":"Concepts","text":""},{"location":"concepts/#usage","title":"Usage","text":"<p>GERD is primarly a tool for prototyping workflows for working with Large Language Models. It is meant to act as 'glue' between different tools and services and should ease the access to these tools.</p> <p>In general, there should be only be two components involved in a GERD workflow: A configuration and a service. The configuration can be assembled from different sources and should be able to be used in different services. The foundation of such a configration is a YAML file. GERD provides a set of those which can be found in the <code>config</code> directory. Configurations in this directly can be accessed by name as shown in the following example. A simple configutation file may look like this:</p> <pre><code>model:\n  name: \"Qwen/Qwen2.5-0.5B-Instruct\"\n  temperature: 0.1\n  top_p: 0.90\n  max_new_tokens: 256\n  repetition_penalty: 1.1\n  prompt_setup:\n    - [system, text: \"You are a helpful assistant. Start your answers with 'Sure thing, buddy!'.\"]\n  prompt_config:\n    text: \"Please say the word '{word}' three times.\"\n</code></pre> <p>And can be used with a <code>ChatService</code> via <code>load_gen_config</code> shown in the following example:</p> <pre><code>\"\"\"A 'hello world' example for GERD.\"\"\"\n\nimport logging\n\nfrom gerd.config import load_gen_config\nfrom gerd.gen.chat_service import ChatService\nfrom gerd.models.model import PromptConfig\n\nlogging.basicConfig(level=logging.WARNING)\nlogging.getLogger(\"gerd\").setLevel(logging.DEBUG)\n\nlogging.info(\n    \"Loading chat service...\"\n    \" When this is the first time you run this script, it will download the model.\"\n    \" This may take a few minutes.\"\n)\n\nchat = ChatService(load_gen_config(\"hello\"))\nres = chat.submit_user_message({\"word\": \"teleportation\"})\nlogging.info(res)\n\nchat.set_prompt_config(PromptConfig.model_validate({\"text\": \"{message}\"}))\nres = chat.submit_user_message({\"message\": \"Hello! What is one plus one?\"})\nlogging.info(res)\n</code></pre>"},{"location":"concepts/#configuration","title":"Configuration","text":"<p>Parameters in a configuration can be overwritten by environment variables. This is useful for secrets or other sensitive information or when using the same configuration in different environments. Environment variables may be provided in a <code>.env</code> file in the root directory of the project. <code>GenerationConfig</code> are prefixed with <code>gerd_gen_</code>, so to override the temperature of a model, the environment variable <code>gerd_gen_model__temperature</code> needs to be set. Similarly, <code>QAConfig</code> configurations are prefixed with <code>gerd_qa_</code>. The path for the used vector store can be set with <code>gerd_qa_embedding__db_path</code>. Note that nested configurations are separated by <code>__</code>. All environment variables are case insensitive, so you can use <code>GERD_GEN_MODEL__TEMPERATURE</code> as well to keep the common style of environment variables.</p>"},{"location":"concepts/#services","title":"Services","text":"<p>Gerd can work with models using transformers or llama.cpp bindings via llama-cpp-python. As <code>llama-cpp-python</code> is an optional dependency, you need to specify <code>llama-cpp</code> when you install GERD.</p> <pre><code># when installing from source\npip install .[llama-cpp]\n# when using uv\nuv sync --group llama-cpp\n</code></pre> <p>GERD choses whether to use <code>transformers</code> or <code>llama-cpp</code> based on the configuration. Usually, you need to specify a certain GGUF file when using quantized models which is the primary use for GERD and <code>llama-cpp</code>. Consequently, if <code>model.file</code> is set, GERD will assume that you want to use a quantized model and load llama-cpp bindings. If only a huggingface model handle (e.g. <code>org/model_name</code>) is provided, GERD will use the transformers library to load the model.</p> <p>Furthermore, GERD supports the usage of llama.cpp servers as well as the OpenAI API. If you set an endpoint, the model handle and file parameters will be ignored and the model will be ignored. Make sure to set the right endpoint type because the provided capabilities differ.</p>"},{"location":"develop/","title":"Development Guide","text":""},{"location":"develop/#basics","title":"Basics","text":"<p>To get started on development you need to install uv. You can use <code>pip</code>, <code>pipx</code> or <code>conda</code> to do so:</p> <pre><code>pip install uv\n</code></pre> <p>Next install the package and all dependencies with <code>uv sync</code>.</p> <pre><code># cd &lt;gerd_project_root&gt;\nuv sync\n</code></pre> <p>After that, it should be possible to run scripts without further issues:</p> <pre><code>uv run examples/hello.py\n</code></pre> <p>To add a new runtime dependency, just run <code>uv add</code>:</p> <pre><code>uv add langchain\n</code></pre> <p>To add a new development dependency, run <code>uv add</code> with the <code>--dev</code> flag:</p> <pre><code>uv add mypy --dev\n</code></pre>"},{"location":"develop/#pre-commit-hooks-recommended","title":"Pre-commit hooks (recommended)","text":"<p>Pre-commit hooks are used to check linting and run tests before commit changes to prevent faulty commits. Thus, it is recommended to use these hooks! Hooks should not include long running actions (such as tests) since committing should be fast. To install pre-commit hooks, execute this once:</p> <pre><code>uv run pre-commit install\n</code></pre>"},{"location":"develop/#further-tools","title":"Further tools","text":""},{"location":"develop/#poe-task-runner","title":"Poe Task Runner","text":"<p>Task runner configuration are stored in the <code>pyproject.toml</code> file. You can run most of the tools mentioned above with a (shorter) call to <code>poe</code>.</p> <pre><code>uv run poe lint  # do some linting (with mypy)\n</code></pre>"},{"location":"develop/#pytest","title":"PyTest","text":"<p>Test case are run via pytest. Tests can be found in the <code>/tests</code> folder. Tests will not be run via pre-commit since they might be too complex to be done before commits. To run the standard set of tests use the <code>poe</code> task <code>test</code>:</p> <pre><code>uv run poe test\n</code></pre> <p>More excessive testing can be trigger with <code>test_manual</code> which will NOT mock calls to the used models:</p> <pre><code>uv run poe test_manual\n</code></pre>"},{"location":"develop/#ruff","title":"Ruff","text":"<p>Ruff is used for linting and code formatting. Ruff follows <code>black</code> styling ref. Ruff will be run automatically before commits when pre-commit hooks are installed. To run <code>ruff</code> manually, use uv:</p> <pre><code>uv run ruff check gerd\n</code></pre> <p>There is a VSCode extension that handles formatting and linting.</p>"},{"location":"develop/#mypy","title":"MyPy","text":"<p>MyPy does static type checking. It will not be run automatically. To run MyPy manually use uv with the folder to be checked:</p> <pre><code>uv run mypy gerd\n</code></pre>"},{"location":"develop/#implemented-guis","title":"Implemented GUIs","text":""},{"location":"develop/#run-frontend","title":"Run Frontend","text":"<p>Either run Generate Frontend:</p> <pre><code>uv run poe gen_dev\n</code></pre> <p>or QA Frontend:</p> <pre><code>uv run poe qa_dev\n</code></pre> <p>or the GERD Router:</p> <pre><code># add _dev for the gradio live reload version\n# omit in 'prod'\nuv run router[_dev]\n</code></pre>"},{"location":"develop/#cicd-and-distribution","title":"CI/CD and Distribution","text":""},{"location":"develop/#github-actions","title":"GitHub Actions","text":"<p>GitHub Actions can be found under .github/workflows. There is currently one main CI workflow called <code>python-ci.yml</code>:</p> <pre><code># This workflow will install Python dependencies, run tests and lint with a variety of Python versions\n# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions\n\nname: Linting/Testing Python\n\non:\n  push:\n    branches: [main, dev-gha]\n  pull_request:\n    branches: [main]\n\njobs:\n  linting:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Set up Python 3.12\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n      - name: Install uv and development dependencies with extras\n        run: |\n          pipx install uv \n          uv sync\n      - name: Run linting\n        run: uv run poe lint\n  testing:\n    needs: linting\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest]\n        python-version: [\"3.12\", \"3.10\"]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/checkout@v4\n      - name: Cache HuggingFace models and data\n        uses: actions/cache@v4\n        with:\n          path: ~/.cache/huggingface\n          key: ${{ runner.os }}-hf\n      - name: Install uv\n        run: pipx install uv\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Install development dependencies with extras\n        run: uv sync\n      - name: Run Tests\n        run: uv run poe cov\n      - name: Report Coverage\n        run: uv run coverage report --skip-covered --omit=\"tests/*\" --format=markdown &gt;&gt; $GITHUB_STEP_SUMMARY\n</code></pre> <p>In its current config it will only be executed when a PR for <code>main</code> is created or when a special <code>dev-gha</code> branch is created. It will also trigger actions when commits are pushed to <code>main</code> directly but this should be avoided.</p>"},{"location":"develop/#github-issue-templates","title":"GitHub Issue Templates","text":"<p>This project uses GitHub issue templates. Currently, there are three templates available.</p>"},{"location":"develop/#bug-report","title":"Bug Report","text":"<pre><code>name: Bug\ndescription: File a bug report\ntitle: \"[Bug]: \"\nlabels: [\"bug\"]\n# assignees:\n#   - aleneum\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        Thanks for taking the time to fill out this bug report!\n  - type: textarea\n    id: expected-behavior\n    attributes:\n      label: Expected Behavior\n      description: What should happen\n    validations:\n      required: true\n  - type: textarea\n    id: actual-behavior\n    attributes:\n      label: Actual Behavior\n      description: What happened instead\n    validations:\n      required: true\n  - type: textarea\n    id: steps-reproduce\n    attributes:\n      label: Steps to Reproduce the Problem\n      value: |\n        1.\n        2.\n        3.\n\n    validations:\n      required: true\n  - type: input\n    id: affected-version\n    attributes:\n      label: Affected version\n      description: Version number or commit hash\n      placeholder: \"0.1.0\"\n    validations:\n      required: true\n  - type: dropdown\n    id: affected-components\n    attributes:\n      label: Affected components\n      description: Which components are affected\n      multiple: true\n      options:\n        - Gen Service\n        - Qa Service\n        - Frontend\n        - Backend\n    validations:\n      required: false\n  - type: dropdown\n    id: affected-plattforms\n    attributes:\n      label: Affected plattforms\n      description: Which plattforms are affected\n      multiple: true\n      options:\n        - Windows\n        - Linux\n        - MacOS\n    validations:\n      required: false\n  - type: textarea\n    id: further-information\n    attributes:\n      label: Further contextual information and suggestions\n      description: More detailed descriptions or possible solutions\n    validations:\n      required: false\n</code></pre>"},{"location":"develop/#feature-request","title":"Feature Request","text":"<pre><code>name: Feature Request\ndescription: Suggest an idea for this project\ntitle: \"[FR]: \"\nlabels: [\"enhancement\"]\n# assignees:\n#   - aleneum\nbody:\n  - type: markdown\n    attributes:\n      value: Do you have an idea for a feature that would make this project more helpful or easier to use?\n  - type: textarea\n    id: problem-description\n    attributes:\n      label: Problem description\n      description: Is your feature request related to a problem? Please describe.\n    validations:\n      required: true\n  - type: textarea\n    id: solution\n    attributes:\n      label: Solution\n      description: A clear and concise description of what you want to happen.\n    validations:\n      required: true\n  - type: textarea\n    id: additional-context\n    attributes:\n      label: Additional context\n      description: Add any other context or screenshots about the feature request here.\n    validations:\n      required: false\n  - type: dropdown\n    id: affected-components\n    attributes:\n      label: Affected components\n      description: Which components are affected\n      multiple: true\n      options:\n        - Gen Service\n        - Qa Service\n        - Frontend\n        - Backend\n    validations:\n      required: false\n</code></pre>"},{"location":"develop/#use-case","title":"Use Case","text":"<pre><code>name: Use Case\ndescription: A description of a user-centered system requirement\ntitle: \"[UC]: \"\nlabels: [\"use case\"]\n# assignees:\n#   - aleneum\nbody:\n  - type: textarea\n    id: summary\n    attributes:\n      label: Summary\n      description: A concise description of the use case\n    validations:\n      required: true\n  - type: textarea\n    id: rationale\n    attributes:\n      label: Rationale\n      description: The motivation and value added by the described use case\n    validations:\n      required: true\n  - type: dropdown\n    id: level\n    attributes:\n      label: Level\n      multiple: false\n      options:\n        - user goal\n        - subfunction\n  - type: input\n    id: actors\n    attributes:\n      label: Actors\n      description: Someone or something with behavior, e.g. a person (identified by role), a computer system, or an organization\n    validations:\n      required: true\n  - type: textarea\n    id: preconditions\n    attributes:\n      label: Preconditions\n      description: What needs to be true on start\n      placeholder: |\n        - Use Cases: #1, #2\n        - User is authenticated\n    validations:\n      required: true\n  - type: textarea\n    id: postconditions\n    attributes:\n      label: Postconditions\n      description: What must be true on successful completion\n      placeholder: \"The form \"\n    validations:\n      required: true\n  - type: textarea\n    id: basic-flow\n    attributes:\n      label: Basic Flow\n      description: A typical, unconditional happy path\n      value: |\n        1.\n        2.\n        3.\n\n    validations:\n      required: true\n  - type: textarea\n    id: alternative-paths\n    attributes:\n      label: Alternative Paths\n      description: Alternate scenarios of success or failure\n      placeholder: |\n        2.1\n        2.2\n        3.1\n\n    validations:\n      required: false\n  - type: textarea\n    id: visualisation\n    attributes:\n      label: Visualisation\n      description: A mermaid diagram or image depiciting the action flow\n      value: |\n        ```mermaid\n        flowchart LR;\n          1--&gt;2\n          2--&gt;3\n          3--&gt;4\n          3--&gt;3.1\n          3.1--&gt;3.2\n          3.2--&gt;2\n          4--&gt;4.1\n          4.1--&gt;4\n        ```\n    validations:\n      required: false\n  - type: textarea\n    id: related-issues\n    attributes:\n      label: Other related issues, use cases, features\n      description: What must be true on successful completion\n      placeholder: \"#1 #2 #3\"\n    validations:\n      required: false\n</code></pre>"},{"location":"related/","title":"Related tools and service","text":"<p>There is a lot of momentum in the field of Large Language Models (LLMs) and many tools and services are being developed to work with them. GERD is not intended to be a daily driver for working with LLMs, but rather a tool to prototype workflows and to ease the access and configuration of currently available models.</p> <p>There are many tools, some with large and striving communities, that can be used to work with LLMs. Here are some of them:</p>"},{"location":"related/#hugging-face-transformers","title":"Hugging Face Transformers","text":"<p>The Transformers library is a popular open-source library that provides a wide range of pre-trained models for Natural Language Processing (NLP) tasks. It is built on top of PyTorch and TensorFlow and provides a simple API for working with LLMs.   </p>"},{"location":"related/#llamacpp","title":"llama.cpp","text":"<p>The main goal of llama.cpp is to enable LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware - locally and in the cloud.</p>"},{"location":"related/#gpt4all","title":"GPT4All","text":"<p>GPT4All is a desktop application that allows you to download many different LLM and use them locally. The application is available for Windows, MacOS, and Linux. Furthermore, you can use use the generation services from OpenAI, DeepSeek or Mistral as well.</p>"},{"location":"related/#private-gpt","title":"Private GPT","text":"<p>This application is very similar to GPT4All. It used to provide more options to tweak workflows that uses Retrieval Augmented Generation (RAG). However, as the development of both applications progresses rapidly, it is best to check them out yourself.</p>"},{"location":"related/#text-generation-web-ui","title":"Text generation web UI","text":"<p>A feature-rich web UI based on gradio with a primary focus on text generation. It can be used to test models for text generation, for instruction based tasks, even with different roles/custom characters. As it is a web UI, it can also be used to host a llm service in a local network or -- thanks to gradio -- even on the internet. The ui also features means to train LoRA on unstructured data or for instruction-based tasks.</p>"},{"location":"related/#ollama","title":"Ollama","text":"<p>Ollama provides a terminal server and client to host and use llm models within minutes. For most use cases it wraps configurations (e.g. for llama.cpp) and provides a simple interface to use them. It a good tool to get started, however, it is not as flexible as llama.cpp itself and also sometimes provides configurations that could be misleading. For instance, if you use <code>deepseek-r1</code> with ollama, it will download and host a quantized and distilled model, which is not the same as the original model. Usually, having a look at the configuration in the online database should clarifty this and as most machine aren't capable of hosting a full <code>r1</code> model, this is not a bad thing, but it is important to know that the model is not the same as the original one.</p>"},{"location":"reference/gerd/","title":"gerd","text":""},{"location":"reference/gerd/#gerd","title":"gerd","text":"<p>Generating and evaluating relevant documentation (GERD).</p> <p>This package provides the GERD system for working with large language models (LLMs). This includes means to generate texts using different backends and frontends. The system is designed to be flexible and extensible to support different use cases. It can also be used for Retrieval Augmented Generation (RAG) tasks or as a chatbot.</p> <p>Modules:</p> Name Description <code>backends</code> <p>This module contains backend implementations that manage services.</p> <code>config</code> <p>Configuration for the application.</p> <code>features</code> <p>Special features to extend the functionality of GERD services.</p> <code>frontends</code> <p>A collection of several gradio frontends.</p> <code>gen</code> <p>Services and utilities for text generation with LLMs.</p> <code>loader</code> <p>Module for loading language models.</p> <code>models</code> <p>Pydantic model definitions and data classes that are share accross modules.</p> <code>qa</code> <p>Services and utilities for retrieval augmented generation (RAG).</p> <code>rag</code> <p>Retrieval-Augmented Generation (RAG) backend.</p> <code>training</code> <p>Collections of training routines for GERD.</p> <code>transport</code> <p>Module to define the transport protocol.</p>"},{"location":"reference/gerd/backends/","title":"gerd.backends","text":""},{"location":"reference/gerd/backends/#gerd.backends","title":"gerd.backends","text":"<p>This module contains backend implementations that manage services.</p> <p>These backends can be used by frontends such as gradio. Furthermore, the backend module contains service implementations for loading LLMs or vector stores for Retrieval Augmented Generation.</p> <p>Modules:</p> Name Description <code>bridge</code> <p>The Bridge connects backend and frontend services directly for local use.</p> <code>rest_client</code> <p>REST client for the GERD server.</p> <code>rest_server</code> <p>REST server as a GERD backend.</p> <p>Attributes:</p> Name Type Description <code>TRANSPORTER</code> <code>Transport</code> <p>The default transporter that connects the backend services to the frontend.</p>"},{"location":"reference/gerd/backends/#gerd.backends.TRANSPORTER","title":"TRANSPORTER  <code>module-attribute</code>","text":"<pre><code>TRANSPORTER: Transport = Bridge()\n</code></pre> <p>The default transporter that connects the backend services to the frontend.</p>"},{"location":"reference/gerd/backends/bridge/","title":"gerd.backends.bridge","text":""},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge","title":"gerd.backends.bridge","text":"<p>The Bridge connects backend and frontend services directly for local use.</p> <p>Classes:</p> Name Description <code>Bridge</code> <p>Direct connection between backend services and frontend.</p>"},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge","title":"Bridge","text":"<pre><code>Bridge()\n</code></pre> <p>               Bases: <code>Transport</code></p> <p>Direct connection between backend services and frontend.</p> <p>Frontends that make use of the <code>Transport</code> abstraction can use <code>Bridge</code> to get accessto generation and QA services directly. This is useful for local use cases where the frontend and backend are running in the same process.</p> <p>The services associated with the bridge are initialized lazily.</p> <p>Methods:</p> Name Description <code>add_file</code> <p>Add a file to the vector store.</p> <code>analyze_mult_prompts_query</code> <p>Queries the vector store with a set of predefined queries.</p> <code>analyze_query</code> <p>Queries the vector store with a predefined query.</p> <code>db_embedding</code> <p>Converts a question to an embedding.</p> <code>db_query</code> <p>Queries the vector store with a question.</p> <code>generate</code> <p>Generates text with the generation service.</p> <code>get_gen_prompt</code> <p>Gets the prompt configuration for the generation service.</p> <code>get_qa_prompt</code> <p>Gets the prompt configuration for a mode of the QA service.</p> <code>qa_query</code> <p>Query the QA service with a question.</p> <code>remove_file</code> <p>Remove a file from the vector store.</p> <code>set_gen_prompt</code> <p>Sets the prompt configuration for the generation service.</p> <code>set_qa_prompt</code> <p>Sets the prompt configuration for the QA service.</p> <p>Attributes:</p> Name Type Description <code>gen</code> <code>GenerationService</code> <p>Get the generation service instance.</p> <code>qa</code> <code>QAService</code> <p>Get the QA service instance. It will be created if it does not exist.</p> Source code in <code>gerd/backends/bridge.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"The services associated with the bridge are initialized lazily.\"\"\"\n    super().__init__()\n    self._qa: Optional[QAService] = None\n    self._gen: Optional[GenerationService] = None\n</code></pre>"},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.gen","title":"gen  <code>property</code>","text":"<pre><code>gen: GenerationService\n</code></pre> <p>Get the generation service instance.</p> <p>It will be created if it does not exist.</p>"},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.qa","title":"qa  <code>property</code>","text":"<pre><code>qa: QAService\n</code></pre> <p>Get the QA service instance. It will be created if it does not exist.</p>"},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.add_file","title":"add_file","text":"<pre><code>add_file(file: QAFileUpload) -&gt; QAAnswer\n</code></pre> <p>Add a file to the vector store.</p> <p>The returned answer has a status code of 200 if the file was added successfully. Parameters:     file: The file to add to the vector store.</p> <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer from the QA service</p> Source code in <code>gerd/backends/bridge.py</code> <pre><code>@override\ndef add_file(self, file: QAFileUpload) -&gt; QAAnswer:\n    return self.qa.add_file(file)\n</code></pre>"},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.analyze_mult_prompts_query","title":"analyze_mult_prompts_query","text":"<pre><code>analyze_mult_prompts_query() -&gt; QAAnalyzeAnswer\n</code></pre> <p>Queries the vector store with a set of predefined queries.</p> <p>In contrast to <code>analyze_query</code>, this method queries the vector store with multiple prompts.</p> <p>Returns:</p> Type Description <code>QAAnalyzeAnswer</code> <p>The answer from the QA service.</p> Source code in <code>gerd/backends/bridge.py</code> <pre><code>@override\ndef analyze_mult_prompts_query(self) -&gt; QAAnalyzeAnswer:\n    return self.qa.analyze_mult_prompts_query()\n</code></pre>"},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.analyze_query","title":"analyze_query","text":"<pre><code>analyze_query() -&gt; QAAnalyzeAnswer\n</code></pre> <p>Queries the vector store with a predefined query.</p> <p>The query should return vital information gathered from letters of discharge.</p> <p>Returns:</p> Type Description <code>QAAnalyzeAnswer</code> <p>The answer from the QA service.</p> Source code in <code>gerd/backends/bridge.py</code> <pre><code>@override\ndef analyze_query(self) -&gt; QAAnalyzeAnswer:\n    return self.qa.analyze_query()\n</code></pre>"},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.db_embedding","title":"db_embedding","text":"<pre><code>db_embedding(question: QAQuestion) -&gt; List[float]\n</code></pre> <p>Converts a question to an embedding.</p> <p>The embedding is defined by the vector store.</p> <p>Parameters:</p> Name Type Description Default <code>QAQuestion</code> <p>The question to convert to an embedding.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>The embedding of the question</p> Source code in <code>gerd/backends/bridge.py</code> <pre><code>@override\ndef db_embedding(self, question: QAQuestion) -&gt; List[float]:\n    return self.qa.db_embedding(question)\n</code></pre>"},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.db_embedding(question)","title":"<code>question</code>","text":""},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.db_query","title":"db_query","text":"<pre><code>db_query(question: QAQuestion) -&gt; List[DocumentSource]\n</code></pre> <p>Queries the vector store with a question.</p> <p>Parameters:</p> Name Type Description Default <code>QAQuestion</code> <p>The question to query the vector store with.</p> required <p>Returns:</p> Type Description <code>List[DocumentSource]</code> <p>A list of document sources</p> Source code in <code>gerd/backends/bridge.py</code> <pre><code>@override\ndef db_query(self, question: QAQuestion) -&gt; List[DocumentSource]:\n    return self.qa.db_query(question)\n</code></pre>"},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.db_query(question)","title":"<code>question</code>","text":""},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.generate","title":"generate","text":"<pre><code>generate(parameters: Dict[str, str]) -&gt; GenResponse\n</code></pre> <p>Generates text with the generation service.</p> <p>Parameters:</p> Name Type Description Default <code>Dict[str, str]</code> <p>The parameters to generate text with</p> required <p>Returns:</p> Type Description <code>GenResponse</code> <p>The generation result</p> Source code in <code>gerd/backends/bridge.py</code> <pre><code>@override\ndef generate(self, parameters: Dict[str, str]) -&gt; GenResponse:\n    return self.gen.generate(parameters)\n</code></pre>"},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.generate(parameters)","title":"<code>parameters</code>","text":""},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.get_gen_prompt","title":"get_gen_prompt","text":"<pre><code>get_gen_prompt() -&gt; PromptConfig\n</code></pre> <p>Gets the prompt configuration for the generation service.</p> <p>Returns:</p> Type Description <code>PromptConfig</code> <p>The current prompt configuration</p> Source code in <code>gerd/backends/bridge.py</code> <pre><code>@override\ndef get_gen_prompt(self) -&gt; PromptConfig:\n    return self.gen.get_prompt_config()\n</code></pre>"},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.get_qa_prompt","title":"get_qa_prompt","text":"<pre><code>get_qa_prompt(qa_mode: QAModesEnum) -&gt; PromptConfig\n</code></pre> <p>Gets the prompt configuration for a mode of the QA service.</p> <p>Parameters:</p> Name Type Description Default <code>QAModesEnum</code> <p>The mode to get the prompt configuration for</p> required <p>Returns:</p> Type Description <code>PromptConfig</code> <p>The prompt configuration for the QA service</p> Source code in <code>gerd/backends/bridge.py</code> <pre><code>@override\ndef get_qa_prompt(self, qa_mode: QAModesEnum) -&gt; PromptConfig:\n    return self.qa.get_prompt_config(qa_mode)\n</code></pre>"},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.get_qa_prompt(qa_mode)","title":"<code>qa_mode</code>","text":""},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.qa_query","title":"qa_query","text":"<pre><code>qa_query(question: QAQuestion) -&gt; QAAnswer\n</code></pre> <p>Query the QA service with a question.</p> <p>Parameters:</p> Name Type Description Default <code>QAQuestion</code> <p>The question to query the QA service with.</p> required <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer from the QA service.</p> Source code in <code>gerd/backends/bridge.py</code> <pre><code>@override\ndef qa_query(self, question: QAQuestion) -&gt; QAAnswer:\n    return self.qa.query(question)\n</code></pre>"},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.qa_query(query)","title":"<code>query</code>","text":""},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.remove_file","title":"remove_file","text":"<pre><code>remove_file(file_name: str) -&gt; QAAnswer\n</code></pre> <p>Remove a file from the vector store.</p> <p>The returned answer has a status code of 200 if the file was removed successfully. Parameters:     file_name: The name of the file to remove from the vector store.</p> <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer from the QA service</p> Source code in <code>gerd/backends/bridge.py</code> <pre><code>@override\ndef remove_file(self, file_name: str) -&gt; QAAnswer:\n    return self.qa.remove_file(file_name)\n</code></pre>"},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.set_gen_prompt","title":"set_gen_prompt","text":"<pre><code>set_gen_prompt(config: PromptConfig) -&gt; PromptConfig\n</code></pre> <p>Sets the prompt configuration for the generation service.</p> <p>The prompt configuration that is returned should in most cases be the same as the one that was set. Parameters:     config: The prompt configuration to set</p> <p>Returns:</p> Type Description <code>PromptConfig</code> <p>The prompt configuration that was set</p> Source code in <code>gerd/backends/bridge.py</code> <pre><code>@override\ndef set_gen_prompt(self, config: PromptConfig) -&gt; PromptConfig:\n    return self.gen.set_prompt_config(config)\n</code></pre>"},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.set_qa_prompt","title":"set_qa_prompt","text":"<pre><code>set_qa_prompt(config: PromptConfig, qa_mode: QAModesEnum) -&gt; QAAnswer\n</code></pre> <p>Sets the prompt configuration for the QA service.</p> <p>Since the QA service uses multiple prompt configurations, the mode should be specified. For more details, see the documentation of <code>QAService.set_prompt_config</code>.</p> <p>Parameters:</p> Name Type Description Default <code>PromptConfig</code> <p>The prompt configuration to set</p> required <code>QAModesEnum</code> <p>The mode to set the prompt configuration for</p> required <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer from the QA service</p> Source code in <code>gerd/backends/bridge.py</code> <pre><code>@override\ndef set_qa_prompt(self, config: PromptConfig, qa_mode: QAModesEnum) -&gt; QAAnswer:\n    return self.qa.set_prompt_config(config, qa_mode)\n</code></pre>"},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.set_qa_prompt(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/backends/bridge/#gerd.backends.bridge.Bridge.set_qa_prompt(qa_mode)","title":"<code>qa_mode</code>","text":""},{"location":"reference/gerd/backends/rest_client/","title":"gerd.backends.rest_client","text":""},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client","title":"gerd.backends.rest_client","text":"<p>REST client for the GERD server.</p> <p>Classes:</p> Name Description <code>RestClient</code> <p>REST client for the GERD server.</p>"},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient","title":"RestClient","text":"<pre><code>RestClient()\n</code></pre> <p>               Bases: <code>Transport</code></p> <p>REST client for the GERD server.</p> <p>The client initalizes the server URL.</p> <p>It is retrieved from the global CONFIG. Other (timeout) settings are also set here but not configurable as of now.</p> <p>Methods:</p> Name Description <code>add_file</code> <p>Add a file to the vector store.</p> <code>analyze_mult_prompts_query</code> <p>Queries the vector store with a set of predefined queries.</p> <code>analyze_query</code> <p>Queries the vector store with a predefined query.</p> <code>db_embedding</code> <p>Converts a question to an embedding.</p> <code>db_query</code> <p>Queries the vector store with a question.</p> <code>generate</code> <p>Generates text with the generation service.</p> <code>get_gen_prompt</code> <p>Gets the prompt configuration for the generation service.</p> <code>get_qa_prompt</code> <p>Gets the prompt configuration for a mode of the QA service.</p> <code>qa_query</code> <p>Query the QA service with a question.</p> <code>remove_file</code> <p>Remove a file from the vector store.</p> <code>set_gen_prompt</code> <p>Sets the prompt configuration for the generation service.</p> <code>set_qa_prompt</code> <p>Sets the prompt configuration for the QA service.</p> Source code in <code>gerd/backends/rest_client.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"The client initalizes the server URL.\n\n    It is retrieved from the global [CONFIG][gerd.config.CONFIG].\n    Other (timeout) settings are also set here but not configurable as of now.\n    \"\"\"\n    super().__init__()\n    self._url = f\"http://{CONFIG.server.host}:{CONFIG.server.port}{CONFIG.server.api_prefix}\"\n    self.timeout = 10\n    self.longtimeout = 10000\n</code></pre>"},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.add_file","title":"add_file","text":"<pre><code>add_file(file: QAFileUpload) -&gt; QAAnswer\n</code></pre> <p>Add a file to the vector store.</p> <p>The returned answer has a status code of 200 if the file was added successfully. Parameters:     file: The file to add to the vector store.</p> <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer from the QA service</p> Source code in <code>gerd/backends/rest_client.py</code> <pre><code>@override\ndef add_file(self, file: QAFileUpload) -&gt; QAAnswer:\n    t = file.model_dump_json().encode(\"utf-8\")\n    return QAAnswer.model_validate(\n        requests.post(\n            f\"{self._url}/qa/file\",\n            data=file.model_dump_json().encode(\"utf-8\"),\n            timeout=self.timeout,\n        ).json()\n    )\n</code></pre>"},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.analyze_mult_prompts_query","title":"analyze_mult_prompts_query","text":"<pre><code>analyze_mult_prompts_query() -&gt; QAAnalyzeAnswer\n</code></pre> <p>Queries the vector store with a set of predefined queries.</p> <p>In contrast to <code>analyze_query</code>, this method queries the vector store with multiple prompts.</p> <p>Returns:</p> Type Description <code>QAAnalyzeAnswer</code> <p>The answer from the QA service.</p> Source code in <code>gerd/backends/rest_client.py</code> <pre><code>@override\ndef analyze_mult_prompts_query(self) -&gt; QAAnalyzeAnswer:\n    return QAAnalyzeAnswer.model_validate(\n        requests.post(\n            f\"{self._url}/qa/query_analyze_mult_prompt\",\n            timeout=self.longtimeout,\n        ).json()\n    )\n</code></pre>"},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.analyze_query","title":"analyze_query","text":"<pre><code>analyze_query() -&gt; QAAnalyzeAnswer\n</code></pre> <p>Queries the vector store with a predefined query.</p> <p>The query should return vital information gathered from letters of discharge.</p> <p>Returns:</p> Type Description <code>QAAnalyzeAnswer</code> <p>The answer from the QA service.</p> Source code in <code>gerd/backends/rest_client.py</code> <pre><code>@override\ndef analyze_query(self) -&gt; QAAnalyzeAnswer:\n    return QAAnalyzeAnswer.model_validate(\n        requests.post(\n            f\"{self._url}/qa/query_analyze\",\n            timeout=self.longtimeout,\n        ).json()\n    )\n</code></pre>"},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.db_embedding","title":"db_embedding","text":"<pre><code>db_embedding(question: QAQuestion) -&gt; List[float]\n</code></pre> <p>Converts a question to an embedding.</p> <p>The embedding is defined by the vector store.</p> <p>Parameters:</p> Name Type Description Default <code>QAQuestion</code> <p>The question to convert to an embedding.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>The embedding of the question</p> Source code in <code>gerd/backends/rest_client.py</code> <pre><code>@override\ndef db_embedding(self, question: QAQuestion) -&gt; List[float]:\n    request = question.model_dump_json()\n    _LOGGER.debug(\"db_embedding - request: %s\", request)\n    response = requests.post(\n        f\"{self._url}/qa/db_embedding\",\n        data=question.model_dump_json().encode(\"utf-8\"),\n        timeout=self.timeout,\n    )\n    _LOGGER.debug(\"db_embedding - response: %s\", response.json())\n    return TypeAdapter(List[float]).validate_python(response.json())\n</code></pre>"},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.db_embedding(question)","title":"<code>question</code>","text":""},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.db_query","title":"db_query","text":"<pre><code>db_query(question: QAQuestion) -&gt; List[DocumentSource]\n</code></pre> <p>Queries the vector store with a question.</p> <p>Parameters:</p> Name Type Description Default <code>QAQuestion</code> <p>The question to query the vector store with.</p> required <p>Returns:</p> Type Description <code>List[DocumentSource]</code> <p>A list of document sources</p> Source code in <code>gerd/backends/rest_client.py</code> <pre><code>@override\ndef db_query(self, question: QAQuestion) -&gt; List[DocumentSource]:\n    request = question.model_dump_json()\n    _LOGGER.debug(\"db_query - request: %s\", request)\n    response = requests.post(\n        f\"{self._url}/qa/db_query\",\n        data=question.model_dump_json(),\n        timeout=self.timeout,\n    )\n    _LOGGER.debug(\"db_query - response: %s\", response.json())\n    return TypeAdapter(List[DocumentSource]).validate_python(response.json())\n</code></pre>"},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.db_query(question)","title":"<code>question</code>","text":""},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.generate","title":"generate","text":"<pre><code>generate(parameters: Dict[str, str]) -&gt; GenResponse\n</code></pre> <p>Generates text with the generation service.</p> <p>Parameters:</p> Name Type Description Default <code>Dict[str, str]</code> <p>The parameters to generate text with</p> required <p>Returns:</p> Type Description <code>GenResponse</code> <p>The generation result</p> Source code in <code>gerd/backends/rest_client.py</code> <pre><code>@override\ndef generate(self, parameters: Dict[str, str]) -&gt; GenResponse:\n    return GenResponse.model_validate(\n        requests.post(\n            f\"{self._url}/gen/generate\",\n            json=parameters,\n            timeout=self.timeout,\n        ).json()\n    )\n</code></pre>"},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.generate(parameters)","title":"<code>parameters</code>","text":""},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.get_gen_prompt","title":"get_gen_prompt","text":"<pre><code>get_gen_prompt() -&gt; PromptConfig\n</code></pre> <p>Gets the prompt configuration for the generation service.</p> <p>Returns:</p> Type Description <code>PromptConfig</code> <p>The current prompt configuration</p> Source code in <code>gerd/backends/rest_client.py</code> <pre><code>@override\ndef get_gen_prompt(self) -&gt; PromptConfig:\n    return PromptConfig.model_validate(\n        requests.get(f\"{self._url}/gen/prompt\", timeout=self.timeout).json()\n    )\n</code></pre>"},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.get_qa_prompt","title":"get_qa_prompt","text":"<pre><code>get_qa_prompt(qa_mode: QAModesEnum) -&gt; PromptConfig\n</code></pre> <p>Gets the prompt configuration for a mode of the QA service.</p> <p>Parameters:</p> Name Type Description Default <code>QAModesEnum</code> <p>The mode to get the prompt configuration for</p> required <p>Returns:</p> Type Description <code>PromptConfig</code> <p>The prompt configuration for the QA service</p> Source code in <code>gerd/backends/rest_client.py</code> <pre><code>@override\ndef get_qa_prompt(self, qa_mode: QAModesEnum) -&gt; PromptConfig:\n    return PromptConfig.model_validate(\n        requests.get(\n            f\"{self._url}/qa/prompt\",\n            timeout=self.timeout,\n            params={\"qa_mode\": qa_mode.value},\n        ).json()\n    )\n</code></pre>"},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.get_qa_prompt(qa_mode)","title":"<code>qa_mode</code>","text":""},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.qa_query","title":"qa_query","text":"<pre><code>qa_query(question: QAQuestion) -&gt; QAAnswer\n</code></pre> <p>Query the QA service with a question.</p> <p>Parameters:</p> Name Type Description Default <code>QAQuestion</code> <p>The question to query the QA service with.</p> required <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer from the QA service.</p> Source code in <code>gerd/backends/rest_client.py</code> <pre><code>@override\ndef qa_query(self, question: QAQuestion) -&gt; QAAnswer:\n    return QAAnswer.model_validate(\n        requests.post(\n            f\"{self._url}/qa/query\",\n            data=question.model_dump_json().encode(\"utf-8\"),\n            timeout=self.longtimeout,\n        ).json()\n    )\n</code></pre>"},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.qa_query(query)","title":"<code>query</code>","text":""},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.remove_file","title":"remove_file","text":"<pre><code>remove_file(file_name: str) -&gt; QAAnswer\n</code></pre> <p>Remove a file from the vector store.</p> <p>The returned answer has a status code of 200 if the file was removed successfully. Parameters:     file_name: The name of the file to remove from the vector store.</p> <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer from the QA service</p> Source code in <code>gerd/backends/rest_client.py</code> <pre><code>@override\ndef remove_file(self, file_name: str) -&gt; QAAnswer:\n    return QAAnswer.model_validate(\n        requests.delete(\n            f\"{self._url}/qa/file\",\n            data=file_name.encode(\"utf-8\"),\n            timeout=self.timeout,\n        ).json()\n    )\n</code></pre>"},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.set_gen_prompt","title":"set_gen_prompt","text":"<pre><code>set_gen_prompt(config: PromptConfig) -&gt; PromptConfig\n</code></pre> <p>Sets the prompt configuration for the generation service.</p> <p>The prompt configuration that is returned should in most cases be the same as the one that was set. Parameters:     config: The prompt configuration to set</p> <p>Returns:</p> Type Description <code>PromptConfig</code> <p>The prompt configuration that was set</p> Source code in <code>gerd/backends/rest_client.py</code> <pre><code>@override\ndef set_gen_prompt(self, config: PromptConfig) -&gt; PromptConfig:\n    return PromptConfig.model_validate(\n        requests.post(\n            f\"{self._url}/gen/prompt\",\n            data=config.model_dump_json(),\n            timeout=self.timeout,\n        ).json()\n    )\n</code></pre>"},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.set_qa_prompt","title":"set_qa_prompt","text":"<pre><code>set_qa_prompt(config: PromptConfig, qa_mode: QAModesEnum) -&gt; QAAnswer\n</code></pre> <p>Sets the prompt configuration for the QA service.</p> <p>Since the QA service uses multiple prompt configurations, the mode should be specified. For more details, see the documentation of <code>QAService.set_prompt_config</code>.</p> <p>Parameters:</p> Name Type Description Default <code>PromptConfig</code> <p>The prompt configuration to set</p> required <code>QAModesEnum</code> <p>The mode to set the prompt configuration for</p> required <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer from the QA service</p> Source code in <code>gerd/backends/rest_client.py</code> <pre><code>@override\ndef set_qa_prompt(self, config: PromptConfig, qa_mode: QAModesEnum) -&gt; QAAnswer:\n    return QAAnswer.model_validate(\n        requests.post(\n            f\"{self._url}/qa/prompt\",\n            data=QAPromptConfig(config=config, mode=qa_mode).model_dump_json(),\n            timeout=self.timeout,\n        ).json()\n    )\n</code></pre>"},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.set_qa_prompt(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/backends/rest_client/#gerd.backends.rest_client.RestClient.set_qa_prompt(qa_mode)","title":"<code>qa_mode</code>","text":""},{"location":"reference/gerd/backends/rest_server/","title":"gerd.backends.rest_server","text":""},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server","title":"gerd.backends.rest_server","text":"<p>REST server as a GERD backend.</p> <p>Classes:</p> Name Description <code>RestServer</code> <p>REST server as a GERD backend.</p>"},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer","title":"RestServer","text":"<pre><code>RestServer()\n</code></pre> <p>               Bases: <code>Transport</code></p> <p>REST server as a GERD backend.</p> <p>The REST server initializes a private bridge and an API router.</p> <p>The API router is used to define the endpoints for the REST server.</p> <p>Methods:</p> Name Description <code>add_file</code> <p>Add a file to the vector store.</p> <code>analyze_mult_prompts_query</code> <p>Queries the vector store with a set of predefined queries.</p> <code>analyze_query</code> <p>Queries the vector store with a predefined query.</p> <code>db_embedding</code> <p>Converts a question to an embedding.</p> <code>db_query</code> <p>Queries the vector store with a question.</p> <code>generate</code> <p>Generates text with the generation service.</p> <code>get_gen_prompt</code> <p>Gets the prompt configuration for the generation service.</p> <code>get_qa_prompt</code> <p>Gets the prompt configuration for a mode of the QA service.</p> <code>get_qa_prompt_rest</code> <p>Get the QA prompt configuration.</p> <code>qa_query</code> <p>Query the QA service with a question.</p> <code>remove_file</code> <p>Remove a file from the vector store.</p> <code>set_gen_prompt</code> <p>Sets the prompt configuration for the generation service.</p> <code>set_qa_prompt</code> <p>Sets the prompt configuration for the QA service.</p> <code>set_qa_prompt_rest</code> <p>Set the QA prompt configuration.</p> Source code in <code>gerd/backends/rest_server.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"The REST server initializes a private bridge and an API router.\n\n    The API router is used to define the endpoints for the REST server.\n    \"\"\"\n    super().__init__()\n\n    prefix = CONFIG.server.api_prefix\n    self._bridge = Bridge()\n    self.router = APIRouter()\n    self.router.add_api_route(f\"{prefix}/qa/query\", self.qa_query, methods=[\"POST\"])\n    self.router.add_api_route(\n        f\"{prefix}/qa/query_analyze\", self.analyze_query, methods=[\"POST\"]\n    )\n    self.router.add_api_route(\n        f\"{prefix}/qa/query_analyze_mult_prompt\",\n        self.analyze_mult_prompts_query,\n        methods=[\"POST\"],\n    )\n    self.router.add_api_route(\n        f\"{prefix}/qa/db_query\", self.db_query, methods=[\"POST\"]\n    )\n    self.router.add_api_route(\n        f\"{prefix}/qa/db_embedding\", self.db_embedding, methods=[\"POST\"]\n    )\n    self.router.add_api_route(f\"{prefix}/qa/file\", self.add_file, methods=[\"POST\"])\n    self.router.add_api_route(\n        f\"{prefix}/qa/file\", self.remove_file, methods=[\"DELETE\"]\n    )\n    self.router.add_api_route(\n        f\"{prefix}/gen/prompt\", self.set_gen_prompt, methods=[\"POST\"]\n    )\n    self.router.add_api_route(\n        f\"{prefix}/gen/prompt\", self.get_gen_prompt, methods=[\"GET\"]\n    )\n    self.router.add_api_route(\n        f\"{prefix}/qa/prompt\", self.set_qa_prompt_rest, methods=[\"POST\"]\n    )\n    self.router.add_api_route(\n        f\"{prefix}/qa/prompt\", self.get_qa_prompt_rest, methods=[\"GET\"]\n    )\n    self.router.add_api_route(\n        f\"{prefix}/gen/generate\", self.generate, methods=[\"POST\"]\n    )\n</code></pre>"},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.add_file","title":"add_file","text":"<pre><code>add_file(file: QAFileUpload) -&gt; QAAnswer\n</code></pre> <p>Add a file to the vector store.</p> <p>The returned answer has a status code of 200 if the file was added successfully. Parameters:     file: The file to add to the vector store.</p> <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer from the QA service</p> Source code in <code>gerd/backends/rest_server.py</code> <pre><code>@override\ndef add_file(self, file: QAFileUpload) -&gt; QAAnswer:\n    return self._bridge.add_file(file)\n</code></pre>"},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.analyze_mult_prompts_query","title":"analyze_mult_prompts_query","text":"<pre><code>analyze_mult_prompts_query() -&gt; QAAnalyzeAnswer\n</code></pre> <p>Queries the vector store with a set of predefined queries.</p> <p>In contrast to <code>analyze_query</code>, this method queries the vector store with multiple prompts.</p> <p>Returns:</p> Type Description <code>QAAnalyzeAnswer</code> <p>The answer from the QA service.</p> Source code in <code>gerd/backends/rest_server.py</code> <pre><code>@override\ndef analyze_mult_prompts_query(self) -&gt; QAAnalyzeAnswer:\n    return self._bridge.qa.analyze_mult_prompts_query()\n</code></pre>"},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.analyze_query","title":"analyze_query","text":"<pre><code>analyze_query() -&gt; QAAnalyzeAnswer\n</code></pre> <p>Queries the vector store with a predefined query.</p> <p>The query should return vital information gathered from letters of discharge.</p> <p>Returns:</p> Type Description <code>QAAnalyzeAnswer</code> <p>The answer from the QA service.</p> Source code in <code>gerd/backends/rest_server.py</code> <pre><code>@override\ndef analyze_query(self) -&gt; QAAnalyzeAnswer:\n    return self._bridge.qa.analyze_query()\n</code></pre>"},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.db_embedding","title":"db_embedding","text":"<pre><code>db_embedding(question: QAQuestion) -&gt; List[float]\n</code></pre> <p>Converts a question to an embedding.</p> <p>The embedding is defined by the vector store.</p> <p>Parameters:</p> Name Type Description Default <code>QAQuestion</code> <p>The question to convert to an embedding.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>The embedding of the question</p> Source code in <code>gerd/backends/rest_server.py</code> <pre><code>@override\ndef db_embedding(self, question: QAQuestion) -&gt; List[float]:\n    return self._bridge.db_embedding(question)\n</code></pre>"},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.db_embedding(question)","title":"<code>question</code>","text":""},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.db_query","title":"db_query","text":"<pre><code>db_query(question: QAQuestion) -&gt; List[DocumentSource]\n</code></pre> <p>Queries the vector store with a question.</p> <p>Parameters:</p> Name Type Description Default <code>QAQuestion</code> <p>The question to query the vector store with.</p> required <p>Returns:</p> Type Description <code>List[DocumentSource]</code> <p>A list of document sources</p> Source code in <code>gerd/backends/rest_server.py</code> <pre><code>@override\ndef db_query(self, question: QAQuestion) -&gt; List[DocumentSource]:\n    # _LOGGER.debug(\"dq_query - request: %s\", question)\n    response = self._bridge.db_query(question)\n    # _LOGGER.debug(\"dq_query - response: %s\", response)\n    return response\n</code></pre>"},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.db_query(question)","title":"<code>question</code>","text":""},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.generate","title":"generate","text":"<pre><code>generate(parameters: Dict[str, str]) -&gt; GenResponse\n</code></pre> <p>Generates text with the generation service.</p> <p>Parameters:</p> Name Type Description Default <code>Dict[str, str]</code> <p>The parameters to generate text with</p> required <p>Returns:</p> Type Description <code>GenResponse</code> <p>The generation result</p> Source code in <code>gerd/backends/rest_server.py</code> <pre><code>@override\ndef generate(self, parameters: Dict[str, str]) -&gt; GenResponse:\n    return self._bridge.generate(parameters)\n</code></pre>"},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.generate(parameters)","title":"<code>parameters</code>","text":""},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.get_gen_prompt","title":"get_gen_prompt","text":"<pre><code>get_gen_prompt() -&gt; PromptConfig\n</code></pre> <p>Gets the prompt configuration for the generation service.</p> <p>Returns:</p> Type Description <code>PromptConfig</code> <p>The current prompt configuration</p> Source code in <code>gerd/backends/rest_server.py</code> <pre><code>@override\ndef get_gen_prompt(self) -&gt; PromptConfig:\n    return self._bridge.get_gen_prompt()\n</code></pre>"},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.get_qa_prompt","title":"get_qa_prompt","text":"<pre><code>get_qa_prompt(qa_mode: QAModesEnum) -&gt; PromptConfig\n</code></pre> <p>Gets the prompt configuration for a mode of the QA service.</p> <p>Parameters:</p> Name Type Description Default <code>QAModesEnum</code> <p>The mode to get the prompt configuration for</p> required <p>Returns:</p> Type Description <code>PromptConfig</code> <p>The prompt configuration for the QA service</p> Source code in <code>gerd/backends/rest_server.py</code> <pre><code>@override\ndef get_qa_prompt(self, qa_mode: QAModesEnum) -&gt; PromptConfig:\n    return self._bridge.get_qa_prompt(qa_mode)\n</code></pre>"},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.get_qa_prompt(qa_mode)","title":"<code>qa_mode</code>","text":""},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.get_qa_prompt_rest","title":"get_qa_prompt_rest","text":"<pre><code>get_qa_prompt_rest(qa_mode: int) -&gt; PromptConfig\n</code></pre> <p>Get the QA prompt configuration.</p> <p>The call is forwarded to the bridge. Parameters:     qa_mode: The QA mode Returns:     The QA prompt configuration</p> Source code in <code>gerd/backends/rest_server.py</code> <pre><code>def get_qa_prompt_rest(self, qa_mode: int) -&gt; PromptConfig:\n    \"\"\"Get the QA prompt configuration.\n\n    The call is forwarded to the bridge.\n    Parameters:\n        qa_mode: The QA mode\n    Returns:\n        The QA prompt configuration\n    \"\"\"\n    return self._bridge.get_qa_prompt(QAModesEnum(qa_mode))\n</code></pre>"},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.qa_query","title":"qa_query","text":"<pre><code>qa_query(question: QAQuestion) -&gt; QAAnswer\n</code></pre> <p>Query the QA service with a question.</p> <p>Parameters:</p> Name Type Description Default <code>QAQuestion</code> <p>The question to query the QA service with.</p> required <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer from the QA service.</p> Source code in <code>gerd/backends/rest_server.py</code> <pre><code>@override\ndef qa_query(self, question: QAQuestion) -&gt; QAAnswer:\n    return self._bridge.qa_query(question)\n</code></pre>"},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.qa_query(query)","title":"<code>query</code>","text":""},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.remove_file","title":"remove_file","text":"<pre><code>remove_file(file_name: str) -&gt; QAAnswer\n</code></pre> <p>Remove a file from the vector store.</p> <p>The returned answer has a status code of 200 if the file was removed successfully. Parameters:     file_name: The name of the file to remove from the vector store.</p> <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer from the QA service</p> Source code in <code>gerd/backends/rest_server.py</code> <pre><code>@override\ndef remove_file(self, file_name: str) -&gt; QAAnswer:\n    return self._bridge.remove_file(file_name)\n</code></pre>"},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.set_gen_prompt","title":"set_gen_prompt","text":"<pre><code>set_gen_prompt(config: PromptConfig) -&gt; PromptConfig\n</code></pre> <p>Sets the prompt configuration for the generation service.</p> <p>The prompt configuration that is returned should in most cases be the same as the one that was set. Parameters:     config: The prompt configuration to set</p> <p>Returns:</p> Type Description <code>PromptConfig</code> <p>The prompt configuration that was set</p> Source code in <code>gerd/backends/rest_server.py</code> <pre><code>@override\ndef set_gen_prompt(self, config: PromptConfig) -&gt; PromptConfig:\n    return self._bridge.set_gen_prompt(config)\n</code></pre>"},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.set_qa_prompt","title":"set_qa_prompt","text":"<pre><code>set_qa_prompt(config: PromptConfig, qa_mode: QAModesEnum) -&gt; QAAnswer\n</code></pre> <p>Sets the prompt configuration for the QA service.</p> <p>Since the QA service uses multiple prompt configurations, the mode should be specified. For more details, see the documentation of <code>QAService.set_prompt_config</code>.</p> <p>Parameters:</p> Name Type Description Default <code>PromptConfig</code> <p>The prompt configuration to set</p> required <code>QAModesEnum</code> <p>The mode to set the prompt configuration for</p> required <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer from the QA service</p> Source code in <code>gerd/backends/rest_server.py</code> <pre><code>@override\ndef set_qa_prompt(self, config: PromptConfig, qa_mode: QAModesEnum) -&gt; QAAnswer:\n    return self._bridge.set_qa_prompt(config, qa_mode)\n</code></pre>"},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.set_qa_prompt(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.set_qa_prompt(qa_mode)","title":"<code>qa_mode</code>","text":""},{"location":"reference/gerd/backends/rest_server/#gerd.backends.rest_server.RestServer.set_qa_prompt_rest","title":"set_qa_prompt_rest","text":"<pre><code>set_qa_prompt_rest(config: QAPromptConfig) -&gt; QAAnswer\n</code></pre> <p>Set the QA prompt configuration.</p> <p>The call is forwarded to the bridge. Parameters:     config: The QA prompt configuration Returns:     The QA prompt configuration; Should be the same as the input in most cases</p> Source code in <code>gerd/backends/rest_server.py</code> <pre><code>def set_qa_prompt_rest(self, config: QAPromptConfig) -&gt; QAAnswer:\n    \"\"\"Set the QA prompt configuration.\n\n    The call is forwarded to the bridge.\n    Parameters:\n        config: The QA prompt configuration\n    Returns:\n        The QA prompt configuration; Should be the same as the input in most cases\n    \"\"\"\n    return self._bridge.set_qa_prompt(config.config, config.mode)\n</code></pre>"},{"location":"reference/gerd/config/","title":"gerd.config","text":""},{"location":"reference/gerd/config/#gerd.config","title":"gerd.config","text":"<p>Configuration for the application.</p> <p>Classes:</p> Name Description <code>EnvVariables</code> <p>Environment variables.</p> <code>Settings</code> <p>Settings for the application.</p> <code>YamlConfig</code> <p>YAML configuration source.</p> <p>Functions:</p> Name Description <code>load_gen_config</code> <p>Load the LLM model configuration.</p> <code>load_qa_config</code> <p>Load the LLM model configuration.</p> <p>Attributes:</p> Name Type Description <code>CONFIG</code> <p>The global configuration object.</p>"},{"location":"reference/gerd/config/#gerd.config.CONFIG","title":"CONFIG  <code>module-attribute</code>","text":"<pre><code>CONFIG = Settings()\n</code></pre> <p>The global configuration object.</p>"},{"location":"reference/gerd/config/#gerd.config.EnvVariables","title":"EnvVariables","text":"<p>               Bases: <code>BaseModel</code></p> <p>Environment variables.</p>"},{"location":"reference/gerd/config/#gerd.config.Settings","title":"Settings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for the application.</p> <p>Methods:</p> Name Description <code>settings_customise_sources</code> <p>Customize the settings sources used by pydantic-settings.</p>"},{"location":"reference/gerd/config/#gerd.config.Settings.settings_customise_sources","title":"settings_customise_sources  <code>classmethod</code>","text":"<pre><code>settings_customise_sources(settings_cls: Type[BaseSettings], init_settings: PydanticBaseSettingsSource, env_settings: PydanticBaseSettingsSource, dotenv_settings: PydanticBaseSettingsSource, file_secret_settings: PydanticBaseSettingsSource) -&gt; Tuple[PydanticBaseSettingsSource, ...]\n</code></pre> <p>Customize the settings sources used by pydantic-settings.</p> <p>The order of the sources is important. The first source has the highest priority.</p> <p>Parameters:</p> Name Type Description Default <p>The class of the settings.</p> required <code>PydanticBaseSettingsSource</code> <p>The settings from the initialization.</p> required <code>PydanticBaseSettingsSource</code> <p>The settings from the environment.</p> required <code>PydanticBaseSettingsSource</code> <p>The settings from the dotenv file.</p> required <code>PydanticBaseSettingsSource</code> <p>The settings from the secret file.</p> required <p>Returns:</p> Type Description <code>Tuple[PydanticBaseSettingsSource, ...]</code> <p>The customized settings sources.</p> Source code in <code>gerd/config.py</code> <pre><code>@classmethod\ndef settings_customise_sources(\n    cls,\n    settings_cls: Type[BaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,\n    file_secret_settings: PydanticBaseSettingsSource,\n) -&gt; Tuple[PydanticBaseSettingsSource, ...]:\n    \"\"\"Customize the settings sources used by pydantic-settings.\n\n    The order of the sources is important.\n    The first source has the highest priority.\n\n    Parameters:\n        cls: The class of the settings.\n        init_settings: The settings from the initialization.\n        env_settings: The settings from the environment.\n        dotenv_settings: The settings from the dotenv file.\n        file_secret_settings: The settings from the secret file.\n\n    Returns:\n        The customized settings sources.\n    \"\"\"\n    return (\n        file_secret_settings,\n        env_settings,\n        dotenv_settings,\n        init_settings,\n        YamlConfig(settings_cls),\n    )\n</code></pre>"},{"location":"reference/gerd/config/#gerd.config.Settings.settings_customise_sources(cls)","title":"<code>cls</code>","text":""},{"location":"reference/gerd/config/#gerd.config.Settings.settings_customise_sources(init_settings)","title":"<code>init_settings</code>","text":""},{"location":"reference/gerd/config/#gerd.config.Settings.settings_customise_sources(env_settings)","title":"<code>env_settings</code>","text":""},{"location":"reference/gerd/config/#gerd.config.Settings.settings_customise_sources(dotenv_settings)","title":"<code>dotenv_settings</code>","text":""},{"location":"reference/gerd/config/#gerd.config.Settings.settings_customise_sources(file_secret_settings)","title":"<code>file_secret_settings</code>","text":""},{"location":"reference/gerd/config/#gerd.config.YamlConfig","title":"YamlConfig","text":"<p>               Bases: <code>PydanticBaseSettingsSource</code></p> <p>YAML configuration source.</p> <p>Methods:</p> Name Description <code>get_field_value</code> <p>Overrides a method from <code>PydanticBaseSettingsSource</code>.</p>"},{"location":"reference/gerd/config/#gerd.config.YamlConfig.get_field_value","title":"get_field_value","text":"<pre><code>get_field_value(field: FieldInfo, field_name: str) -&gt; Tuple[Any, str, bool]\n</code></pre> <p>Overrides a method from <code>PydanticBaseSettingsSource</code>.</p> <p>Fails if it should ever be called. Parameters:     field: The field to get the value for.     field_name: The name of the field.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Always.</p> Source code in <code>gerd/config.py</code> <pre><code>def get_field_value(\n    self, field: FieldInfo, field_name: str\n) -&gt; Tuple[Any, str, bool]:\n    \"\"\"Overrides a method from `PydanticBaseSettingsSource`.\n\n    Fails if it should ever be called.\n    Parameters:\n        field: The field to get the value for.\n        field_name: The name of the field.\n\n    Raises:\n        NotImplementedError: Always.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/gerd/config/#gerd.config.load_gen_config","title":"load_gen_config","text":"<pre><code>load_gen_config(config: str = 'gen_default') -&gt; GenerationConfig\n</code></pre> <p>Load the LLM model configuration.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the configuration.</p> <code>'gen_default'</code> <p>Returns:</p> Type Description <code>GenerationConfig</code> <p>The model configuration.</p> Source code in <code>gerd/config.py</code> <pre><code>def load_gen_config(config: str = \"gen_default\") -&gt; GenerationConfig:\n    \"\"\"Load the LLM model configuration.\n\n    Parameters:\n        config: The name of the configuration.\n\n    Returns:\n        The model configuration.\n    \"\"\"\n    config_path = (\n        Path(config)\n        if config.endswith(\"yml\")\n        else Path(PROJECT_DIR, \"config\", f\"{config}.yml\")\n    )\n    with config_path.open(\"r\", encoding=\"utf-8\") as f:\n        conf = GenerationConfig.model_validate(safe_load(f))\n    if CONFIG.env and CONFIG.env.api_token and conf.model.endpoint:\n        conf.model.endpoint.key = CONFIG.env.api_token\n    return conf\n</code></pre>"},{"location":"reference/gerd/config/#gerd.config.load_gen_config(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/config/#gerd.config.load_qa_config","title":"load_qa_config","text":"<pre><code>load_qa_config(config: str = 'qa_default') -&gt; QAConfig\n</code></pre> <p>Load the LLM model configuration.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the configuration.</p> <code>'qa_default'</code> <p>Returns:</p> Type Description <code>QAConfig</code> <p>The model configuration.</p> Source code in <code>gerd/config.py</code> <pre><code>def load_qa_config(config: str = \"qa_default\") -&gt; QAConfig:\n    \"\"\"Load the LLM model configuration.\n\n    Parameters:\n        config: The name of the configuration.\n\n    Returns:\n        The model configuration.\n    \"\"\"\n    config_path = (\n        Path(config)\n        if config.endswith(\"yml\")\n        else Path(PROJECT_DIR, \"config\", f\"{config}.yml\")\n    )\n    with config_path.open(\"r\", encoding=\"utf-8\") as f:\n        conf = QAConfig.model_validate(safe_load(f))\n\n    if CONFIG.env and CONFIG.env.api_token and conf.model.endpoint:\n        conf.model.endpoint.key = CONFIG.env.api_token\n    return conf\n</code></pre>"},{"location":"reference/gerd/config/#gerd.config.load_qa_config(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/features/","title":"gerd.features","text":""},{"location":"reference/gerd/features/#gerd.features","title":"gerd.features","text":"<p>Special features to extend the functionality of GERD services.</p> <p>Modules:</p> Name Description <code>prompt_chaining</code> <p>The prompt chaining extension.</p>"},{"location":"reference/gerd/features/prompt_chaining/","title":"gerd.features.prompt_chaining","text":""},{"location":"reference/gerd/features/prompt_chaining/#gerd.features.prompt_chaining","title":"gerd.features.prompt_chaining","text":"<p>The prompt chaining extension.</p> <p>Prompt chaining is a method to improve the factual accuracy of the model's output. To do this, the model generates a series of prompts and uses the output of each prompt as the input for the next prompt. This allows the model to reflect on its own output and generate a more coherent response.</p> <p>Classes:</p> Name Description <code>PromptChaining</code> <p>The prompt chaining extension.</p> <code>PromptChainingConfig</code> <p>Configuration for prompt chaining.</p>"},{"location":"reference/gerd/features/prompt_chaining/#gerd.features.prompt_chaining.PromptChaining","title":"PromptChaining","text":"<pre><code>PromptChaining(config: PromptChainingConfig, llm: LLM, prompt: PromptConfig)\n</code></pre> <p>The prompt chaining extension.</p> <p>The service is initialized with a chaining configuration and an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>PromptChainingConfig</code> <p>The configuration for the prompt chaining</p> required <code>LLM</code> <p>The language model to use for the generation</p> required <code>PromptConfig</code> <p>The prompt that is used to wrap the questions</p> required <p>Methods:</p> Name Description <code>generate</code> <p>Generate text based on the prompt configuration and use chaining.</p> Source code in <code>gerd/features/prompt_chaining.py</code> <pre><code>def __init__(\n    self, config: PromptChainingConfig, llm: LLM, prompt: PromptConfig\n) -&gt; None:\n    \"\"\"The service is initialized with a chaining configuration and an LLM.\n\n    Parameters:\n        config: The configuration for the prompt chaining\n        llm: The language model to use for the generation\n        prompt: The prompt that is used to wrap the questions\n    \"\"\"\n    self.llm = llm\n    self.config = config\n    self.prompt = prompt\n</code></pre>"},{"location":"reference/gerd/features/prompt_chaining/#gerd.features.prompt_chaining.PromptChaining(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/features/prompt_chaining/#gerd.features.prompt_chaining.PromptChaining(llm)","title":"<code>llm</code>","text":""},{"location":"reference/gerd/features/prompt_chaining/#gerd.features.prompt_chaining.PromptChaining(prompt)","title":"<code>prompt</code>","text":""},{"location":"reference/gerd/features/prompt_chaining/#gerd.features.prompt_chaining.PromptChaining.generate","title":"generate","text":"<pre><code>generate(parameters: dict[str, str]) -&gt; str\n</code></pre> <p>Generate text based on the prompt configuration and use chaining.</p> <p>Parameters:</p> Name Type Description Default <code>dict[str, str]</code> <p>The parameters to format the prompt with</p> required <p>Returns:</p> Type Description <code>str</code> <p>The result of the last prompt that was chained</p> Source code in <code>gerd/features/prompt_chaining.py</code> <pre><code>def generate(self, parameters: dict[str, str]) -&gt; str:\n    \"\"\"Generate text based on the prompt configuration and use chaining.\n\n    Parameters:\n        parameters: The parameters to format the prompt with\n\n    Returns:\n        The result of the last prompt that was chained\n    \"\"\"\n    res = \"\"\n    for i, prompt in enumerate(self.config.prompts, 1):\n        resolved = self.prompt.format({\"prompt\": prompt.format(parameters)})\n        _LOGGER.debug(\"\\n===== Input =====\\n\\n%s\\n\\n====================\", resolved)\n        res = self.llm.generate(resolved).strip()\n        _LOGGER.debug(\"\\n===== Response =====\\n\\n%s\\n\\n====================\", res)\n        parameters[f\"response_{i}\"] = res\n    return res\n</code></pre>"},{"location":"reference/gerd/features/prompt_chaining/#gerd.features.prompt_chaining.PromptChaining.generate(parameters)","title":"<code>parameters</code>","text":""},{"location":"reference/gerd/features/prompt_chaining/#gerd.features.prompt_chaining.PromptChainingConfig","title":"PromptChainingConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for prompt chaining.</p> <p>Note that prompts should contain placeholders for the responses to be inserted. The initial question can be used with <code>{question}</code> if it is passed as a parameter with this key. The responses will be indexed as <code>response_1</code>, <code>response_2</code>, etc. This way any prompt can refer to all previous responses and the initial question if needed.</p> <p>Attributes:</p> Name Type Description <code>prompts</code> <code>list[PromptConfig]</code> <p>The list of prompts to chain.</p>"},{"location":"reference/gerd/features/prompt_chaining/#gerd.features.prompt_chaining.PromptChainingConfig.prompts","title":"prompts  <code>instance-attribute</code>","text":"<pre><code>prompts: list[PromptConfig]\n</code></pre> <p>The list of prompts to chain.</p>"},{"location":"reference/gerd/frontends/","title":"gerd.frontends","text":""},{"location":"reference/gerd/frontends/#gerd.frontends","title":"gerd.frontends","text":"<p>A collection of several gradio frontends.</p> <p>A variety of frontends to interact with GERD services and backends.</p> <p>Modules:</p> Name Description <code>gen_frontend</code> <p>A gradio frontend to interact with the generation service.</p> <code>generate</code> <p>A simple gradio frontend to interact with the GERD chat and generate service.</p> <code>instruct</code> <p>A gradio frontend to interact with the GERD instruct service.</p> <code>qa_frontend</code> <p>A gradio frontend to query the QA service and upload files to the vectorstore.</p> <code>router</code> <p>A gradio frontend to start and stop the GERD services.</p> <code>training</code> <p>A gradio frontend to train LoRAs with.</p>"},{"location":"reference/gerd/frontends/gen_frontend/","title":"gerd.frontends.gen_frontend","text":""},{"location":"reference/gerd/frontends/gen_frontend/#gerd.frontends.gen_frontend","title":"gerd.frontends.gen_frontend","text":"<p>A gradio frontend to interact with the generation service.</p> <p>This frontend is tailored to the letter of discharge generation task. For a more general frontend see <code>gerd.frontend.generate</code>.</p> <p>Functions:</p> Name Description <code>compare_paragraphs</code> <p>Compare paragraphs of two documents and return the modified parts.</p> <code>generate</code> <p>Generate a letter of discharge based on the provided fields.</p> <code>insert_paragraphs</code> <p>Insert modified paragraphs into the source document.</p> <code>response_parser</code> <p>Parse the response from the generation service.</p>"},{"location":"reference/gerd/frontends/gen_frontend/#gerd.frontends.gen_frontend.compare_paragraphs","title":"compare_paragraphs","text":"<pre><code>compare_paragraphs(src_doc: str, mod_doc: str) -&gt; Dict[str, str]\n</code></pre> <p>Compare paragraphs of two documents and return the modified parts.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The source document</p> required <code>str</code> <p>The modified document</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>The modified parts of the document</p> Source code in <code>gerd/frontends/gen_frontend.py</code> <pre><code>def compare_paragraphs(src_doc: str, mod_doc: str) -&gt; Dict[str, str]:\n    \"\"\"Compare paragraphs of two documents and return the modified parts.\n\n    Parameters:\n        src_doc: The source document\n        mod_doc: The modified document\n\n    Returns:\n        The modified parts of the document\n    \"\"\"\n    mod_parts = {}\n    src_doc_split = src_doc.split(\"\\n\\n\")\n    mod_doc_split = mod_doc.split(\"\\n\\n\")\n    for section_order, src_para in zip(sections, src_doc_split, strict=True):\n        mod_para = mod_doc_split[sections.index(section_order)]\n        if src_para != mod_para:\n            mod_parts[section_order] = mod_para\n    return mod_parts\n</code></pre>"},{"location":"reference/gerd/frontends/gen_frontend/#gerd.frontends.gen_frontend.compare_paragraphs(src_doc)","title":"<code>src_doc</code>","text":""},{"location":"reference/gerd/frontends/gen_frontend/#gerd.frontends.gen_frontend.compare_paragraphs(mod_doc)","title":"<code>mod_doc</code>","text":""},{"location":"reference/gerd/frontends/gen_frontend/#gerd.frontends.gen_frontend.generate","title":"generate","text":"<pre><code>generate(*fields: str) -&gt; Tuple[str, str, gr.TextArea, gr.Button]\n</code></pre> <p>Generate a letter of discharge based on the provided fields.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The fields to generate the letter of discharge from.</p> <code>()</code> <p>Returns:</p> Type Description <code>str</code> <p>The generated letter of discharge, a text area to display it,</p> <code>str</code> <p>and a button state to continue the generation</p> Source code in <code>gerd/frontends/gen_frontend.py</code> <pre><code>def generate(*fields: str) -&gt; Tuple[str, str, gr.TextArea, gr.Button]:\n    \"\"\"Generate a letter of discharge based on the provided fields.\n\n    Parameters:\n        *fields: The fields to generate the letter of discharge from.\n\n    Returns:\n        The generated letter of discharge, a text area to display it,\n        and a button state to continue the generation\n    \"\"\"\n    params = {}\n    for key, name, value in _pairwise(fields):\n        if not value:\n            msg = f\"Feld '{name}' darf nicht leer sein!\"\n            raise gr.Error(msg)\n        params[key] = value\n    response = TRANSPORTER.generate(params)\n    return (\n        response.text,\n        response.text,\n        gr.TextArea(label=\"Dokument\", interactive=True),\n        gr.Button(\"Kontinuiere Dokument\", visible=True),\n    )\n</code></pre>"},{"location":"reference/gerd/frontends/gen_frontend/#gerd.frontends.gen_frontend.generate(*fields)","title":"<code>*fields</code>","text":""},{"location":"reference/gerd/frontends/gen_frontend/#gerd.frontends.gen_frontend.insert_paragraphs","title":"insert_paragraphs","text":"<pre><code>insert_paragraphs(src_doc: str, new_para: Dict[str, str]) -&gt; str\n</code></pre> <p>Insert modified paragraphs into the source document.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The source document</p> required <code>Dict[str, str]</code> <p>The modified paragraphs</p> required <p>Returns:</p> Type Description <code>str</code> <p>The updated document</p> Source code in <code>gerd/frontends/gen_frontend.py</code> <pre><code>def insert_paragraphs(src_doc: str, new_para: Dict[str, str]) -&gt; str:\n    \"\"\"Insert modified paragraphs into the source document.\n\n    Parameters:\n        src_doc: The source document\n        new_para: The modified paragraphs\n\n    Returns:\n        The updated document\n    \"\"\"\n    for section_order, mod_para in new_para.items():\n        split_doc = src_doc.split(\"\\n\\n\")[sections.index(section_order)]\n        src_doc = src_doc.replace(split_doc, mod_para)\n    return src_doc\n</code></pre>"},{"location":"reference/gerd/frontends/gen_frontend/#gerd.frontends.gen_frontend.insert_paragraphs(src_doc)","title":"<code>src_doc</code>","text":""},{"location":"reference/gerd/frontends/gen_frontend/#gerd.frontends.gen_frontend.insert_paragraphs(new_para)","title":"<code>new_para</code>","text":""},{"location":"reference/gerd/frontends/gen_frontend/#gerd.frontends.gen_frontend.response_parser","title":"response_parser","text":"<pre><code>response_parser(response: str) -&gt; Dict[str, str]\n</code></pre> <p>Parse the response from the generation service.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The response from the generation service</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>The parsed response</p> Source code in <code>gerd/frontends/gen_frontend.py</code> <pre><code>def response_parser(response: str) -&gt; Dict[str, str]:\n    \"\"\"Parse the response from the generation service.\n\n    Parameters:\n        response: The response from the generation service\n\n    Returns:\n        The parsed response\n    \"\"\"\n    parsed_response = {}\n    split_response = response.split(\"\\n\\n\")\n    for paragraph in split_response:\n        for section in sections:\n            if section in paragraph:\n                parsed_response[section] = paragraph\n                break\n    return parsed_response\n</code></pre>"},{"location":"reference/gerd/frontends/gen_frontend/#gerd.frontends.gen_frontend.response_parser(response)","title":"<code>response</code>","text":""},{"location":"reference/gerd/frontends/generate/","title":"gerd.frontends.generate","text":""},{"location":"reference/gerd/frontends/generate/#gerd.frontends.generate","title":"gerd.frontends.generate","text":"<p>A simple gradio frontend to interact with the GERD chat and generate service.</p> <p>Classes:</p> Name Description <code>Global</code> <p>Singleton to store the service.</p> <p>Functions:</p> Name Description <code>generate</code> <p>Generate text from the model.</p> <code>load_model</code> <p>Load a global large language model.</p> <code>upload_lora</code> <p>Upload a LoRA archive.</p> <p>Attributes:</p> Name Type Description <code>KIOSK_MODE</code> <p>Whether the frontend is running in kiosk mode.</p>"},{"location":"reference/gerd/frontends/generate/#gerd.frontends.generate.KIOSK_MODE","title":"KIOSK_MODE  <code>module-attribute</code>","text":"<pre><code>KIOSK_MODE = kiosk_mode\n</code></pre> <p>Whether the frontend is running in kiosk mode.</p> <p>Kiosk mode reduces the number of options to a minimum and automatically loads the model.</p>"},{"location":"reference/gerd/frontends/generate/#gerd.frontends.generate.Global","title":"Global","text":"<p>Singleton to store the service.</p>"},{"location":"reference/gerd/frontends/generate/#gerd.frontends.generate.generate","title":"generate","text":"<pre><code>generate(textbox: str, temp: float, top_p: float, max_tokens: int) -&gt; str\n</code></pre> <p>Generate text from the model.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The text to generate from</p> required <code>float</code> <p>The temperature for the generation</p> required <code>float</code> <p>The top p value for the generation</p> required <code>int</code> <p>The maximum number of tokens to generate</p> required <p>Returns:</p> Type Description <code>str</code> <p>The generated text</p> Source code in <code>gerd/frontends/generate.py</code> <pre><code>def generate(textbox: str, temp: float, top_p: float, max_tokens: int) -&gt; str:\n    \"\"\"Generate text from the model.\n\n    Parameters:\n        textbox: The text to generate from\n        temp: The temperature for the generation\n        top_p: The top p value for the generation\n        max_tokens: The maximum number of tokens to generate\n\n    Returns:\n        The generated text\n    \"\"\"\n    if Global.service is None:\n        msg = \"Model not loaded\"\n        raise gr.Error(msg)\n    Global.service.config.model.top_p = top_p\n    Global.service.config.model.temperature = temp\n    Global.service.config.model.max_new_tokens = max_tokens\n    return Global.service.generate({\"message\": textbox}).text\n</code></pre>"},{"location":"reference/gerd/frontends/generate/#gerd.frontends.generate.generate(textbox)","title":"<code>textbox</code>","text":""},{"location":"reference/gerd/frontends/generate/#gerd.frontends.generate.generate(temp)","title":"<code>temp</code>","text":""},{"location":"reference/gerd/frontends/generate/#gerd.frontends.generate.generate(top_p)","title":"<code>top_p</code>","text":""},{"location":"reference/gerd/frontends/generate/#gerd.frontends.generate.generate(max_tokens)","title":"<code>max_tokens</code>","text":""},{"location":"reference/gerd/frontends/generate/#gerd.frontends.generate.load_model","title":"load_model","text":"<pre><code>load_model(model_name: str, origin: str) -&gt; dict[str, Any]\n</code></pre> <p>Load a global large language model.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the model</p> required <code>str</code> <p>Whether to use an extra LoRA</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The updated interactive state, returns interactive=True when the model is loaded</p> Source code in <code>gerd/frontends/generate.py</code> <pre><code>def load_model(model_name: str, origin: str) -&gt; dict[str, Any]:\n    \"\"\"Load a global large language model.\n\n    Parameters:\n        model_name: The name of the model\n        origin: Whether to use an extra LoRA\n\n    Returns:\n        The updated interactive state, returns interactive=True when the model is loaded\n    \"\"\"\n    if Global.service is not None:\n        del Global.service\n    model_config = config.model_copy(deep=True)\n    model_config.model.name = model_name\n    if origin != \"None\":\n        model_config.model.loras.add(lora_dir / origin)\n    Global.service = ChatService(model_config)\n    return gr.update(interactive=True)\n</code></pre>"},{"location":"reference/gerd/frontends/generate/#gerd.frontends.generate.load_model(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/gerd/frontends/generate/#gerd.frontends.generate.load_model(origin)","title":"<code>origin</code>","text":""},{"location":"reference/gerd/frontends/generate/#gerd.frontends.generate.upload_lora","title":"upload_lora","text":"<pre><code>upload_lora(file_upload: str) -&gt; None\n</code></pre> <p>Upload a LoRA archive.</p> <p>Returns None to reset file upload state.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The path to the uploaded archive</p> required Source code in <code>gerd/frontends/generate.py</code> <pre><code>def upload_lora(file_upload: str) -&gt; None:\n    \"\"\"Upload a LoRA archive.\n\n    Returns None to reset file upload state.\n\n    Parameters:\n        file_upload: The path to the uploaded archive\n    \"\"\"\n    if file_upload is None:\n        gr.Warning(\"No file uploaded\")\n    p = Path(file_upload)\n    if p.exists() is False:\n        gr.Warning(\"File does not exist\")\n    if (lora_dir / p.stem).exists():\n        _LOGGER.info(\"Removing existing '%s'\", p.stem)\n        shutil.rmtree(lora_dir / p.stem)\n    shutil.unpack_archive(p.as_posix(), lora_dir / p.stem)\n    gr.Success(f\"Uploaded '{p.stem}'\")\n</code></pre>"},{"location":"reference/gerd/frontends/generate/#gerd.frontends.generate.upload_lora(file_upload)","title":"<code>file_upload</code>","text":""},{"location":"reference/gerd/frontends/instruct/","title":"gerd.frontends.instruct","text":""},{"location":"reference/gerd/frontends/instruct/#gerd.frontends.instruct","title":"gerd.frontends.instruct","text":"<p>A gradio frontend to interact with the GERD instruct service.</p> <p>Classes:</p> Name Description <code>Global</code> <p>Singleton to store the service.</p> <p>Functions:</p> Name Description <code>generate</code> <p>Generate text from the model.</p> <code>load_model</code> <p>Load a global large language model.</p> <p>Attributes:</p> Name Type Description <code>KIOSK_MODE</code> <p>Whether the frontend is running in kiosk mode.</p>"},{"location":"reference/gerd/frontends/instruct/#gerd.frontends.instruct.KIOSK_MODE","title":"KIOSK_MODE  <code>module-attribute</code>","text":"<pre><code>KIOSK_MODE = kiosk_mode\n</code></pre> <p>Whether the frontend is running in kiosk mode.</p> <p>Kiosk mode reduces the number of options to a minimum and automatically loads the model.</p>"},{"location":"reference/gerd/frontends/instruct/#gerd.frontends.instruct.Global","title":"Global","text":"<p>Singleton to store the service.</p>"},{"location":"reference/gerd/frontends/instruct/#gerd.frontends.instruct.generate","title":"generate","text":"<pre><code>generate(temperature: float, top_p: float, max_tokens: int, system_text: str, *args: str) -&gt; str\n</code></pre> <p>Generate text from the model.</p> <p>Parameters:</p> Name Type Description Default <code>float</code> <p>The temperature for the generation</p> required <code>float</code> <p>The top-p value for the generation</p> required <code>int</code> <p>The maximum number of tokens to generate</p> required <code>str</code> <p>The system text to set up the context</p> required <code>str</code> <p>The user input</p> <code>()</code> Source code in <code>gerd/frontends/instruct.py</code> <pre><code>def generate(\n    temperature: float,\n    top_p: float,\n    max_tokens: int,\n    system_text: str,\n    *args: str,\n) -&gt; str:\n    \"\"\"Generate text from the model.\n\n    Parameters:\n        temperature: The temperature for the generation\n        top_p: The top-p value for the generation\n        max_tokens: The maximum number of tokens to generate\n        system_text: The system text to set up the context\n        args: The user input\n    \"\"\"\n    fields = dict(zip(config.model.prompt_config.parameters, args, strict=True))\n    if Global.service is None:\n        msg = \"Model not loaded\"\n        raise gr.Error(msg)\n    if system_text:\n        Global.service.config.model.prompt_setup = [\n            (\"system\", PromptConfig(text=system_text))\n        ]\n    Global.service.config.model.top_p = top_p\n    Global.service.config.model.temperature = temperature\n    Global.service.config.model.max_new_tokens = max_tokens\n    Global.service.reset()\n    return Global.service.submit_user_message(fields).text\n</code></pre>"},{"location":"reference/gerd/frontends/instruct/#gerd.frontends.instruct.generate(temperature)","title":"<code>temperature</code>","text":""},{"location":"reference/gerd/frontends/instruct/#gerd.frontends.instruct.generate(top_p)","title":"<code>top_p</code>","text":""},{"location":"reference/gerd/frontends/instruct/#gerd.frontends.instruct.generate(max_tokens)","title":"<code>max_tokens</code>","text":""},{"location":"reference/gerd/frontends/instruct/#gerd.frontends.instruct.generate(system_text)","title":"<code>system_text</code>","text":""},{"location":"reference/gerd/frontends/instruct/#gerd.frontends.instruct.generate(args)","title":"<code>args</code>","text":""},{"location":"reference/gerd/frontends/instruct/#gerd.frontends.instruct.load_model","title":"load_model","text":"<pre><code>load_model(model_name: str, origin: str = 'None') -&gt; dict[str, Any]\n</code></pre> <p>Load a global large language model.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the model</p> required <code>str</code> <p>Whether to use an extra LoRA</p> <code>'None'</code> Source code in <code>gerd/frontends/instruct.py</code> <pre><code>def load_model(model_name: str, origin: str = \"None\") -&gt; dict[str, Any]:\n    \"\"\"Load a global large language model.\n\n    Parameters:\n        model_name: The name of the model\n        origin: Whether to use an extra LoRA\n    \"\"\"\n    if Global.service is not None:\n        _LOGGER.debug(\"Unloading model\")\n        del Global.service\n    model_config = config.model_copy()\n    model_config.model.name = model_name\n    if origin != \"None\" and (lora_dir / origin).is_dir():\n        model_config.model.loras.add(lora_dir / origin)\n    _LOGGER.info(\"Loading model %s\", model_config.model.name)\n    Global.service = ChatService(model_config)\n    _LOGGER.info(\"Model loaded\")\n    return gr.update(interactive=True)\n</code></pre>"},{"location":"reference/gerd/frontends/instruct/#gerd.frontends.instruct.load_model(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/gerd/frontends/instruct/#gerd.frontends.instruct.load_model(origin)","title":"<code>origin</code>","text":""},{"location":"reference/gerd/frontends/qa_frontend/","title":"gerd.frontends.qa_frontend","text":""},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend","title":"gerd.frontends.qa_frontend","text":"<p>A gradio frontend to query the QA service and upload files to the vectorstore.</p> <p>Functions:</p> Name Description <code>files_changed</code> <p>Check if the file upload element has changed.</p> <code>get_qa_mode</code> <p>Get QAMode from string.</p> <code>handle_developer_mode_checkbox_change</code> <p>Enable/disable developermode.</p> <code>handle_type_radio_selection_change</code> <p>Enable/disable gui elements depend on which mode is selected.</p> <code>query</code> <p>Starts the selected QA Mode.</p> <code>set_prompt</code> <p>Updates the prompt of the selected QA Mode.</p>"},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.files_changed","title":"files_changed","text":"<pre><code>files_changed(file_paths: Optional[list[str]]) -&gt; None\n</code></pre> <p>Check if the file upload element has changed.</p> <p>If so, upload the new files to the vectorstore and delete the one that have been removed.</p> <p>Parameters:</p> Name Type Description Default <code>Optional[list[str]]</code> <p>The file paths to upload</p> required Source code in <code>gerd/frontends/qa_frontend.py</code> <pre><code>def files_changed(file_paths: Optional[list[str]]) -&gt; None:\n    \"\"\"Check if the file upload element has changed.\n\n    If so, upload the new files to the vectorstore and delete the one that\n    have been removed.\n\n    Parameters:\n        file_paths: The file paths to upload\n    \"\"\"\n    file_paths = file_paths or []\n    progress = gr.Progress()\n    new_set = set(file_paths)\n    new_files = new_set - store_set\n    delete_files = store_set - new_set\n    for new_file in new_files:\n        store_set.add(new_file)\n        with pathlib.Path(new_file).open(\"rb\") as file:\n            data = QAFileUpload(\n                data=file.read(),\n                name=pathlib.Path(new_file).name,\n            )\n        res = TRANSPORTER.add_file(data)\n        if res.status != 200:\n            _LOGGER.warning(\n                \"Data upload failed with error code: %d\\nReason: %s\",\n                res.status,\n                res.error_msg,\n            )\n            msg = (\n                f\"Datei konnte nicht hochgeladen werden: {res.error_msg}\"\n                \"(Error Code {res.status})\"\n            )\n            raise gr.Error(msg)\n    for delete_file in delete_files:\n        store_set.remove(delete_file)\n        res = TRANSPORTER.remove_file(pathlib.Path(delete_file).name)\n    progress(100, desc=\"Fertig!\")\n</code></pre>"},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.files_changed(file_paths)","title":"<code>file_paths</code>","text":""},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.get_qa_mode","title":"get_qa_mode","text":"<pre><code>get_qa_mode(search_type: str) -&gt; QAModesEnum\n</code></pre> <p>Get QAMode from string.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The search type</p> required <p>Returns:</p> Type Description <code>QAModesEnum</code> <p>The QAMode</p> Source code in <code>gerd/frontends/qa_frontend.py</code> <pre><code>def get_qa_mode(search_type: str) -&gt; QAModesEnum:\n    \"\"\"Get QAMode from string.\n\n    Parameters:\n        search_type: The search type\n\n    Returns:\n        The QAMode\n    \"\"\"\n    if search_type in qa_modes_dict:\n        return qa_modes_dict[search_type]\n    else:\n        return QAModesEnum.NONE\n</code></pre>"},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.get_qa_mode(search_type)","title":"<code>search_type</code>","text":""},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.handle_developer_mode_checkbox_change","title":"handle_developer_mode_checkbox_change","text":"<pre><code>handle_developer_mode_checkbox_change(check: bool) -&gt; List[Any]\n</code></pre> <p>Enable/disable developermode.</p> <p>Enables or disables the developer mode and the corresponding GUI elements. Parameters:     check: The current state of the developer mode checkbox Returns:     The list of GUI element property changes to update</p> Source code in <code>gerd/frontends/qa_frontend.py</code> <pre><code>def handle_developer_mode_checkbox_change(check: bool) -&gt; List[Any]:\n    \"\"\"Enable/disable developermode.\n\n    Enables or disables the developer mode and the corresponding GUI elements.\n    Parameters:\n        check: The current state of the developer mode checkbox\n    Returns:\n        The list of GUI element property changes to update\n    \"\"\"\n    return [\n        gr.update(visible=check),\n        gr.update(visible=check),\n        gr.update(visible=check),\n        gr.update(visible=check),\n        gr.update(\n            choices=(\n                [\"LLM\", \"Analyze\", \"Analyze mult.\", \"VectorDB\"]\n                if check\n                else [\"LLM\", \"Analyze mult.\"]\n            )\n        ),\n    ]\n</code></pre>"},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.handle_type_radio_selection_change","title":"handle_type_radio_selection_change","text":"<pre><code>handle_type_radio_selection_change(search_type: str) -&gt; List[Any]\n</code></pre> <p>Enable/disable gui elements depend on which mode is selected.</p> This order of the updates elements must be considered <ul> <li>input text</li> <li>prompt</li> <li>k_slider</li> <li>search strategy_dropdown</li> </ul> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The current search type</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of GUI element property changes to update</p> Source code in <code>gerd/frontends/qa_frontend.py</code> <pre><code>def handle_type_radio_selection_change(search_type: str) -&gt; List[Any]:\n    \"\"\"Enable/disable gui elements depend on which mode is selected.\n\n    This order of the updates elements must be considered:\n        - input text\n        - prompt\n        - k_slider\n        - search strategy_dropdown\n\n    Parameters:\n        search_type: The current search type\n\n    Returns:\n        The list of GUI element property changes to update\n    \"\"\"\n    if search_type == \"LLM\":\n        return [\n            gr.update(interactive=True, placeholder=\"Wie hei\u00dft der Patient?\"),\n            gr.update(value=TRANSPORTER.get_qa_prompt(get_qa_mode(search_type)).text),\n            gr.update(interactive=False),\n            gr.update(interactive=False),\n        ]\n    elif search_type == \"VectorDB\":\n        return [\n            gr.update(interactive=True, placeholder=\"Wie hei\u00dft der Patient?\"),\n            gr.update(value=TRANSPORTER.get_qa_prompt(get_qa_mode(search_type)).text),\n            gr.update(interactive=True),\n            gr.update(interactive=True),\n        ]\n\n    return [\n        gr.update(interactive=False, placeholder=\"\"),\n        gr.update(value=TRANSPORTER.get_qa_prompt(get_qa_mode(search_type)).text),\n        gr.update(interactive=False),\n        gr.update(interactive=False),\n    ]\n</code></pre>"},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.handle_type_radio_selection_change(search_type)","title":"<code>search_type</code>","text":""},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.query","title":"query","text":"<pre><code>query(question: str, search_type: str, k_source: int, search_strategy: str) -&gt; str\n</code></pre> <p>Starts the selected QA Mode.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The question to ask</p> required <code>str</code> <p>The search type</p> required <code>int</code> <p>The number of sources</p> required <code>str</code> <p>The search strategy</p> required <p>Returns:</p> Type Description <code>str</code> <p>The response from the QA service</p> Source code in <code>gerd/frontends/qa_frontend.py</code> <pre><code>def query(question: str, search_type: str, k_source: int, search_strategy: str) -&gt; str:\n    \"\"\"Starts the selected QA Mode.\n\n    Parameters:\n        question: The question to ask\n        search_type: The search type\n        k_source: The number of sources\n        search_strategy: The search strategy\n\n    Returns:\n        The response from the QA service\n    \"\"\"\n    q = QAQuestion(\n        question=question, search_strategy=search_strategy, max_sources=k_source\n    )\n    # start search mode\n    if search_type == \"LLM\":\n        qa_res = TRANSPORTER.qa_query(q)\n        if qa_res.status != 200:\n            msg = (\n                f\"Query was unsuccessful: {qa_res.error_msg}\"\n                f\" (Error Code {qa_res.status})\"\n            )\n            raise gr.Error(msg)\n        return qa_res.response\n    # start analyze mode\n    elif search_type == \"Analyze\":\n        qa_analyze_res = TRANSPORTER.analyze_query()\n        if qa_analyze_res.status != 200:\n            msg = (\n                f\"Query was unsuccessful: {qa_analyze_res.error_msg}\"\n                f\" (Error Code {qa_analyze_res.status})\"\n            )\n            raise gr.Error(msg)\n        # remove unwanted fields from answer\n        qa_res_dic = {\n            key: value\n            for key, value in vars(qa_analyze_res).items()\n            if value is not None\n            and value != \"\"\n            and key not in qa_analyze_res.__class__.__dict__\n            and key != \"sources\"\n            and key != \"status\"\n            and key != \"response\"\n            and key != \"prompt\"\n        }\n        qa_res_str = \", \".join(f\"{key}={value}\" for key, value in qa_res_dic.items())\n        return qa_res_str\n    # start analyze mult prompts mode\n    elif search_type == \"Analyze mult.\":\n        qa_analyze_mult_res = TRANSPORTER.analyze_mult_prompts_query()\n        if qa_analyze_mult_res.status != 200:\n            msg = (\n                f\"Query was unsuccessful: {qa_analyze_mult_res.error_msg}\"\n                f\" (Error Code {qa_analyze_mult_res.status})\"\n            )\n            raise gr.Error(msg)\n        # remove unwanted fields from answer\n        qa_res_dic = {\n            key: value\n            for key, value in vars(qa_analyze_mult_res).items()\n            if value is not None\n            and value != \"\"\n            and key not in qa_analyze_mult_res.__class__.__dict__\n            and key != \"sources\"\n            and key != \"status\"\n            and key != \"response\"\n            and key != \"prompt\"\n        }\n        qa_res_str = \", \".join(f\"{key}={value}\" for key, value in qa_res_dic.items())\n        return qa_res_str\n    # start db search mode\n    db_res = TRANSPORTER.db_query(q)\n    if not db_res:\n        msg = f\"Database query returned empty!\"\n        raise gr.Error(msg)\n    output = \"\"\n    for doc in db_res:\n        output += f\"{doc.content}\\n\"\n        output += f\"({doc.name} / {doc.page})\\n----------\\n\\n\"\n    return output\n</code></pre>"},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.query(question)","title":"<code>question</code>","text":""},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.query(search_type)","title":"<code>search_type</code>","text":""},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.query(k_source)","title":"<code>k_source</code>","text":""},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.query(search_strategy)","title":"<code>search_strategy</code>","text":""},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.set_prompt","title":"set_prompt","text":"<pre><code>set_prompt(prompt: str, search_type: str, progress: Optional[gr.Progress] = None) -&gt; None\n</code></pre> <p>Updates the prompt of the selected QA Mode.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The new prompt</p> required <code>str</code> <p>The search type</p> required <code>Optional[Progress]</code> <p>The progress bar to update</p> <code>None</code> Source code in <code>gerd/frontends/qa_frontend.py</code> <pre><code>def set_prompt(\n    prompt: str, search_type: str, progress: Optional[gr.Progress] = None\n) -&gt; None:\n    \"\"\"Updates the prompt of the selected QA Mode.\n\n    Parameters:\n        prompt: The new prompt\n        search_type: The search type\n        progress: The progress bar to update\n    \"\"\"\n    if progress is None:\n        progress = gr.Progress()\n    progress(0, \"Aktualisiere Prompt...\")\n\n    answer = TRANSPORTER.set_qa_prompt(\n        PromptConfig(text=prompt), get_qa_mode(search_type)\n    )\n    if answer.error_msg:\n        if answer.status != 200:\n            msg = (\n                f\"Prompt konnte nicht aktualisiert werden: {answer.error_msg}\"\n                f\" (Error Code {answer.status})\"\n            )\n            raise gr.Error(msg)\n        else:\n            msg = f\"{answer.error_msg}\"\n            gr.Warning(msg)\n    else:\n        gr.Info(\"Prompt wurde aktualisiert!\")\n    progress(100, \"Fertig!\")\n</code></pre>"},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.set_prompt(prompt)","title":"<code>prompt</code>","text":""},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.set_prompt(search_type)","title":"<code>search_type</code>","text":""},{"location":"reference/gerd/frontends/qa_frontend/#gerd.frontends.qa_frontend.set_prompt(progress)","title":"<code>progress</code>","text":""},{"location":"reference/gerd/frontends/router/","title":"gerd.frontends.router","text":""},{"location":"reference/gerd/frontends/router/#gerd.frontends.router","title":"gerd.frontends.router","text":"<p>A gradio frontend to start and stop the GERD services.</p> <p>Since most hosts that use a frontend will not have enough memory to run multiple services at the same time this router is used to start and stop the services as needed.</p> <p>Classes:</p> Name Description <code>AppController</code> <p>The controller for the app.</p> <code>AppState</code> <p>The state of the service.</p> <p>Functions:</p> Name Description <code>check_state</code> <p>Checks the app state and waits for the service to start.</p> <p>Attributes:</p> Name Type Description <code>GRADIO_ROUTER_PORT</code> <p>The port the router is running on.</p> <code>GRADIO_SERVER_PORT</code> <p>The port the gradio server is running on.</p>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.GRADIO_ROUTER_PORT","title":"GRADIO_ROUTER_PORT  <code>module-attribute</code>","text":"<pre><code>GRADIO_ROUTER_PORT = get('GRADIO_ROUTER_PORT', '7860')\n</code></pre> <p>The port the router is running on.</p>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.GRADIO_SERVER_PORT","title":"GRADIO_SERVER_PORT  <code>module-attribute</code>","text":"<pre><code>GRADIO_SERVER_PORT = get('GRADIO_SERVER_PORT', '12121')\n</code></pre> <p>The port the gradio server is running on.</p>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppController","title":"AppController","text":"<pre><code>AppController()\n</code></pre> <p>The controller for the app.</p> <p>The controlller is initialized in the stopped state.</p> <p>Methods:</p> Name Description <code>check_port</code> <p>Check if the service port is open.</p> <code>instance</code> <p>Get the instance of the controller.</p> <code>start</code> <p>Start the service with the given frontend.</p> <code>start_gen</code> <p>Start the generation service.</p> <code>start_instruct</code> <p>Start the instruct service.</p> <code>start_qa</code> <p>Start the QA service.</p> <code>start_simple</code> <p>Start the simple generation service.</p> <code>start_training</code> <p>Start the training service.</p> <code>stop</code> <p>Stop the service when it is running.</p> Source code in <code>gerd/frontends/router.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"The controlller is initialized in the stopped state.\"\"\"\n    self.process: subprocess.Popen | None = None\n    self.state = AppState.STOPPED\n</code></pre>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppController.check_port","title":"check_port  <code>staticmethod</code>","text":"<pre><code>check_port(port: int) -&gt; bool\n</code></pre> <p>Check if the service port is open.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>The port to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the port us open, False otherwise.</p> Source code in <code>gerd/frontends/router.py</code> <pre><code>@staticmethod\ndef check_port(port: int) -&gt; bool:\n    \"\"\"Check if the service port is open.\n\n    Parameters:\n        port: The port to check\n\n    Returns:\n        True if the port us open, False otherwise.\n    \"\"\"\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    result = sock.connect_ex((\"127.0.0.1\", port)) == 0\n    sock.close()\n    return result\n</code></pre>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppController.check_port(port)","title":"<code>port</code>","text":""},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppController.instance","title":"instance  <code>classmethod</code>","text":"<pre><code>instance() -&gt; AppController\n</code></pre> <p>Get the instance of the controller.</p> Source code in <code>gerd/frontends/router.py</code> <pre><code>@classmethod\ndef instance(cls) -&gt; \"AppController\":\n    \"\"\"Get the instance of the controller.\"\"\"\n    if cls._instance is None:\n        cls._instance = AppController()\n    return cls._instance\n</code></pre>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppController.start","title":"start","text":"<pre><code>start(frontend: str) -&gt; None\n</code></pre> <p>Start the service with the given frontend.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The frontend service name to start.</p> required Source code in <code>gerd/frontends/router.py</code> <pre><code>def start(self, frontend: str) -&gt; None:\n    \"\"\"Start the service with the given frontend.\n\n    Parameters:\n        frontend: The frontend service name to start.\n    \"\"\"\n    if not re.match(\"^[a-zA-Z0-9_]+$\", frontend):\n        msg = \"Invalid frontend name\"\n        raise gr.Error(msg)\n    self.stop()\n    cmd = [sys.executable, \"-m\", f\"gerd.frontends.{frontend}\"]\n    self.process = subprocess.Popen(  # noqa: S603\n        cmd,\n        env=os.environ | {\"GRADIO_SERVER_PORT\": GRADIO_SERVER_PORT},\n    )\n</code></pre>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppController.start(frontend)","title":"<code>frontend</code>","text":""},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppController.start_gen","title":"start_gen","text":"<pre><code>start_gen() -&gt; str\n</code></pre> <p>Start the generation service.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the current app state.</p> Source code in <code>gerd/frontends/router.py</code> <pre><code>def start_gen(self) -&gt; str:\n    \"\"\"Start the generation service.\n\n    Returns:\n        The name of the current app state.\n    \"\"\"\n    self.start(\"gen_frontend\")\n    self.state = AppState.GENERATE_STARTING\n    return self.state.name\n</code></pre>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppController.start_instruct","title":"start_instruct","text":"<pre><code>start_instruct() -&gt; str\n</code></pre> <p>Start the instruct service.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the current app state</p> Source code in <code>gerd/frontends/router.py</code> <pre><code>def start_instruct(self) -&gt; str:\n    \"\"\"Start the instruct service.\n\n    Returns:\n        The name of the current app state\n    \"\"\"\n    self.start(\"instruct\")\n    self.state = AppState.INSTRUCT_STARTING\n    return self.state.name\n</code></pre>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppController.start_qa","title":"start_qa","text":"<pre><code>start_qa() -&gt; str\n</code></pre> <p>Start the QA service.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the current app state</p> Source code in <code>gerd/frontends/router.py</code> <pre><code>def start_qa(self) -&gt; str:\n    \"\"\"Start the QA service.\n\n    Returns:\n        The name of the current app state\n    \"\"\"\n    self.start(\"qa_frontend\")\n    self.state = AppState.QA_STARTING\n    return self.state.name\n</code></pre>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppController.start_simple","title":"start_simple","text":"<pre><code>start_simple() -&gt; str\n</code></pre> <p>Start the simple generation service.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the current app state</p> Source code in <code>gerd/frontends/router.py</code> <pre><code>def start_simple(self) -&gt; str:\n    \"\"\"Start the simple generation service.\n\n    Returns:\n        The name of the current app state\n    \"\"\"\n    self.start(\"generate\")\n    self.state = AppState.SIMPLE_STARTING\n    return self.state.name\n</code></pre>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppController.start_training","title":"start_training","text":"<pre><code>start_training() -&gt; str\n</code></pre> <p>Start the training service.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the current app state</p> Source code in <code>gerd/frontends/router.py</code> <pre><code>def start_training(self) -&gt; str:\n    \"\"\"Start the training service.\n\n    Returns:\n        The name of the current app state\n    \"\"\"\n    self.start(\"training\")\n    self.state = AppState.TRAINING_STARTING\n    return self.state.name\n</code></pre>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppController.stop","title":"stop","text":"<pre><code>stop() -&gt; str\n</code></pre> <p>Stop the service when it is running.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the current app state.</p> Source code in <code>gerd/frontends/router.py</code> <pre><code>def stop(self) -&gt; str:\n    \"\"\"Stop the service when it is running.\n\n    Returns:\n        The name of the current app state.\n    \"\"\"\n    if self.state != AppState.STOPPED and self.process:\n        gr.Info(f\"Trying to stop process... {self.process.pid}\")\n        self.process.terminate()\n        ret = self.process.wait(5)\n        if ret is None:\n            gr.Info(\"Process did not stop in time, killing it\")\n            self.process.kill()\n        gr.Info(\"Stopped\")\n    if self.check_port(int(GRADIO_SERVER_PORT)):\n        gr.Info(\"Stopping service\")\n        prt = int(GRADIO_SERVER_PORT)\n        res = subprocess.check_output([\"lsof\", \"-i\", f\":{prt}\"])  # noqa: S603, S607\n        m = re.search(r\"[Py]ython\\s+(\\d+)\", res.decode(encoding=\"utf-8\"))\n        if m:\n            subprocess.check_call([\"kill\", m.group(1)])  # noqa: S603, S607\n            gr.Warning(f\"Killed service on port {prt}\")\n        else:\n            msg = \"Service could not be stopped\"\n            raise gr.Error(msg)\n    self.state = AppState.STOPPED\n    return self.state.name\n</code></pre>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppState","title":"AppState","text":"<p>               Bases: <code>Enum</code></p> <p>The state of the service.</p> <p>Attributes:</p> Name Type Description <code>GENERATE_STARTED</code> <p>The generation service is started.</p> <code>GENERATE_STARTING</code> <p>The generation service is starting.</p> <code>INSTRUCT_STARTED</code> <p>The instruct service is started.</p> <code>INSTRUCT_STARTING</code> <p>The instruct service is starting.</p> <code>QA_STARTED</code> <p>The QA service is started.</p> <code>QA_STARTING</code> <p>The QA service is starting.</p> <code>SIMPLE_STARTED</code> <p>The simple generation service is started.</p> <code>SIMPLE_STARTING</code> <p>The simple generation service is starting.</p> <code>STOPPED</code> <p>All services is stopped.</p> <code>TRAINING_STARTED</code> <p>The training service is started.</p> <code>TRAINING_STARTING</code> <p>The training service is starting.</p>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppState.GENERATE_STARTED","title":"GENERATE_STARTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>GENERATE_STARTED = auto()\n</code></pre> <p>The generation service is started.</p>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppState.GENERATE_STARTING","title":"GENERATE_STARTING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>GENERATE_STARTING = auto()\n</code></pre> <p>The generation service is starting.</p>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppState.INSTRUCT_STARTED","title":"INSTRUCT_STARTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INSTRUCT_STARTED = auto()\n</code></pre> <p>The instruct service is started.</p>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppState.INSTRUCT_STARTING","title":"INSTRUCT_STARTING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INSTRUCT_STARTING = auto()\n</code></pre> <p>The instruct service is starting.</p>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppState.QA_STARTED","title":"QA_STARTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>QA_STARTED = auto()\n</code></pre> <p>The QA service is started.</p>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppState.QA_STARTING","title":"QA_STARTING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>QA_STARTING = auto()\n</code></pre> <p>The QA service is starting.</p>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppState.SIMPLE_STARTED","title":"SIMPLE_STARTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SIMPLE_STARTED = auto()\n</code></pre> <p>The simple generation service is started.</p>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppState.SIMPLE_STARTING","title":"SIMPLE_STARTING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SIMPLE_STARTING = auto()\n</code></pre> <p>The simple generation service is starting.</p>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppState.STOPPED","title":"STOPPED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>STOPPED = auto()\n</code></pre> <p>All services is stopped.</p>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppState.TRAINING_STARTED","title":"TRAINING_STARTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TRAINING_STARTED = auto()\n</code></pre> <p>The training service is started.</p>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.AppState.TRAINING_STARTING","title":"TRAINING_STARTING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TRAINING_STARTING = auto()\n</code></pre> <p>The training service is starting.</p>"},{"location":"reference/gerd/frontends/router/#gerd.frontends.router.check_state","title":"check_state","text":"<pre><code>check_state() -&gt; str\n</code></pre> <p>Checks the app state and waits for the service to start.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the current app state.</p> Source code in <code>gerd/frontends/router.py</code> <pre><code>def check_state() -&gt; str:\n    \"\"\"Checks the app state and waits for the service to start.\n\n    Returns:\n        The name of the current app state.\n    \"\"\"\n    app = AppController.instance()\n    cnt = 0\n    while not app.check_port(int(GRADIO_SERVER_PORT)):\n        _LOGGER.info(\"Waiting for service to start\")\n        sleep(1)\n        cnt += 1\n        if cnt &gt; 30:\n            app.state = AppState.STOPPED\n            msg = \"Service did not start in time\"\n            raise Exception(msg)\n    app.state = AppState(app.state.value + 1)\n    gr.Success(f\"Service started on port {GRADIO_SERVER_PORT}\")\n    return app.state.name\n</code></pre>"},{"location":"reference/gerd/frontends/training/","title":"gerd.frontends.training","text":""},{"location":"reference/gerd/frontends/training/#gerd.frontends.training","title":"gerd.frontends.training","text":"<p>A gradio frontend to train LoRAs with.</p> <p>Classes:</p> Name Description <code>Global</code> <p>A singleton class handle to store the current trainer instance.</p> <p>Functions:</p> Name Description <code>check_trainer</code> <p>Check if the trainer is (still) running.</p> <code>get_file_list</code> <p>Get a list of files matching the glob pattern.</p> <code>get_loras</code> <p>Get a list of available LoRAs.</p> <code>start_training</code> <p>Start the training process.</p> <code>validate_files</code> <p>Validate the uploaded files.</p>"},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.Global","title":"Global","text":"<p>A singleton class handle to store the current trainer instance.</p>"},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.check_trainer","title":"check_trainer","text":"<pre><code>check_trainer() -&gt; dict[str, Any]\n</code></pre> <p>Check if the trainer is (still) running.</p> <p>When the trainer is running, a progress bar is shown. The method returns a gradio property update of 'visible' which can be used to activate and deactivate elements based on the current training status.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary with the status of gradio 'visible' property</p> Source code in <code>gerd/frontends/training.py</code> <pre><code>def check_trainer() -&gt; dict[str, Any]:\n    \"\"\"Check if the trainer is (still) running.\n\n    When the trainer is running, a progress bar is shown.\n    The method returns a gradio property update of 'visible'\n    which can be used to activate and deactivate elements based\n    on the current training status.\n\n    Returns:\n        A dictionary with the status of gradio 'visible' property\n    \"\"\"\n    if Global.trainer is not None:\n        progress = gr.Progress()\n        progress(0)\n        while Global.trainer is not None and Global.trainer.thread.is_alive():\n            max_steps = max(Global.trainer.tracked.max_steps, 1)\n            progress(Global.trainer.tracked.current_steps / max_steps)\n            time.sleep(0.5)\n        return gr.update(visible=True, value=\"Training complete\")\n    return gr.update(visible=False)\n</code></pre>"},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.get_file_list","title":"get_file_list","text":"<pre><code>get_file_list(glob_pattern: str) -&gt; str\n</code></pre> <p>Get a list of files matching the glob pattern.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The glob pattern to search for files</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string with the list of files</p> Source code in <code>gerd/frontends/training.py</code> <pre><code>def get_file_list(glob_pattern: str) -&gt; str:\n    \"\"\"Get a list of files matching the glob pattern.\n\n    Parameters:\n        glob_pattern: The glob pattern to search for files\n\n    Returns:\n        A string with the list of files\n    \"\"\"\n    if not glob_pattern:\n        return \"\"\n    res = [str(f) for f in Path().glob(glob_pattern)]\n    return \"\\n\".join(res) if res else \"&lt;No files found&gt;\"\n</code></pre>"},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.get_file_list(glob_pattern)","title":"<code>glob_pattern</code>","text":""},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.get_loras","title":"get_loras","text":"<pre><code>get_loras() -&gt; dict[str, Path]\n</code></pre> <p>Get a list of available LoRAs.</p> <p>LORAs are loaded from the path defined in the default LoraTrainingConfig.</p> <p>Returns:</p> Type Description <code>dict[str, Path]</code> <p>A dictionary with the LoRA names as keys and the paths as values</p> Source code in <code>gerd/frontends/training.py</code> <pre><code>def get_loras() -&gt; dict[str, Path]:\n    \"\"\"Get a list of available LoRAs.\n\n    LORAs are loaded from the path defined in the default\n    [LoraTrainingConfig][gerd.training.lora.LoraTrainingConfig].\n\n    Returns:\n        A dictionary with the LoRA names as keys and the paths as values\n    \"\"\"\n    return {\n        path.stem: path\n        for path in default_config.output_dir.parent.iterdir()\n        if path.is_file() and path.suffix == \".zip\"\n    }\n</code></pre>"},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.start_training","title":"start_training","text":"<pre><code>start_training(files: list[str] | None, model_name: str, lora_name: str, mode: str, data_source: str, input_glob: str, override: bool, modules: list[str], flags: list[str], epochs: int, batch_size: int, micro_batch_size: int, cutoff_len: int, overlap_len: int) -&gt; str\n</code></pre> <p>Start the training process.</p> <p>While training, the method will update the progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>list[str] | None</code> <p>The list of files to train on</p> required <code>str</code> <p>The name of the model to train</p> required <code>str</code> <p>The name of the LoRA to train</p> required <code>str</code> <p>The training mode</p> required <code>str</code> <p>The source of the data</p> required <code>str</code> <p>The glob pattern to search for files</p> required <code>bool</code> <p>Whether to override existing models</p> required <code>list[str]</code> <p>The modules to train</p> required <code>list[str]</code> <p>The flags to set</p> required <code>int</code> <p>The number of epochs to train</p> required <code>int</code> <p>The batch size</p> required <code>int</code> <p>The micro batch size</p> required <code>int</code> <p>The cutoff length</p> required <code>int</code> <p>The overlap length</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string with the status of the training</p> Source code in <code>gerd/frontends/training.py</code> <pre><code>def start_training(\n    files: list[str] | None,\n    model_name: str,\n    lora_name: str,\n    mode: str,\n    data_source: str,\n    input_glob: str,\n    override: bool,\n    modules: list[str],\n    flags: list[str],\n    epochs: int,\n    batch_size: int,\n    micro_batch_size: int,\n    cutoff_len: int,\n    overlap_len: int,\n) -&gt; str:\n    \"\"\"Start the training process.\n\n    While training, the method will update the progress bar.\n\n    Parameters:\n        files: The list of files to train on\n        model_name: The name of the model to train\n        lora_name: The name of the LoRA to train\n        mode: The training mode\n        data_source: The source of the data\n        input_glob: The glob pattern to search for files\n        override: Whether to override existing models\n        modules: The modules to train\n        flags: The flags to set\n        epochs: The number of epochs to train\n        batch_size: The batch size\n        micro_batch_size: The micro batch size\n        cutoff_len: The cutoff length\n        overlap_len: The overlap length\n\n    Returns:\n        A string with the status of the training\n    \"\"\"\n    if \"..\" in lora_name:\n        msg = \"Invalid LoRA name\"\n        raise gr.Error(msg)\n    progress = gr.Progress()\n    # a bit verbose conversion to appease mypy\n    # since training_mode is a Literal type\n    if mode.lower() not in [\"unstructured\", \"instructions\"]:\n        msg = f\"Invalid training mode '{mode}'\"\n        raise AssertionError(msg)\n    training_mode: Literal[\"unstructured\", \"instructions\"] = (\n        \"unstructured\" if mode.lower() == \"unstructured\" else \"instructions\"\n    )\n    train_config = LoraTrainingConfig(\n        model=ModelConfig(name=model_name),\n        mode=training_mode,\n        output_dir=default_config.output_dir.parent / lora_name,\n        override_existing=override,\n        modules=LoraModules(default=False, **{mod: True for mod in modules}),\n        flags=TrainingFlags(**{flag: True for flag in flags}),\n        epochs=epochs,\n        batch_size=batch_size,\n        micro_batch_size=micro_batch_size,\n        cutoff_len=cutoff_len,\n        input_glob=input_glob,\n        overlap_len=overlap_len,\n        zip_output=True,\n    )\n    # init settings will be overriden by env settings\n    # we need to set the output_dir explicitly\n    train_config.output_dir = default_config.output_dir.parent / lora_name\n\n    if files is None or not files and data_source == \"Upload\":\n        msg = \"No files uploaded\"\n        raise gr.Error(msg)\n    progress(0)\n    try:\n        if train_config.mode == \"unstructured\":\n            Global.trainer = train_unstructured(\n                train_config,\n                (\n                    [Path(f).read_text() for f in files]\n                    if data_source == \"Upload\"\n                    else None\n                ),\n            )\n        elif train_config.mode == \"instructions\":\n\n            def load_data() -&gt; InstructTrainingData:\n                data = InstructTrainingData()\n                for f in files:\n                    with Path(f).open(\"r\") as file:\n                        data.samples.extend(\n                            InstructTrainingData.model_validate(json.load(file)).samples\n                        )\n                return data\n\n            Global.trainer = train_instruct(\n                train_config, load_data() if data_source == \"Upload\" else None\n            )\n        else:\n            msg = \"Invalid training mode\"\n            raise AssertionError(msg)\n    except AssertionError as e:\n        msg = str(e)\n        raise gr.Error(msg) from e\n    return \"Training started\"\n</code></pre>"},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.start_training(files)","title":"<code>files</code>","text":""},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.start_training(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.start_training(lora_name)","title":"<code>lora_name</code>","text":""},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.start_training(mode)","title":"<code>mode</code>","text":""},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.start_training(data_source)","title":"<code>data_source</code>","text":""},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.start_training(input_glob)","title":"<code>input_glob</code>","text":""},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.start_training(override)","title":"<code>override</code>","text":""},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.start_training(modules)","title":"<code>modules</code>","text":""},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.start_training(flags)","title":"<code>flags</code>","text":""},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.start_training(epochs)","title":"<code>epochs</code>","text":""},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.start_training(batch_size)","title":"<code>batch_size</code>","text":""},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.start_training(micro_batch_size)","title":"<code>micro_batch_size</code>","text":""},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.start_training(cutoff_len)","title":"<code>cutoff_len</code>","text":""},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.start_training(overlap_len)","title":"<code>overlap_len</code>","text":""},{"location":"reference/gerd/frontends/training/#gerd.frontends.training.validate_files","title":"validate_files","text":"<pre><code>validate_files(file_paths: list[str] | None, mode: str) -&gt; tuple[list[str], dict[str, bool]]\n</code></pre> <p>Validate the uploaded files.</p> <p>Whether the property 'interactive' is True depends on whether any files were valid. Parameters:     file_paths: The list of file paths     mode: The training mode</p> <p>Returns:</p> Type Description <code>tuple[list[str], dict[str, bool]]</code> <p>A tuple with the validated file paths and gradio property 'interactive'</p> Source code in <code>gerd/frontends/training.py</code> <pre><code>def validate_files(\n    file_paths: list[str] | None, mode: str\n) -&gt; tuple[list[str], dict[str, bool]]:\n    \"\"\"Validate the uploaded files.\n\n    Whether the property 'interactive' is True depends on whether\n    any files were valid.\n    Parameters:\n        file_paths: The list of file paths\n        mode: The training mode\n\n    Returns:\n        A tuple with the validated file paths and gradio property 'interactive'\n    \"\"\"\n    file_paths = file_paths or []\n    if mode.lower() == \"instructions\":\n\n        def validate(path: Path) -&gt; bool:\n            if path.suffix != \".json\":\n                gr.Warning(f\"{path.name} has an invalid file type\")\n                return False\n            try:\n                with path.open(\"r\") as f:\n                    InstructTrainingData.model_validate(json.load(f))\n            except Exception as e:\n                gr.Warning(f\"{path.name} could not be validated: {e}\")\n                return False\n            return True\n\n    elif mode.lower() == \"unstructured\":\n\n        def validate(path: Path) -&gt; bool:\n            if path.suffix != \".txt\":\n                gr.Warning(f\"{path.name} has an invalid file type\")\n                return False\n            return True\n\n    else:\n        gr.Error(\"Invalid training mode\")\n        return (file_paths, gr.update(interactive=False))\n\n    res = [f_path for f_path in file_paths if validate(Path(f_path).resolve())]\n    return (res, gr.update(interactive=len(res) &gt; 0))\n</code></pre>"},{"location":"reference/gerd/gen/","title":"gerd.gen","text":""},{"location":"reference/gerd/gen/#gerd.gen","title":"gerd.gen","text":"<p>Services and utilities for text generation with LLMs.</p> <p>Modules:</p> Name Description <code>chat_service</code> <p>Implementation of the ChatService class.</p> <code>generation_service</code> <p>Implements the Generation class.</p>"},{"location":"reference/gerd/gen/chat_service/","title":"gerd.gen.chat_service","text":""},{"location":"reference/gerd/gen/chat_service/#gerd.gen.chat_service","title":"gerd.gen.chat_service","text":"<p>Implementation of the ChatService class.</p> <p>This features the currently favoured approach of instruction-based work with large language models. Thus, models fined tuned for chat or instructions work best with this service. The service can be used to generate text as well as long as the model features a chat template. In this case this service should be prefered over the GenerationService since it is easier to setup a prompt according to the model's requirements.</p> <p>Classes:</p> Name Description <code>ChatService</code> <p>Service to generate text based on a chat history.</p>"},{"location":"reference/gerd/gen/chat_service/#gerd.gen.chat_service.ChatService","title":"ChatService","text":"<pre><code>ChatService(config: GenerationConfig, parameters: Dict[str, str] | None = None)\n</code></pre> <p>Service to generate text based on a chat history.</p> <p>The service is initialized with a config and parameters.</p> <p>The parameters are used to initialize the message history. However, future reset will not consider them. loads a model according to this config. The used LLM is loaded according to the model configuration right on initialization.</p> <p>Methods:</p> Name Description <code>add_message</code> <p>Add a message to the chat history.</p> <code>generate</code> <p>Generate a response based on the chat history.</p> <code>get_prompt_config</code> <p>Get the prompt configuration.</p> <code>reset</code> <p>Reset the chat history.</p> <code>set_prompt_config</code> <p>Set the prompt configuration.</p> <code>submit_user_message</code> <p>Submit a message with the user role and generates a response.</p> Source code in <code>gerd/gen/chat_service.py</code> <pre><code>def __init__(\n    self, config: GenerationConfig, parameters: Dict[str, str] | None = None\n) -&gt; None:\n    \"\"\"The service is initialized with a config and parameters.\n\n    The parameters are used to initialize the message history.\n    However, future reset will not consider them.\n    loads a model according to this config.\n    The used LLM is loaded according to the model configuration\n    right on initialization.\n    \"\"\"\n    self.config = config\n    self._model = gerd_loader.load_model_from_config(self.config.model)\n    self.messages: list[ChatMessage] = []\n    self.reset(parameters)\n</code></pre>"},{"location":"reference/gerd/gen/chat_service/#gerd.gen.chat_service.ChatService.add_message","title":"add_message","text":"<pre><code>add_message(parameters: Dict[str, str] | None = None, role: Literal['user', 'system', 'assistant'] = 'user', prompt_config: Optional[PromptConfig] = None) -&gt; None\n</code></pre> <p>Add a message to the chat history.</p> Source code in <code>gerd/gen/chat_service.py</code> <pre><code>def add_message(\n    self,\n    parameters: Dict[str, str] | None = None,\n    role: Literal[\"user\", \"system\", \"assistant\"] = \"user\",\n    prompt_config: Optional[PromptConfig] = None,\n) -&gt; None:\n    \"\"\"Add a message to the chat history.\"\"\"\n    parameters = parameters or {}\n    user_prompt: PromptConfig = prompt_config or self.config.model.prompt_config\n    self.messages.append(\n        {\n            \"role\": role,\n            \"content\": user_prompt.format(parameters),\n        }\n    )\n</code></pre>"},{"location":"reference/gerd/gen/chat_service/#gerd.gen.chat_service.ChatService.generate","title":"generate","text":"<pre><code>generate(parameters: Dict[str, str]) -&gt; GenResponse\n</code></pre> <p>Generate a response based on the chat history.</p> <p>This method can be used as a replacement for GenerationService.generate in cases where the used model provides a chat template. When this is the case, using this method is more reliable as it requires less manual configuration to set up the prompt according to the model's requirements.</p> <p>Parameters:</p> Name Type Description Default <code>Dict[str, str]</code> <p>The parameters to format the prompt with</p> required <p>Returns:</p> Type Description <code>GenResponse</code> <p>The generation result</p> Source code in <code>gerd/gen/chat_service.py</code> <pre><code>def generate(self, parameters: Dict[str, str]) -&gt; GenResponse:\n    \"\"\"Generate a response based on the chat history.\n\n    This method can be used as a replacement for\n    [GenerationService.generate][gerd.gen.generation_service.GenerationService.generate]\n    in cases where the used model provides a chat template.\n    When this is the case, using this method is more reliable as it requires less\n    manual configuration to set up the prompt according to the model's requirements.\n\n    Parameters:\n        parameters: The parameters to format the prompt with\n\n    Returns:\n        The generation result\n    \"\"\"\n    self.reset(parameters)\n    self.add_message(parameters, role=\"user\")\n\n    if self.config.features.prompt_chaining:\n        for i, prompt_config in enumerate(\n            self.config.features.prompt_chaining.prompts, 1\n        ):\n            self.reset()\n            res = self.submit_user_message(parameters, prompt_config=prompt_config)\n            parameters[f\"response_{i}\"] = res.text\n        response = res.text\n        resolved = \"\\n\".join(parameters.values())\n    else:\n        resolved = \"\\n\".join(m[\"content\"] for m in self.messages)\n        _LOGGER.debug(\n            \"\\n\"\n            \"===== Resolved prompt ======\\n\\n\"\n            \"%s\\n\\n\"\n            \"============================\",\n            resolved,\n        )\n        response = self._model.generate(resolved)\n        _LOGGER.debug(\n            \"\\n\"\n            \"========= Response =========\\n\\n\"\n            \"%s\\n\\n\"\n            \"============================\",\n            response,\n        )\n    return GenResponse(text=response, prompt=resolved)\n</code></pre>"},{"location":"reference/gerd/gen/chat_service/#gerd.gen.chat_service.ChatService.generate(parameters)","title":"<code>parameters</code>","text":""},{"location":"reference/gerd/gen/chat_service/#gerd.gen.chat_service.ChatService.get_prompt_config","title":"get_prompt_config","text":"<pre><code>get_prompt_config() -&gt; PromptConfig\n</code></pre> <p>Get the prompt configuration.</p> Source code in <code>gerd/gen/chat_service.py</code> <pre><code>def get_prompt_config(self) -&gt; PromptConfig:\n    \"\"\"Get the prompt configuration.\"\"\"\n    return self.config.model.prompt_config\n</code></pre>"},{"location":"reference/gerd/gen/chat_service/#gerd.gen.chat_service.ChatService.reset","title":"reset","text":"<pre><code>reset(parameters: Dict[str, str] | None = None) -&gt; None\n</code></pre> <p>Reset the chat history.</p> Source code in <code>gerd/gen/chat_service.py</code> <pre><code>def reset(self, parameters: Dict[str, str] | None = None) -&gt; None:\n    \"\"\"Reset the chat history.\"\"\"\n    parameters = parameters or {}\n    self.messages.clear()\n    for role, message in self.config.model.prompt_setup:\n        self.messages.append(\n            {\n                \"role\": role,\n                \"content\": message.format(parameters),\n            }\n        )\n</code></pre>"},{"location":"reference/gerd/gen/chat_service/#gerd.gen.chat_service.ChatService.set_prompt_config","title":"set_prompt_config","text":"<pre><code>set_prompt_config(config: PromptConfig) -&gt; PromptConfig\n</code></pre> <p>Set the prompt configuration.</p> Source code in <code>gerd/gen/chat_service.py</code> <pre><code>def set_prompt_config(\n    self,\n    config: PromptConfig,\n) -&gt; PromptConfig:\n    \"\"\"Set the prompt configuration.\"\"\"\n    self.config.model.prompt_config = config\n    return self.config.model.prompt_config\n</code></pre>"},{"location":"reference/gerd/gen/chat_service/#gerd.gen.chat_service.ChatService.submit_user_message","title":"submit_user_message","text":"<pre><code>submit_user_message(parameters: Dict[str, str] | None = None, prompt_config: Optional[PromptConfig] = None) -&gt; GenResponse\n</code></pre> <p>Submit a message with the user role and generates a response.</p> <p>The service's prompt configuration is used to format the prompt unless a different prompt configuration is provided. Parameters:     parameters: The parameters to format the prompt with     prompt_config: The optional prompt configuration to be used</p> <p>Returns:</p> Type Description <code>GenResponse</code> <p>The generation result</p> Source code in <code>gerd/gen/chat_service.py</code> <pre><code>def submit_user_message(\n    self,\n    parameters: Dict[str, str] | None = None,\n    prompt_config: Optional[PromptConfig] = None,\n) -&gt; GenResponse:\n    \"\"\"Submit a message with the user role and generates a response.\n\n    The service's prompt configuration is used to format the prompt unless\n    a different prompt configuration is provided.\n    Parameters:\n        parameters: The parameters to format the prompt with\n        prompt_config: The optional prompt configuration to be used\n\n    Returns:\n        The generation result\n    \"\"\"\n    self.add_message(parameters, role=\"user\", prompt_config=prompt_config)\n    _LOGGER.debug(\n        \"\\n\"\n        \"===== Resolved prompt ======\\n\\n\"\n        \"%s\\n\\n\"\n        \"============================\",\n        \"\\n\".join(m[\"role\"] + \": \" + str(m[\"content\"]) for m in self.messages),\n    )\n    role, response = self._model.create_chat_completion(self.messages)\n    _LOGGER.debug(\n        \"\\n\"\n        \"========= Response =========\\n\\n\"\n        \"%s: %s\\n\\n\"\n        \"============================\",\n        role,\n        response,\n    )\n    self.messages.append({\"role\": role, \"content\": response})\n    return GenResponse(text=response, prompt=self.messages[-2][\"content\"])\n</code></pre>"},{"location":"reference/gerd/gen/generation_service/","title":"gerd.gen.generation_service","text":""},{"location":"reference/gerd/gen/generation_service/#gerd.gen.generation_service","title":"gerd.gen.generation_service","text":"<p>Implements the Generation class.</p> <p>The generation services is meant to generate text based on a prompt and/or the continuation of a provided text.</p> <p>Classes:</p> Name Description <code>GenerationService</code> <p>Service to generate text based on a prompt.</p>"},{"location":"reference/gerd/gen/generation_service/#gerd.gen.generation_service.GenerationService","title":"GenerationService","text":"<pre><code>GenerationService(config: GenerationConfig)\n</code></pre> <p>Service to generate text based on a prompt.</p> <p>Initialize the generation service and loads the model.</p> <p>Parameters:</p> Name Type Description Default <code>GenerationConfig</code> <p>The configuration for the generation service</p> required <p>Methods:</p> Name Description <code>generate</code> <p>Generate text based on the prompt configuration.</p> <code>get_prompt_config</code> <p>Get the prompt configuration.</p> <code>set_prompt_config</code> <p>Sets the prompt configuration.</p> Source code in <code>gerd/gen/generation_service.py</code> <pre><code>def __init__(self, config: GenerationConfig) -&gt; None:\n    \"\"\"Initialize the generation service and loads the model.\n\n    Parameters:\n        config: The configuration for the generation service\n    \"\"\"\n    self.config = config\n    self._model = gerd_loader.load_model_from_config(self.config.model)\n</code></pre>"},{"location":"reference/gerd/gen/generation_service/#gerd.gen.generation_service.GenerationService(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/gen/generation_service/#gerd.gen.generation_service.GenerationService.generate","title":"generate","text":"<pre><code>generate(parameters: Dict[str, str], add_prompt: bool = False) -&gt; GenResponse\n</code></pre> <p>Generate text based on the prompt configuration.</p> <p>The actual prompt is provided by the prompt configuration. The list of parameters is used to format the prompt and replace the placeholders. The list can be empty if the prompt does not contain any placeholders.</p> <p>Parameters:</p> Name Type Description Default <code>Dict[str, str]</code> <p>The parameters to format the prompt with</p> required <code>bool</code> <p>Whether to add the prompt to the response</p> <code>False</code> <p>Returns:</p> Type Description <code>GenResponse</code> <p>The generation result</p> Source code in <code>gerd/gen/generation_service.py</code> <pre><code>def generate(\n    self, parameters: Dict[str, str], add_prompt: bool = False\n) -&gt; GenResponse:\n    \"\"\"Generate text based on the prompt configuration.\n\n    The actual prompt is provided by the prompt configuration.\n    The list of parameters is used to format the prompt\n    and replace the placeholders. The list can be empty if\n    the prompt does not contain any placeholders.\n\n    Parameters:\n        parameters: The parameters to format the prompt with\n        add_prompt: Whether to add the prompt to the response\n\n    Returns:\n        The generation result\n    \"\"\"\n    if self.config.features.prompt_chaining:\n        from gerd.features.prompt_chaining import PromptChaining\n\n        response = PromptChaining(\n            self.config.features.prompt_chaining,\n            self._model,\n            self.config.model.prompt_config,\n        ).generate(parameters)\n    else:\n        template = self.config.model.prompt_config.template\n        resolved = (\n            template.render(**parameters)\n            if template\n            else self.config.model.prompt_config.text.format(**parameters)\n        )\n        _LOGGER.debug(\n            \"\\n====== Resolved prompt =====\\n\\n%s\\n\\n=============================\",\n            resolved,\n        )\n        response = self._model.generate(resolved)\n        _LOGGER.debug(\n            \"\\n====== Response =====\\n\\n%s\\n\\n=============================\",\n            response,\n        )\n    return GenResponse(text=response, prompt=resolved if add_prompt else None)\n</code></pre>"},{"location":"reference/gerd/gen/generation_service/#gerd.gen.generation_service.GenerationService.generate(parameters)","title":"<code>parameters</code>","text":""},{"location":"reference/gerd/gen/generation_service/#gerd.gen.generation_service.GenerationService.generate(add_prompt)","title":"<code>add_prompt</code>","text":""},{"location":"reference/gerd/gen/generation_service/#gerd.gen.generation_service.GenerationService.get_prompt_config","title":"get_prompt_config","text":"<pre><code>get_prompt_config() -&gt; PromptConfig\n</code></pre> <p>Get the prompt configuration.</p> <p>Returns:</p> Type Description <code>PromptConfig</code> <p>The prompt configuration</p> Source code in <code>gerd/gen/generation_service.py</code> <pre><code>def get_prompt_config(self) -&gt; PromptConfig:\n    \"\"\"Get the prompt configuration.\n\n    Returns:\n        The prompt configuration\n    \"\"\"\n    return self.config.model.prompt_config\n</code></pre>"},{"location":"reference/gerd/gen/generation_service/#gerd.gen.generation_service.GenerationService.set_prompt_config","title":"set_prompt_config","text":"<pre><code>set_prompt_config(config: PromptConfig) -&gt; PromptConfig\n</code></pre> <p>Sets the prompt configuration.</p> <p>Parameters:</p> Name Type Description Default <code>PromptConfig</code> <p>The prompt configuration</p> required <p>Returns:     The prompt configuration; Should be the same as the input in most cases</p> Source code in <code>gerd/gen/generation_service.py</code> <pre><code>def set_prompt_config(\n    self,\n    config: PromptConfig,\n) -&gt; PromptConfig:\n    \"\"\"Sets the prompt configuration.\n\n    Parameters:\n        config: The prompt configuration\n    Returns:\n        The prompt configuration; Should be the same as the input in most cases\n    \"\"\"\n    self.config.model.prompt_config = config\n    return self.config.model.prompt_config\n</code></pre>"},{"location":"reference/gerd/gen/generation_service/#gerd.gen.generation_service.GenerationService.set_prompt_config(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/loader/","title":"gerd.loader","text":""},{"location":"reference/gerd/loader/#gerd.loader","title":"gerd.loader","text":"<p>Module for loading language models.</p> <p>Depending on the configuration, different language models are loaded and different libraries are used. The main goal is to provide a unified interface to the different models and libraries.</p> <p>Classes:</p> Name Description <code>LLM</code> <p>The abstract base class for large language models.</p> <code>LlamaCppLLM</code> <p>A language model using the Llama.cpp library.</p> <code>MockLLM</code> <p>A mock language model for testing purposes.</p> <code>RemoteLLM</code> <p>A language model using a remote endpoint.</p> <code>TransformerLLM</code> <p>A language model using the transformers library.</p> <p>Functions:</p> Name Description <code>load_model_from_config</code> <p>Loads a language model based on the configuration.</p>"},{"location":"reference/gerd/loader/#gerd.loader.LLM","title":"LLM","text":"<pre><code>LLM(config: ModelConfig)\n</code></pre> <p>The abstract base class for large language models.</p> <p>Should be implemented by all language model backends.</p> <p>A language model is initialized with a configuration.</p> <p>Parameters:</p> Name Type Description Default <code>ModelConfig</code> <p>The configuration for the language model</p> required <p>Methods:</p> Name Description <code>create_chat_completion</code> <p>Create a chat completion based on a list of messages.</p> <code>generate</code> <p>Generate text based on a prompt.</p> Source code in <code>gerd/loader.py</code> <pre><code>@abc.abstractmethod\ndef __init__(self, config: ModelConfig) -&gt; None:\n    \"\"\"A language model is initialized with a configuration.\n\n    Parameters:\n        config: The configuration for the language model\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/gerd/loader/#gerd.loader.LLM(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/loader/#gerd.loader.LLM.create_chat_completion","title":"create_chat_completion  <code>abstractmethod</code>","text":"<pre><code>create_chat_completion(messages: list[ChatMessage]) -&gt; tuple[ChatRole, str]\n</code></pre> <p>Create a chat completion based on a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>list[ChatMessage]</code> <p>The list of messages in the chat history</p> required <p>Returns:</p> Type Description <code>tuple[ChatRole, str]</code> <p>The role of the generated message and the content</p> Source code in <code>gerd/loader.py</code> <pre><code>@abc.abstractmethod\ndef create_chat_completion(\n    self, messages: list[ChatMessage]\n) -&gt; tuple[ChatRole, str]:\n    \"\"\"Create a chat completion based on a list of messages.\n\n    Parameters:\n        messages: The list of messages in the chat history\n\n    Returns:\n        The role of the generated message and the content\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/gerd/loader/#gerd.loader.LLM.create_chat_completion(messages)","title":"<code>messages</code>","text":""},{"location":"reference/gerd/loader/#gerd.loader.LLM.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(prompt: str) -&gt; str\n</code></pre> <p>Generate text based on a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The prompt to generate text from</p> required <p>Returns:</p> Type Description <code>str</code> <p>The generated text</p> Source code in <code>gerd/loader.py</code> <pre><code>@abc.abstractmethod\ndef generate(self, prompt: str) -&gt; str:\n    \"\"\"Generate text based on a prompt.\n\n    Parameters:\n        prompt: The prompt to generate text from\n\n    Returns:\n        The generated text\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/gerd/loader/#gerd.loader.LLM.generate(prompt)","title":"<code>prompt</code>","text":""},{"location":"reference/gerd/loader/#gerd.loader.LlamaCppLLM","title":"LlamaCppLLM","text":"<pre><code>LlamaCppLLM(config: ModelConfig)\n</code></pre> <p>               Bases: <code>LLM</code></p> <p>A language model using the Llama.cpp library.</p> <p>A language model is initialized with a configuration.</p> <p>Parameters:</p> Name Type Description Default <code>ModelConfig</code> <p>The configuration for the language model</p> required <p>Methods:</p> Name Description <code>create_chat_completion</code> <p>Create a chat completion based on a list of messages.</p> <code>generate</code> <p>Generate text based on a prompt.</p> Source code in <code>gerd/loader.py</code> <pre><code>@override\ndef __init__(self, config: ModelConfig) -&gt; None:\n    from llama_cpp import Llama\n\n    self.config = config\n    self._model = Llama.from_pretrained(\n        repo_id=config.name,\n        filename=config.file,\n        n_ctx=config.context_length,\n        n_gpu_layers=config.gpu_layers,\n        n_threads=config.threads,\n        **config.extra_kwargs or {},\n    )\n</code></pre>"},{"location":"reference/gerd/loader/#gerd.loader.LlamaCppLLM(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/loader/#gerd.loader.LlamaCppLLM.create_chat_completion","title":"create_chat_completion","text":"<pre><code>create_chat_completion(messages: list[ChatMessage]) -&gt; tuple[ChatRole, str]\n</code></pre> <p>Create a chat completion based on a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>list[ChatMessage]</code> <p>The list of messages in the chat history</p> required <p>Returns:</p> Type Description <code>tuple[ChatRole, str]</code> <p>The role of the generated message and the content</p> Source code in <code>gerd/loader.py</code> <pre><code>@override\ndef create_chat_completion(\n    self, messages: list[ChatMessage]\n) -&gt; tuple[ChatRole, str]:\n    res = self._model.create_chat_completion(\n        # mypy cannot resolve the role parameter even though\n        # is is defined on compatible literals\n        [{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in messages],  # type: ignore[misc]\n        stop=self.config.stop,\n        max_tokens=self.config.max_new_tokens,\n        top_p=self.config.top_p,\n        top_k=self.config.top_k,\n        temperature=self.config.temperature,\n        repeat_penalty=self.config.repetition_penalty,\n    )\n    if not isinstance(res, Iterator):\n        msg = res[\"choices\"][0][\"message\"]\n        if msg[\"role\"] == \"function\":\n            error_msg = \"function role not expected\"\n            raise NotImplementedError(error_msg)\n        return (msg[\"role\"], msg[\"content\"].strip() if msg[\"content\"] else \"\")\n\n    error_msg = \"Cannot process stream responses for now\"\n    raise NotImplementedError(error_msg)\n</code></pre>"},{"location":"reference/gerd/loader/#gerd.loader.LlamaCppLLM.create_chat_completion(messages)","title":"<code>messages</code>","text":""},{"location":"reference/gerd/loader/#gerd.loader.LlamaCppLLM.generate","title":"generate","text":"<pre><code>generate(prompt: str) -&gt; str\n</code></pre> <p>Generate text based on a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The prompt to generate text from</p> required <p>Returns:</p> Type Description <code>str</code> <p>The generated text</p> Source code in <code>gerd/loader.py</code> <pre><code>@override\ndef generate(self, prompt: str) -&gt; str:\n    res = self._model(\n        prompt,\n        stop=self.config.stop,\n        max_tokens=self.config.max_new_tokens,\n        top_p=self.config.top_p,\n        top_k=self.config.top_k,\n        temperature=self.config.temperature,\n        repeat_penalty=self.config.repetition_penalty,\n    )\n    output = next(res) if isinstance(res, Iterator) else res\n    return output[\"choices\"][0][\"text\"]\n</code></pre>"},{"location":"reference/gerd/loader/#gerd.loader.LlamaCppLLM.generate(prompt)","title":"<code>prompt</code>","text":""},{"location":"reference/gerd/loader/#gerd.loader.MockLLM","title":"MockLLM","text":"<pre><code>MockLLM(_: ModelConfig)\n</code></pre> <p>               Bases: <code>LLM</code></p> <p>A mock language model for testing purposes.</p> <p>A language model is initialized with a configuration.</p> <p>Parameters:</p> Name Type Description Default <code>ModelConfig</code> <p>The configuration for the language model</p> required <p>Methods:</p> Name Description <code>create_chat_completion</code> <p>Create a chat completion based on a list of messages.</p> <code>generate</code> <p>Generate text based on a prompt.</p> Source code in <code>gerd/loader.py</code> <pre><code>@override\ndef __init__(self, _: ModelConfig) -&gt; None:\n    self.ret_value = \"MockLLM\"\n    pass\n</code></pre>"},{"location":"reference/gerd/loader/#gerd.loader.MockLLM(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/loader/#gerd.loader.MockLLM.create_chat_completion","title":"create_chat_completion","text":"<pre><code>create_chat_completion(_: list[ChatMessage]) -&gt; tuple[ChatRole, str]\n</code></pre> <p>Create a chat completion based on a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>list[ChatMessage]</code> <p>The list of messages in the chat history</p> required <p>Returns:</p> Type Description <code>tuple[ChatRole, str]</code> <p>The role of the generated message and the content</p> Source code in <code>gerd/loader.py</code> <pre><code>@override\ndef create_chat_completion(self, _: list[ChatMessage]) -&gt; tuple[ChatRole, str]:\n    return (\"assistant\", self.ret_value)\n</code></pre>"},{"location":"reference/gerd/loader/#gerd.loader.MockLLM.create_chat_completion(messages)","title":"<code>messages</code>","text":""},{"location":"reference/gerd/loader/#gerd.loader.MockLLM.generate","title":"generate","text":"<pre><code>generate(_: str) -&gt; str\n</code></pre> <p>Generate text based on a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The prompt to generate text from</p> required <p>Returns:</p> Type Description <code>str</code> <p>The generated text</p> Source code in <code>gerd/loader.py</code> <pre><code>@override\ndef generate(self, _: str) -&gt; str:\n    return self.ret_value\n</code></pre>"},{"location":"reference/gerd/loader/#gerd.loader.MockLLM.generate(prompt)","title":"<code>prompt</code>","text":""},{"location":"reference/gerd/loader/#gerd.loader.RemoteLLM","title":"RemoteLLM","text":"<pre><code>RemoteLLM(config: ModelConfig)\n</code></pre> <p>               Bases: <code>LLM</code></p> <p>A language model using a remote endpoint.</p> <p>The endpoint can be any service that are compatible with llama.cpp and openai API. For further information, please refer to the llama.cpp server API.</p> <p>A language model is initialized with a configuration.</p> <p>Parameters:</p> Name Type Description Default <code>ModelConfig</code> <p>The configuration for the language model</p> required <p>Methods:</p> Name Description <code>create_chat_completion</code> <p>Create a chat completion based on a list of messages.</p> <code>generate</code> <p>Generate text based on a prompt.</p> Source code in <code>gerd/loader.py</code> <pre><code>@override\ndef __init__(self, config: ModelConfig) -&gt; None:\n    self.config = config\n    if self.config.endpoint is None:\n        msg = \"Endpoint is required for remote LLM\"\n        raise ValueError(msg)\n\n    self._ep: ModelEndpoint = self.config.endpoint\n    self._prompt_field = \"prompt\"\n    self._header = {\"Content-Type\": \"application/json\"}\n    if self.config.endpoint.key:\n        self._header[\"Authorization\"] = (\n            f\"Bearer {self.config.endpoint.key.get_secret_value()}\"\n        )\n\n    if self.config.endpoint.type == \"openai\":\n        self.msg_template = {\n            \"model\": self.config.name,\n            \"temperature\": self.config.temperature,\n            \"frequency_penalty\": self.config.repetition_penalty,\n            \"max_completion_tokens\": self.config.max_new_tokens,\n            \"n\": 1,\n            \"stop\": self.config.stop,\n            \"top_p\": self.config.top_p,\n        }\n    elif self.config.endpoint.type == \"llama.cpp\":\n        self.msg_template = {\n            \"temperature\": self.config.temperature,\n            \"top_k\": self.config.top_k,\n            \"top_p\": self.config.top_p,\n            \"repeat_penalty\": self.config.repetition_penalty,\n            \"n_predict\": self.config.max_new_tokens,\n            \"stop\": self.config.stop or [],\n        }\n    else:\n        msg = f\"Unknown endpoint type: {self.config.endpoint.type}\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"reference/gerd/loader/#gerd.loader.RemoteLLM(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/loader/#gerd.loader.RemoteLLM.create_chat_completion","title":"create_chat_completion","text":"<pre><code>create_chat_completion(messages: list[ChatMessage]) -&gt; tuple[ChatRole, str]\n</code></pre> <p>Create a chat completion based on a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>list[ChatMessage]</code> <p>The list of messages in the chat history</p> required <p>Returns:</p> Type Description <code>tuple[ChatRole, str]</code> <p>The role of the generated message and the content</p> Source code in <code>gerd/loader.py</code> <pre><code>@override\ndef create_chat_completion(\n    self, messages: list[ChatMessage]\n) -&gt; tuple[ChatRole, str]:\n    import json\n\n    import requests\n\n    self.msg_template[\"messages\"] = messages\n    res = requests.post(\n        self._ep.url + \"/v1/chat/completions\",\n        headers=self._header,\n        data=json.dumps(self.msg_template),\n        timeout=300,\n    )\n    if res.status_code == 200:\n        msg = res.json()[\"choices\"][0][\"message\"]\n        return (msg[\"role\"], msg[\"content\"].strip())\n    else:\n        _LOGGER.warning(\"Server returned error code %d\", res.status_code)\n    return (\"assistant\", \"\")\n</code></pre>"},{"location":"reference/gerd/loader/#gerd.loader.RemoteLLM.create_chat_completion(messages)","title":"<code>messages</code>","text":""},{"location":"reference/gerd/loader/#gerd.loader.RemoteLLM.generate","title":"generate","text":"<pre><code>generate(prompt: str) -&gt; str\n</code></pre> <p>Generate text based on a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The prompt to generate text from</p> required <p>Returns:</p> Type Description <code>str</code> <p>The generated text</p> Source code in <code>gerd/loader.py</code> <pre><code>@override\ndef generate(self, prompt: str) -&gt; str:\n    import json\n\n    import requests\n\n    if self.config.endpoint and self.config.endpoint.type != \"llama.cpp\":\n        msg = (\n            \"Only llama.cpp supports simple completion yet. \"\n            \"Use chat completion instead.\"\n        )\n        raise NotImplementedError(msg)\n\n    self.msg_template[self._prompt_field] = prompt\n    res = requests.post(\n        self._ep.url + \"/completion\",\n        headers=self._header,\n        data=json.dumps(self.msg_template),\n        timeout=300,\n    )\n    if res.status_code == 200:\n        return str(res.json()[\"content\"])\n    else:\n        _LOGGER.warning(\"Server returned error code %d\", res.status_code)\n    return \"\"\n</code></pre>"},{"location":"reference/gerd/loader/#gerd.loader.RemoteLLM.generate(prompt)","title":"<code>prompt</code>","text":""},{"location":"reference/gerd/loader/#gerd.loader.TransformerLLM","title":"TransformerLLM","text":"<pre><code>TransformerLLM(config: ModelConfig)\n</code></pre> <p>               Bases: <code>LLM</code></p> <p>A language model using the transformers library.</p> <p>A language model is initialized with a configuration.</p> <p>Parameters:</p> Name Type Description Default <code>ModelConfig</code> <p>The configuration for the language model</p> required <p>Methods:</p> Name Description <code>create_chat_completion</code> <p>Create a chat completion based on a list of messages.</p> <code>generate</code> <p>Generate text based on a prompt.</p> Source code in <code>gerd/loader.py</code> <pre><code>@override\ndef __init__(self, config: ModelConfig) -&gt; None:\n    import torch\n    from transformers import (\n        AutoModelForCausalLM,\n        AutoTokenizer,\n        PreTrainedModel,\n        pipeline,\n    )\n\n    # use_fast=False is ignored by transformers\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n    self.config = config\n    torch_dtypes: dict[str, torch.dtype] = {\n        \"bfloat16\": torch.bfloat16,\n        \"float16\": torch.float16,\n        \"float32\": torch.float32,\n        \"float64\": torch.float64,\n    }\n\n    model_kwargs = config.extra_kwargs or {}\n    if config.torch_dtype in torch_dtypes:\n        model_kwargs[\"torch_dtype\"] = torch_dtypes[config.torch_dtype]\n\n    tokenizer = AutoTokenizer.from_pretrained(config.name, use_fast=False)\n    model: PreTrainedModel = AutoModelForCausalLM.from_pretrained(  # type: ignore[no-any-unimported]\n        config.name, **model_kwargs\n    )\n\n    loaded_loras = set()\n    for lora in config.loras:\n        _LOGGER.info(\"Loading adapter %s\", lora)\n        if not Path(lora / \"adapter_model.safetensors\").exists():\n            _LOGGER.warning(\"Adapter %s does not exist\", lora)\n            continue\n        model.load_adapter(lora)\n        loaded_loras.add(lora)\n        train_params = Path(lora) / \"training_parameters.json\"\n        if train_params.exists() and tokenizer.pad_token_id is None:\n            from gerd.training.lora import LoraTrainingConfig\n\n            with open(train_params, \"r\") as f:\n                lora_config = LoraTrainingConfig.model_validate_json(f.read())\n                tokenizer.pad_token_id = lora_config.pad_token_id\n                # https://github.com/huggingface/transformers/issues/34842#issuecomment-2490994584\n                tokenizer.padding_side = (\n                    \"left\" if lora_config.padding_side == \"right\" else \"right\"\n                )\n                # tokenizer.padding_side = lora_config.padding_side\n\n    if loaded_loras:\n        model.enable_adapters()\n\n    self._pipe = pipeline(\n        task=\"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        # device_map=\"auto\",  # https://github.com/huggingface/transformers/issues/31922\n        device=(\n            \"cuda\"\n            if config.gpu_layers &gt; 0\n            else \"mps\"\n            if torch.backends.mps.is_available()\n            else \"cpu\"\n        ),\n        framework=\"pt\",\n        use_fast=False,\n    )\n</code></pre>"},{"location":"reference/gerd/loader/#gerd.loader.TransformerLLM(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/loader/#gerd.loader.TransformerLLM.create_chat_completion","title":"create_chat_completion","text":"<pre><code>create_chat_completion(messages: list[ChatMessage]) -&gt; tuple[ChatRole, str]\n</code></pre> <p>Create a chat completion based on a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>list[ChatMessage]</code> <p>The list of messages in the chat history</p> required <p>Returns:</p> Type Description <code>tuple[ChatRole, str]</code> <p>The role of the generated message and the content</p> Source code in <code>gerd/loader.py</code> <pre><code>@override\ndef create_chat_completion(\n    self, messages: list[ChatMessage]\n) -&gt; tuple[ChatRole, str]:\n    msg = self._pipe(\n        messages,\n        max_new_tokens=self.config.max_new_tokens,\n        repetition_penalty=self.config.repetition_penalty,\n        top_k=self.config.top_k,\n        top_p=self.config.top_p,\n        temperature=self.config.temperature,\n        do_sample=True,\n    )[0][\"generated_text\"][-1]\n    return (msg[\"role\"], msg[\"content\"].strip())\n</code></pre>"},{"location":"reference/gerd/loader/#gerd.loader.TransformerLLM.create_chat_completion(messages)","title":"<code>messages</code>","text":""},{"location":"reference/gerd/loader/#gerd.loader.TransformerLLM.generate","title":"generate","text":"<pre><code>generate(prompt: str) -&gt; str\n</code></pre> <p>Generate text based on a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The prompt to generate text from</p> required <p>Returns:</p> Type Description <code>str</code> <p>The generated text</p> Source code in <code>gerd/loader.py</code> <pre><code>@override\ndef generate(self, prompt: str) -&gt; str:\n    res = self._pipe(\n        prompt,\n        max_new_tokens=self.config.max_new_tokens,\n        repetition_penalty=self.config.repetition_penalty,\n        top_k=self.config.top_k,\n        top_p=self.config.top_p,\n        temperature=self.config.temperature,\n        do_sample=True,\n    )\n    output: str = res[0][\"generated_text\"]\n    return output\n</code></pre>"},{"location":"reference/gerd/loader/#gerd.loader.TransformerLLM.generate(prompt)","title":"<code>prompt</code>","text":""},{"location":"reference/gerd/loader/#gerd.loader.load_model_from_config","title":"load_model_from_config","text":"<pre><code>load_model_from_config(config: ModelConfig) -&gt; LLM\n</code></pre> <p>Loads a language model based on the configuration.</p> <p>Which language model is loaded depends on the configuration. For instance, if an endpoint is provided, a remote language model is loaded. If a file is provided, Llama.cpp is used. Otherwise, transformers is used.</p> <p>Parameters:</p> Name Type Description Default <code>ModelConfig</code> <p>The configuration for the language model</p> required <p>Returns:</p> Type Description <code>LLM</code> <p>The loaded language model</p> Source code in <code>gerd/loader.py</code> <pre><code>def load_model_from_config(config: ModelConfig) -&gt; LLM:\n    \"\"\"Loads a language model based on the configuration.\n\n    Which language model is loaded depends on the configuration.\n    For instance, if an endpoint is provided, a remote language model is loaded.\n    If a file is provided, Llama.cpp is used.\n    Otherwise, transformers is used.\n\n    Parameters:\n        config: The configuration for the language model\n\n    Returns:\n        The loaded language model\n\n    \"\"\"\n    if config.endpoint:\n        _LOGGER.info(\"Using remote endpoint %s\", config.endpoint.url)\n        return RemoteLLM(config)\n    if config.file:\n        _LOGGER.info(\n            \"Using Llama.cpp with model %s and file %s\", config.name, config.file\n        )\n        return LlamaCppLLM(config)\n    _LOGGER.info(\"Using transformers with model %s\", config.name)\n    return TransformerLLM(config)\n</code></pre>"},{"location":"reference/gerd/loader/#gerd.loader.load_model_from_config(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/models/","title":"gerd.models","text":""},{"location":"reference/gerd/models/#gerd.models","title":"gerd.models","text":"<p>Pydantic model definitions and data classes that are share accross modules.</p> <p>Modules:</p> Name Description <code>gen</code> <p>Models for the generation and chat service.</p> <code>label</code> <p>Data definitions for Label Studio tasks.</p> <code>logging</code> <p>Logging configuration and utilities.</p> <code>model</code> <p>Model configuration for supported model classes.</p> <code>qa</code> <p>Data definitions for QA model configuration.</p> <code>server</code> <p>Server configuration model for REST backends.</p>"},{"location":"reference/gerd/models/gen/","title":"gerd.models.gen","text":""},{"location":"reference/gerd/models/gen/#gerd.models.gen","title":"gerd.models.gen","text":"<p>Models for the generation and chat service.</p> <p>Classes:</p> Name Description <code>GenerationConfig</code> <p>Configuration for the generation services.</p> <code>GenerationFeaturesConfig</code> <p>Configuration for the generation-specific features.</p>"},{"location":"reference/gerd/models/gen/#gerd.models.gen.GenerationConfig","title":"GenerationConfig","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration for the generation services.</p> <p>A configuration can be used for the GenerationService or the ChatService. Both support to generate text based on a prompt.</p> <p>Methods:</p> Name Description <code>settings_customise_sources</code> <p>Customize the settings sources used by pydantic-settings.</p> <p>Attributes:</p> Name Type Description <code>features</code> <code>GenerationFeaturesConfig</code> <p>The extra features to be used for the generation service.</p> <code>model</code> <code>ModelConfig</code> <p>The model to be used for the generation service.</p>"},{"location":"reference/gerd/models/gen/#gerd.models.gen.GenerationConfig.features","title":"features  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>features: GenerationFeaturesConfig = GenerationFeaturesConfig()\n</code></pre> <p>The extra features to be used for the generation service.</p>"},{"location":"reference/gerd/models/gen/#gerd.models.gen.GenerationConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: ModelConfig = ModelConfig()\n</code></pre> <p>The model to be used for the generation service.</p>"},{"location":"reference/gerd/models/gen/#gerd.models.gen.GenerationConfig.settings_customise_sources","title":"settings_customise_sources  <code>classmethod</code>","text":"<pre><code>settings_customise_sources(_: Type[BaseSettings], init_settings: PydanticBaseSettingsSource, env_settings: PydanticBaseSettingsSource, dotenv_settings: PydanticBaseSettingsSource, file_secret_settings: PydanticBaseSettingsSource) -&gt; Tuple[PydanticBaseSettingsSource, ...]\n</code></pre> <p>Customize the settings sources used by pydantic-settings.</p> <p>The order of the sources is important. The first source has the highest priority.</p> <p>Parameters:</p> Name Type Description Default <p>The class of the settings.</p> required <code>PydanticBaseSettingsSource</code> <p>The settings from the initialization.</p> required <code>PydanticBaseSettingsSource</code> <p>The settings from the environment.</p> required <code>PydanticBaseSettingsSource</code> <p>The settings from the dotenv file.</p> required <code>PydanticBaseSettingsSource</code> <p>The settings from the secret file.</p> required <p>Returns:</p> Type Description <code>Tuple[PydanticBaseSettingsSource, ...]</code> <p>The customized settings sources.</p> Source code in <code>gerd/models/gen.py</code> <pre><code>@classmethod\ndef settings_customise_sources(\n    cls,\n    _: Type[BaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,\n    file_secret_settings: PydanticBaseSettingsSource,\n) -&gt; Tuple[PydanticBaseSettingsSource, ...]:\n    \"\"\"Customize the settings sources used by pydantic-settings.\n\n    The order of the sources is important.\n    The first source has the highest priority.\n\n    Parameters:\n        cls: The class of the settings.\n        init_settings: The settings from the initialization.\n        env_settings: The settings from the environment.\n        dotenv_settings: The settings from the dotenv file.\n        file_secret_settings: The settings from the secret file.\n\n    Returns:\n        The customized settings sources.\n    \"\"\"\n    return (\n        file_secret_settings,\n        env_settings,\n        dotenv_settings,\n        init_settings,\n    )\n</code></pre>"},{"location":"reference/gerd/models/gen/#gerd.models.gen.GenerationConfig.settings_customise_sources(cls)","title":"<code>cls</code>","text":""},{"location":"reference/gerd/models/gen/#gerd.models.gen.GenerationConfig.settings_customise_sources(init_settings)","title":"<code>init_settings</code>","text":""},{"location":"reference/gerd/models/gen/#gerd.models.gen.GenerationConfig.settings_customise_sources(env_settings)","title":"<code>env_settings</code>","text":""},{"location":"reference/gerd/models/gen/#gerd.models.gen.GenerationConfig.settings_customise_sources(dotenv_settings)","title":"<code>dotenv_settings</code>","text":""},{"location":"reference/gerd/models/gen/#gerd.models.gen.GenerationConfig.settings_customise_sources(file_secret_settings)","title":"<code>file_secret_settings</code>","text":""},{"location":"reference/gerd/models/gen/#gerd.models.gen.GenerationFeaturesConfig","title":"GenerationFeaturesConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the generation-specific features.</p> <p>Attributes:</p> Name Type Description <code>prompt_chaining</code> <code>PromptChainingConfig | None</code> <p>Configuration for prompt chaining.</p>"},{"location":"reference/gerd/models/gen/#gerd.models.gen.GenerationFeaturesConfig.prompt_chaining","title":"prompt_chaining  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prompt_chaining: PromptChainingConfig | None = None\n</code></pre> <p>Configuration for prompt chaining.</p>"},{"location":"reference/gerd/models/label/","title":"gerd.models.label","text":""},{"location":"reference/gerd/models/label/#gerd.models.label","title":"gerd.models.label","text":"<p>Data definitions for Label Studio tasks.</p> <p>The defined models and enums are used to parse and work with Label Studio data exported as JSON.</p> <p>Classes:</p> Name Description <code>LabelStudioAnnotation</code> <p>Annotation of a Label Studio task.</p> <code>LabelStudioAnnotationResult</code> <p>Result of a Label Studio annotation.</p> <code>LabelStudioAnnotationValue</code> <p>Value of a Label Studio annotation.</p> <code>LabelStudioLabel</code> <p>Labels for the GRASCCO Label Studio annotations.</p> <code>LabelStudioTask</code> <p>Task of a Label Studio project.</p> <p>Functions:</p> Name Description <code>load_label_studio_tasks</code> <p>Load Label Studio tasks from a JSON file.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation","title":"LabelStudioAnnotation","text":"<p>               Bases: <code>BaseModel</code></p> <p>Annotation of a Label Studio task.</p> <p>A collection of annotations is associated with a task.</p> <p>Attributes:</p> Name Type Description <code>completed_by</code> <code>int</code> <p>The user ID of the user who completed the annotation.</p> <code>created_at</code> <code>str</code> <p>The creation date of the annotation.</p> <code>draft_created_at</code> <code>Optional[str]</code> <p>The creation date of the draft.</p> <code>ground_truth</code> <code>bool</code> <p>Whether the annotation is ground truth.</p> <code>id</code> <code>int</code> <p>The ID of the annotation.</p> <code>import_id</code> <code>Optional[str]</code> <p>The import ID of the annotation.</p> <code>last_action</code> <code>Optional[str]</code> <p>The last action of the annotation.</p> <code>last_created_by</code> <code>Optional[int]</code> <p>The user ID of the user who last created the annotation.</p> <code>lead_time</code> <code>float</code> <p>The lead time of the annotation.</p> <code>parent_annotation</code> <code>Optional[str]</code> <p>The parent annotation.</p> <code>parent_prediction</code> <code>Optional[str]</code> <p>The parent prediction.</p> <code>prediction</code> <code>Dict[str, str]</code> <p>The prediction of the annotation.</p> <code>project</code> <code>int</code> <p>The project ID of the annotation.</p> <code>result</code> <code>List[LabelStudioAnnotationResult]</code> <p>The results of the annotation.</p> <code>result_count</code> <code>int</code> <p>The number of results.</p> <code>task</code> <code>int</code> <p>The task ID of the annotation.</p> <code>unique_id</code> <code>str</code> <p>The unique ID of the annotation.</p> <code>updated_at</code> <code>str</code> <p>The update date of the annotation.</p> <code>updated_by</code> <code>int</code> <p>The user ID of the user who updated the annotation.</p> <code>was_cancelled</code> <code>bool</code> <p>Whether the annotation was cancelled.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.completed_by","title":"completed_by  <code>instance-attribute</code>","text":"<pre><code>completed_by: int\n</code></pre> <p>The user ID of the user who completed the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: str\n</code></pre> <p>The creation date of the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.draft_created_at","title":"draft_created_at  <code>instance-attribute</code>","text":"<pre><code>draft_created_at: Optional[str]\n</code></pre> <p>The creation date of the draft.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.ground_truth","title":"ground_truth  <code>instance-attribute</code>","text":"<pre><code>ground_truth: bool\n</code></pre> <p>Whether the annotation is ground truth.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: int\n</code></pre> <p>The ID of the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.import_id","title":"import_id  <code>instance-attribute</code>","text":"<pre><code>import_id: Optional[str]\n</code></pre> <p>The import ID of the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.last_action","title":"last_action  <code>instance-attribute</code>","text":"<pre><code>last_action: Optional[str]\n</code></pre> <p>The last action of the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.last_created_by","title":"last_created_by  <code>instance-attribute</code>","text":"<pre><code>last_created_by: Optional[int]\n</code></pre> <p>The user ID of the user who last created the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.lead_time","title":"lead_time  <code>instance-attribute</code>","text":"<pre><code>lead_time: float\n</code></pre> <p>The lead time of the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.parent_annotation","title":"parent_annotation  <code>instance-attribute</code>","text":"<pre><code>parent_annotation: Optional[str]\n</code></pre> <p>The parent annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.parent_prediction","title":"parent_prediction  <code>instance-attribute</code>","text":"<pre><code>parent_prediction: Optional[str]\n</code></pre> <p>The parent prediction.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.prediction","title":"prediction  <code>instance-attribute</code>","text":"<pre><code>prediction: Dict[str, str]\n</code></pre> <p>The prediction of the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.project","title":"project  <code>instance-attribute</code>","text":"<pre><code>project: int\n</code></pre> <p>The project ID of the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.result","title":"result  <code>instance-attribute</code>","text":"<pre><code>result: List[LabelStudioAnnotationResult]\n</code></pre> <p>The results of the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.result_count","title":"result_count  <code>instance-attribute</code>","text":"<pre><code>result_count: int\n</code></pre> <p>The number of results.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.task","title":"task  <code>instance-attribute</code>","text":"<pre><code>task: int\n</code></pre> <p>The task ID of the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.unique_id","title":"unique_id  <code>instance-attribute</code>","text":"<pre><code>unique_id: str\n</code></pre> <p>The unique ID of the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.updated_at","title":"updated_at  <code>instance-attribute</code>","text":"<pre><code>updated_at: str\n</code></pre> <p>The update date of the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.updated_by","title":"updated_by  <code>instance-attribute</code>","text":"<pre><code>updated_by: int\n</code></pre> <p>The user ID of the user who updated the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotation.was_cancelled","title":"was_cancelled  <code>instance-attribute</code>","text":"<pre><code>was_cancelled: bool\n</code></pre> <p>Whether the annotation was cancelled.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotationResult","title":"LabelStudioAnnotationResult","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result of a Label Studio annotation.</p> <p>Attributes:</p> Name Type Description <code>from_name</code> <code>str</code> <p>The name of the source.</p> <code>id</code> <code>str</code> <p>The ID of the result.</p> <code>origin</code> <code>str</code> <p>The origin of the result.</p> <code>to_name</code> <code>str</code> <p>The name of the target.</p> <code>type</code> <code>str</code> <p>The type of the result.</p> <code>value</code> <code>LabelStudioAnnotationValue</code> <p>The value of the result.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotationResult.from_name","title":"from_name  <code>instance-attribute</code>","text":"<pre><code>from_name: str\n</code></pre> <p>The name of the source.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotationResult.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The ID of the result.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotationResult.origin","title":"origin  <code>instance-attribute</code>","text":"<pre><code>origin: str\n</code></pre> <p>The origin of the result.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotationResult.to_name","title":"to_name  <code>instance-attribute</code>","text":"<pre><code>to_name: str\n</code></pre> <p>The name of the target.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotationResult.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: str\n</code></pre> <p>The type of the result.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotationResult.value","title":"value  <code>instance-attribute</code>","text":"<pre><code>value: LabelStudioAnnotationValue\n</code></pre> <p>The value of the result.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotationValue","title":"LabelStudioAnnotationValue","text":"<p>               Bases: <code>BaseModel</code></p> <p>Value of a Label Studio annotation.</p> <p>Attributes:</p> Name Type Description <code>end</code> <code>int</code> <p>The end of the annotation.</p> <code>labels</code> <code>List[LabelStudioLabel]</code> <p>The labels of the annotation.</p> <code>start</code> <code>int</code> <p>The start of the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotationValue.end","title":"end  <code>instance-attribute</code>","text":"<pre><code>end: int\n</code></pre> <p>The end of the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotationValue.labels","title":"labels  <code>instance-attribute</code>","text":"<pre><code>labels: List[LabelStudioLabel]\n</code></pre> <p>The labels of the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioAnnotationValue.start","title":"start  <code>instance-attribute</code>","text":"<pre><code>start: int\n</code></pre> <p>The start of the annotation.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioLabel","title":"LabelStudioLabel","text":"<p>               Bases: <code>Enum</code></p> <p>Labels for the GRASCCO Label Studio annotations.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask","title":"LabelStudioTask","text":"<p>               Bases: <code>BaseModel</code></p> <p>Task of a Label Studio project.</p> <p>A task is a single unit of work that can be annotated by a user. Tasks can be used to train an auto labeler or to evaluate the performance of a model.</p> <p>Attributes:</p> Name Type Description <code>annotations</code> <code>List[LabelStudioAnnotation]</code> <p>The annotations of the task.</p> <code>cancelled_annotations</code> <code>int</code> <p>The number of cancelled annotations.</p> <code>comment_authors</code> <code>List[str]</code> <p>The authors of the comments.</p> <code>comment_count</code> <code>int</code> <p>The number of comments.</p> <code>created_at</code> <code>str</code> <p>The creation date of the task.</p> <code>data</code> <code>Optional[Dict[str, str]]</code> <p>The data of the task.</p> <code>drafts</code> <code>List[str]</code> <p>The drafts of the task.</p> <code>file_name</code> <code>str</code> <p>Extracts the original file name from the file upload.</p> <code>file_upload</code> <code>str</code> <p>The file upload of the task.</p> <code>id</code> <code>int</code> <p>The ID of the task.</p> <code>inner_id</code> <code>int</code> <p>The inner ID of the task.</p> <code>last_comment_updated_at</code> <code>Optional[str]</code> <p>The update date of the last comment.</p> <code>meta</code> <code>Optional[Dict[str, str]]</code> <p>The meta data of the task.</p> <code>predictions</code> <code>List[str]</code> <p>The predictions of the task.</p> <code>project</code> <code>int</code> <p>The project ID of the task.</p> <code>total_annotations</code> <code>int</code> <p>The total number of annotations.</p> <code>total_predictions</code> <code>int</code> <p>The total number of predictions.</p> <code>unresolved_comment_count</code> <code>int</code> <p>The number of unresolved comments.</p> <code>updated_at</code> <code>str</code> <p>The update date of the task.</p> <code>updated_by</code> <code>int</code> <p>The user ID of the user who updated the task.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.annotations","title":"annotations  <code>instance-attribute</code>","text":"<pre><code>annotations: List[LabelStudioAnnotation]\n</code></pre> <p>The annotations of the task.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.cancelled_annotations","title":"cancelled_annotations  <code>instance-attribute</code>","text":"<pre><code>cancelled_annotations: int\n</code></pre> <p>The number of cancelled annotations.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.comment_authors","title":"comment_authors  <code>instance-attribute</code>","text":"<pre><code>comment_authors: List[str]\n</code></pre> <p>The authors of the comments.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.comment_count","title":"comment_count  <code>instance-attribute</code>","text":"<pre><code>comment_count: int\n</code></pre> <p>The number of comments.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: str\n</code></pre> <p>The creation date of the task.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: Optional[Dict[str, str]]\n</code></pre> <p>The data of the task.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.drafts","title":"drafts  <code>instance-attribute</code>","text":"<pre><code>drafts: List[str]\n</code></pre> <p>The drafts of the task.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.file_name","title":"file_name  <code>property</code>","text":"<pre><code>file_name: str\n</code></pre> <p>Extracts the original file name from the file upload.</p> <p>File uploads are stored as <code>project-id-filename</code> format to be unique.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.file_upload","title":"file_upload  <code>instance-attribute</code>","text":"<pre><code>file_upload: str\n</code></pre> <p>The file upload of the task.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: int\n</code></pre> <p>The ID of the task.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.inner_id","title":"inner_id  <code>instance-attribute</code>","text":"<pre><code>inner_id: int\n</code></pre> <p>The inner ID of the task.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.last_comment_updated_at","title":"last_comment_updated_at  <code>instance-attribute</code>","text":"<pre><code>last_comment_updated_at: Optional[str]\n</code></pre> <p>The update date of the last comment.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.meta","title":"meta  <code>instance-attribute</code>","text":"<pre><code>meta: Optional[Dict[str, str]]\n</code></pre> <p>The meta data of the task.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.predictions","title":"predictions  <code>instance-attribute</code>","text":"<pre><code>predictions: List[str]\n</code></pre> <p>The predictions of the task.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.project","title":"project  <code>instance-attribute</code>","text":"<pre><code>project: int\n</code></pre> <p>The project ID of the task.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.total_annotations","title":"total_annotations  <code>instance-attribute</code>","text":"<pre><code>total_annotations: int\n</code></pre> <p>The total number of annotations.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.total_predictions","title":"total_predictions  <code>instance-attribute</code>","text":"<pre><code>total_predictions: int\n</code></pre> <p>The total number of predictions.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.unresolved_comment_count","title":"unresolved_comment_count  <code>instance-attribute</code>","text":"<pre><code>unresolved_comment_count: int\n</code></pre> <p>The number of unresolved comments.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.updated_at","title":"updated_at  <code>instance-attribute</code>","text":"<pre><code>updated_at: str\n</code></pre> <p>The update date of the task.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.LabelStudioTask.updated_by","title":"updated_by  <code>instance-attribute</code>","text":"<pre><code>updated_by: int\n</code></pre> <p>The user ID of the user who updated the task.</p>"},{"location":"reference/gerd/models/label/#gerd.models.label.load_label_studio_tasks","title":"load_label_studio_tasks","text":"<pre><code>load_label_studio_tasks(file_path: str) -&gt; List[LabelStudioTask]\n</code></pre> <p>Load Label Studio tasks from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The path to the JSON file.</p> required <p>Returns:</p> Type Description <code>List[LabelStudioTask]</code> <p>The loaded Label Studio tasks</p> Source code in <code>gerd/models/label.py</code> <pre><code>def load_label_studio_tasks(file_path: str) -&gt; List[LabelStudioTask]:\n    \"\"\"Load Label Studio tasks from a JSON file.\n\n    Parameters:\n        file_path: The path to the JSON file.\n\n    Returns:\n        The loaded Label Studio tasks\n    \"\"\"\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        obj = json.load(f)\n\n    tasks = TypeAdapter(List[LabelStudioTask]).validate_python(obj)\n    return tasks\n</code></pre>"},{"location":"reference/gerd/models/label/#gerd.models.label.load_label_studio_tasks(file_path)","title":"<code>file_path</code>","text":""},{"location":"reference/gerd/models/logging/","title":"gerd.models.logging","text":""},{"location":"reference/gerd/models/logging/#gerd.models.logging","title":"gerd.models.logging","text":"<p>Logging configuration and utilities.</p> <p>Classes:</p> Name Description <code>LogLevel</code> <p>Wrapper for string-based log levels.</p> <code>LoggingConfig</code> <p>Configuration for logging.</p>"},{"location":"reference/gerd/models/logging/#gerd.models.logging.LogLevel","title":"LogLevel","text":"<p>               Bases: <code>Enum</code></p> <p>Wrapper for string-based log levels.</p> <p>Translates log levels to integers for Python's logging framework.</p> <p>Methods:</p> Name Description <code>as_int</code> <p>Convert the log level to an integer.</p>"},{"location":"reference/gerd/models/logging/#gerd.models.logging.LogLevel.as_int","title":"as_int","text":"<pre><code>as_int() -&gt; int\n</code></pre> <p>Convert the log level to an integer.</p> Source code in <code>gerd/models/logging.py</code> <pre><code>def as_int(self) -&gt; int:\n    \"\"\"Convert the log level to an integer.\"\"\"\n    return {\n        LogLevel.DEBUG: 10,\n        LogLevel.INFO: 20,\n        LogLevel.WARNING: 30,\n        LogLevel.ERROR: 40,\n        LogLevel.FATAL: 50,\n    }[self]\n</code></pre>"},{"location":"reference/gerd/models/logging/#gerd.models.logging.LoggingConfig","title":"LoggingConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for logging.</p> <p>Attributes:</p> Name Type Description <code>level</code> <code>LogLevel</code> <p>The log level.</p>"},{"location":"reference/gerd/models/logging/#gerd.models.logging.LoggingConfig.level","title":"level  <code>instance-attribute</code>","text":"<pre><code>level: LogLevel\n</code></pre> <p>The log level.</p>"},{"location":"reference/gerd/models/model/","title":"gerd.models.model","text":""},{"location":"reference/gerd/models/model/#gerd.models.model","title":"gerd.models.model","text":"<p>Model configuration for supported model classes.</p> <p>Classes:</p> Name Description <code>ChatMessage</code> <p>Data structure for chat messages.</p> <code>ModelConfig</code> <p>Configuration for large language models.</p> <code>ModelEndpoint</code> <p>Configuration for model endpoints where models are hosted remotely.</p> <code>PromptConfig</code> <p>Configuration for prompts.</p> <code>PromptConfigBase</code> <p>Parameters for a prompt configuration.</p> <p>Attributes:</p> Name Type Description <code>ChatRole</code> <p>Currently supported chat roles.</p> <code>EndpointType</code> <p>Endpoint for remote llm services.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ChatRole","title":"ChatRole  <code>module-attribute</code>","text":"<pre><code>ChatRole = Literal['system', 'user', 'assistant']\n</code></pre> <p>Currently supported chat roles.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.EndpointType","title":"EndpointType  <code>module-attribute</code>","text":"<pre><code>EndpointType = Literal['llama.cpp', 'openai']\n</code></pre> <p>Endpoint for remote llm services.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ChatMessage","title":"ChatMessage","text":"<p>               Bases: <code>TypedDict</code></p> <p>Data structure for chat messages.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str</code> <p>The content of the chat message.</p> <code>role</code> <code>ChatRole</code> <p>The role or source of the chat message.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ChatMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: str\n</code></pre> <p>The content of the chat message.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ChatMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: ChatRole\n</code></pre> <p>The role or source of the chat message.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig","title":"ModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for large language models.</p> <p>Most llm libraries and/or services share common parameters for configuration. Explaining each parameter is out of scope for this documentation. The most essential parameters are explained for instance here. Default values have been chosen according to ctransformers library.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The batch size for the generation.</p> <code>context_length</code> <code>int</code> <p>The context length for the model. Currently only LLaMA, MPT and Falcon</p> <code>endpoint</code> <code>Optional[ModelEndpoint]</code> <p>The endpoint of the model when hosted remotely.</p> <code>extra_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>Additional keyword arguments for the model library.</p> <code>file</code> <code>Optional[str]</code> <p>The path to the model file. For local models only.</p> <code>gpu_layers</code> <code>int</code> <p>The number of layers to run on the GPU.</p> <code>last_n_tokens</code> <code>int</code> <p>The number of tokens to consider for the repetition penalty.</p> <code>loras</code> <code>set[Path]</code> <p>The list of additional LoRAs files to load.</p> <code>max_new_tokens</code> <code>int</code> <p>The maximum number of new tokens to generate.</p> <code>name</code> <code>str</code> <p>The name of the model. Can be a path to a local model or a huggingface handle.</p> <code>prompt_config</code> <code>PromptConfig</code> <p>The prompt configuration.</p> <code>prompt_setup</code> <code>List[Tuple[Literal['system', 'user', 'assistant'], PromptConfig]]</code> <p>A list of predefined prompts for the model.</p> <code>repetition_penalty</code> <code>float</code> <p>The repetition penalty.</p> <code>seed</code> <code>int</code> <p>The seed for the random number generator.</p> <code>stop</code> <code>Optional[List[str]]</code> <p>The stop tokens for the generation.</p> <code>stream</code> <code>bool</code> <p>Whether to stream the output.</p> <code>temperature</code> <code>float</code> <p>The temperature for the sampling.</p> <code>threads</code> <code>Optional[int]</code> <p>The number of threads to use for the generation.</p> <code>top_k</code> <code>int</code> <p>The number of tokens to consider for the top-k sampling.</p> <code>top_p</code> <code>float</code> <p>The cumulative probability for the top-p sampling.</p> <code>torch_dtype</code> <code>Optional[str]</code> <p>The torch data type for the model.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre> <p>The batch size for the generation.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.context_length","title":"context_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>context_length: int = 0\n</code></pre> <p>The context length for the model. Currently only LLaMA, MPT and Falcon</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.endpoint","title":"endpoint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>endpoint: Optional[ModelEndpoint] = None\n</code></pre> <p>The endpoint of the model when hosted remotely.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.extra_kwargs","title":"extra_kwargs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extra_kwargs: Optional[dict[str, Any]] = None\n</code></pre> <p>Additional keyword arguments for the model library.</p> <p>The accepted keys and values depend on the model library used.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.file","title":"file  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>file: Optional[str] = None\n</code></pre> <p>The path to the model file. For local models only.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.gpu_layers","title":"gpu_layers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gpu_layers: int = 0\n</code></pre> <p>The number of layers to run on the GPU.</p> <p>The actual number is only used llama.cpp. The other model libraries will determine whether to run on the GPU just by checking of this value is larger than 0.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.last_n_tokens","title":"last_n_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>last_n_tokens: int = 64\n</code></pre> <p>The number of tokens to consider for the repetition penalty.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.loras","title":"loras  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>loras: set[Path] = set()\n</code></pre> <p>The list of additional LoRAs files to load.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.max_new_tokens","title":"max_new_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_new_tokens: int = 256\n</code></pre> <p>The maximum number of new tokens to generate.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = 'Qwen/Qwen2.5-0.5B-Instruct'\n</code></pre> <p>The name of the model. Can be a path to a local model or a huggingface handle.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.prompt_config","title":"prompt_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prompt_config: PromptConfig = PromptConfig()\n</code></pre> <p>The prompt configuration.</p> <p>This is used to process the input passed to the services.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.prompt_setup","title":"prompt_setup  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prompt_setup: List[Tuple[Literal['system', 'user', 'assistant'], PromptConfig]] = []\n</code></pre> <p>A list of predefined prompts for the model.</p> <p>When a model context is inialized or reset, this will be used to set up the context.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.repetition_penalty","title":"repetition_penalty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>repetition_penalty: float = 1.1\n</code></pre> <p>The repetition penalty.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.seed","title":"seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>seed: int = -1\n</code></pre> <p>The seed for the random number generator.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.stop","title":"stop  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stop: Optional[List[str]] = None\n</code></pre> <p>The stop tokens for the generation.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.stream","title":"stream  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stream: bool = False\n</code></pre> <p>Whether to stream the output.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.8\n</code></pre> <p>The temperature for the sampling.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.threads","title":"threads  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>threads: Optional[int] = None\n</code></pre> <p>The number of threads to use for the generation.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.top_k","title":"top_k  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>top_k: int = 40\n</code></pre> <p>The number of tokens to consider for the top-k sampling.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.top_p","title":"top_p  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>top_p: float = 0.95\n</code></pre> <p>The cumulative probability for the top-p sampling.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelConfig.torch_dtype","title":"torch_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>torch_dtype: Optional[str] = None\n</code></pre> <p>The torch data type for the model.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelEndpoint","title":"ModelEndpoint","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for model endpoints where models are hosted remotely.</p> <p>Attributes:</p> Name Type Description <code>key</code> <code>Optional[SecretStr]</code> <p>Token to use the remote service</p> <code>type</code> <code>EndpointType</code> <p>Type of the remote service</p> <code>url</code> <code>str</code> <p>URL of the remote service</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelEndpoint.key","title":"key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>key: Optional[SecretStr] = None\n</code></pre> <p>Token to use the remote service</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelEndpoint.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: EndpointType\n</code></pre> <p>Type of the remote service</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.ModelEndpoint.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: str\n</code></pre> <p>URL of the remote service</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.PromptConfig","title":"PromptConfig","text":"<p>               Bases: <code>PromptConfigBase</code></p> <p>Configuration for prompts.</p> <p>Methods:</p> Name Description <code>as_base</code> <p>Returns parameter of the prompt configurations.</p> <code>format</code> <p>Format the prompt with the given parameters.</p> <code>model_post_init</code> <p>Post-initialization hook for pyandic.</p> <p>Attributes:</p> Name Type Description <code>is_template</code> <code>bool</code> <p>Whether the config uses jinja2 templates.</p> <code>parameters</code> <code>list[str]</code> <p>Retrieves and returns the parameters of the prompt.</p> <code>path</code> <code>Optional[str]</code> <p>The path to an external prompt file.</p> <code>text</code> <code>str</code> <p>The text of the prompt. Can contain placeholders.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.PromptConfig.is_template","title":"is_template  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_template: bool = False\n</code></pre> <p>Whether the config uses jinja2 templates.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.PromptConfig.parameters","title":"parameters  <code>property</code>","text":"<pre><code>parameters: list[str]\n</code></pre> <p>Retrieves and returns the parameters of the prompt.</p> <p>This happens on-the-fly and is not stored in the model.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>The parameters of the prompt.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.PromptConfig.path","title":"path  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>path: Optional[str] = None\n</code></pre> <p>The path to an external prompt file.</p> <p>This will overload the values of text and/or template.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.PromptConfig.text","title":"text  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>text: str = '{message}'\n</code></pre> <p>The text of the prompt. Can contain placeholders.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.PromptConfig.as_base","title":"as_base","text":"<pre><code>as_base() -&gt; PromptConfigBase\n</code></pre> <p>Returns parameter of the prompt configurations.</p> <p>Returns:</p> Type Description <code>PromptConfigBase</code> <p>Basic prompt configuration parameters</p> Source code in <code>gerd/models/model.py</code> <pre><code>def as_base(self) -&gt; PromptConfigBase:\n    \"\"\"Returns parameter of the prompt configurations.\n\n    Returns:\n        Basic prompt configuration parameters\n    \"\"\"\n    return PromptConfigBase(\n        text=self.text, path=self.path, is_template=self.is_template\n    )\n</code></pre>"},{"location":"reference/gerd/models/model/#gerd.models.model.PromptConfig.format","title":"format","text":"<pre><code>format(parameters: Mapping[str, str | list[ChatMessage]] | None = None) -&gt; str\n</code></pre> <p>Format the prompt with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>Mapping[str, str | list[ChatMessage]] | None</code> <p>The parameters to format the prompt with.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The formatted prompt</p> Source code in <code>gerd/models/model.py</code> <pre><code>def format(\n    self, parameters: Mapping[str, str | list[ChatMessage]] | None = None\n) -&gt; str:\n    \"\"\"Format the prompt with the given parameters.\n\n    Parameters:\n        parameters: The parameters to format the prompt with.\n\n    Returns:\n        The formatted prompt\n    \"\"\"\n    if parameters is None:\n        parameters = {}\n    return (\n        self.template.render(**parameters)\n        if self.template\n        else (\n            self.text.format(**parameters)\n            if self.text\n            else \"\".join(str(parameters.values()))\n        )\n    )\n</code></pre>"},{"location":"reference/gerd/models/model/#gerd.models.model.PromptConfig.format(parameters)","title":"<code>parameters</code>","text":""},{"location":"reference/gerd/models/model/#gerd.models.model.PromptConfig.model_post_init","title":"model_post_init","text":"<pre><code>model_post_init(__context: Any) -&gt; None\n</code></pre> <p>Post-initialization hook for pyandic.</p> <p>When path is set, the text or template is read from the file and the template is created. Path ending with '.jinja2' will be treated as a template. If no path is set, the text parameter is used to initialize the template if is_template is set to True. Parameters:     __context: The context of the model (not used)</p> Source code in <code>gerd/models/model.py</code> <pre><code>def model_post_init(self, __context: Any) -&gt; None:  # noqa: ANN401\n    \"\"\"Post-initialization hook for pyandic.\n\n    When path is set, the text or template is read from the file and\n    the template is created.\n    Path ending with '.jinja2' will be treated as a template.\n    If no path is set, the text parameter is used to initialize the template\n    if is_template is set to True.\n    Parameters:\n        __context: The context of the model (not used)\n    \"\"\"\n    if self.path:\n        # reset self.text when path is set\n        self.text = \"\"\n        path = Path(self.path)\n        if path.exists():\n            with path.open(\"r\", encoding=\"utf-8\") as f:\n                self.text = f.read()\n                if self.is_template or path.suffix == \".jinja2\":\n                    self.is_template = True\n                    loader = FileSystemLoader(path.parent)\n                    env = Environment(\n                        loader=loader,\n                        autoescape=select_autoescape(\n                            disabled_extensions=(\".jinja2\",),\n                            default_for_string=True,\n                            default=True,\n                        ),\n                    )\n                    self.template = env.get_template(path.name)\n        else:\n            msg = f\"'{self.path}' does not exist!\"\n            raise ValueError(msg)\n    elif self.text and self.is_template:\n        self.template = Environment(autoescape=True).from_string(self.text)\n</code></pre>"},{"location":"reference/gerd/models/model/#gerd.models.model.PromptConfigBase","title":"PromptConfigBase","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for a prompt configuration.</p> <p>Attributes:</p> Name Type Description <code>is_template</code> <code>bool</code> <p>Whether the config uses jinja2 templates.</p> <code>path</code> <code>Optional[str]</code> <p>The path to an external prompt file.</p> <code>text</code> <code>str</code> <p>The text of the prompt. Can contain placeholders.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.PromptConfigBase.is_template","title":"is_template  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_template: bool = False\n</code></pre> <p>Whether the config uses jinja2 templates.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.PromptConfigBase.path","title":"path  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>path: Optional[str] = None\n</code></pre> <p>The path to an external prompt file.</p> <p>This will overload the values of text and/or template.</p>"},{"location":"reference/gerd/models/model/#gerd.models.model.PromptConfigBase.text","title":"text  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>text: str = '{message}'\n</code></pre> <p>The text of the prompt. Can contain placeholders.</p>"},{"location":"reference/gerd/models/qa/","title":"gerd.models.qa","text":""},{"location":"reference/gerd/models/qa/#gerd.models.qa","title":"gerd.models.qa","text":"<p>Data definitions for QA model configuration.</p> <p>Classes:</p> Name Description <code>AnalyzeConfig</code> <p>The configuration for the analyze service.</p> <code>EmbeddingConfig</code> <p>Embedding specific model configuration.</p> <code>QAConfig</code> <p>Configuration for the QA services.</p> <code>QAFeaturesConfig</code> <p>Configuration for the QA-specific features.</p>"},{"location":"reference/gerd/models/qa/#gerd.models.qa.AnalyzeConfig","title":"AnalyzeConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>The configuration for the analyze service.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>ModelConfig</code> <p>The model to be used for the analyze service.</p>"},{"location":"reference/gerd/models/qa/#gerd.models.qa.AnalyzeConfig.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: ModelConfig\n</code></pre> <p>The model to be used for the analyze service.</p>"},{"location":"reference/gerd/models/qa/#gerd.models.qa.EmbeddingConfig","title":"EmbeddingConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Embedding specific model configuration.</p> <p>Attributes:</p> Name Type Description <code>chunk_overlap</code> <code>int</code> <p>The overlap between chunks.</p> <code>chunk_size</code> <code>int</code> <p>The size of the chunks stored in the database.</p> <code>db_path</code> <code>Optional[str]</code> <p>The path to the database file.</p> <code>model</code> <code>ModelConfig</code> <p>The model used for the embedding.</p>"},{"location":"reference/gerd/models/qa/#gerd.models.qa.EmbeddingConfig.chunk_overlap","title":"chunk_overlap  <code>instance-attribute</code>","text":"<pre><code>chunk_overlap: int\n</code></pre> <p>The overlap between chunks.</p>"},{"location":"reference/gerd/models/qa/#gerd.models.qa.EmbeddingConfig.chunk_size","title":"chunk_size  <code>instance-attribute</code>","text":"<pre><code>chunk_size: int\n</code></pre> <p>The size of the chunks stored in the database.</p>"},{"location":"reference/gerd/models/qa/#gerd.models.qa.EmbeddingConfig.db_path","title":"db_path  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>db_path: Optional[str] = None\n</code></pre> <p>The path to the database file.</p>"},{"location":"reference/gerd/models/qa/#gerd.models.qa.EmbeddingConfig.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: ModelConfig\n</code></pre> <p>The model used for the embedding.</p> <p>This model should be rather small and fast to compute. Furthermore, not every model is suited for this task.</p>"},{"location":"reference/gerd/models/qa/#gerd.models.qa.QAConfig","title":"QAConfig","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration for the QA services.</p> <p>This model can be used to retrieve parameters from a variety of sources. The main source are YAML files (loaded as <code>Settings</code>) but dotenv files and environment variables can be used to situatively overwrite the values. Environment variables have to be prefixed with <code>gerd_qa_</code> to be recognized.</p> <p>Methods:</p> Name Description <code>settings_customise_sources</code> <p>Customize the settings sources used by pydantic-settings.</p> <p>Attributes:</p> Name Type Description <code>device</code> <code>str</code> <p>The device to run the model on.</p> <code>embedding</code> <code>EmbeddingConfig</code> <p>The configuration for the embedding service.</p> <code>features</code> <code>QAFeaturesConfig</code> <p>The configuration for the QA-specific features.</p> <code>model</code> <code>ModelConfig</code> <p>The model to be used for the QA service.</p>"},{"location":"reference/gerd/models/qa/#gerd.models.qa.QAConfig.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: str = 'cpu'\n</code></pre> <p>The device to run the model on.</p>"},{"location":"reference/gerd/models/qa/#gerd.models.qa.QAConfig.embedding","title":"embedding  <code>instance-attribute</code>","text":"<pre><code>embedding: EmbeddingConfig\n</code></pre> <p>The configuration for the embedding service.</p>"},{"location":"reference/gerd/models/qa/#gerd.models.qa.QAConfig.features","title":"features  <code>instance-attribute</code>","text":"<pre><code>features: QAFeaturesConfig\n</code></pre> <p>The configuration for the QA-specific features.</p>"},{"location":"reference/gerd/models/qa/#gerd.models.qa.QAConfig.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: ModelConfig\n</code></pre> <p>The model to be used for the QA service.</p>"},{"location":"reference/gerd/models/qa/#gerd.models.qa.QAConfig.settings_customise_sources","title":"settings_customise_sources  <code>classmethod</code>","text":"<pre><code>settings_customise_sources(_: Type[BaseSettings], init_settings: PydanticBaseSettingsSource, env_settings: PydanticBaseSettingsSource, dotenv_settings: PydanticBaseSettingsSource, file_secret_settings: PydanticBaseSettingsSource) -&gt; Tuple[PydanticBaseSettingsSource, ...]\n</code></pre> <p>Customize the settings sources used by pydantic-settings.</p> <p>The order of the sources is important. The first source has the highest priority.</p> <p>Parameters:</p> Name Type Description Default <p>The class of the settings.</p> required <code>PydanticBaseSettingsSource</code> <p>The settings from the initialization.</p> required <code>PydanticBaseSettingsSource</code> <p>The settings from the environment.</p> required <code>PydanticBaseSettingsSource</code> <p>The settings from the dotenv file.</p> required <code>PydanticBaseSettingsSource</code> <p>The settings from the secret file.</p> required <p>Returns:</p> Type Description <code>Tuple[PydanticBaseSettingsSource, ...]</code> <p>The customized settings sources.</p> Source code in <code>gerd/models/qa.py</code> <pre><code>@classmethod\ndef settings_customise_sources(\n    cls,\n    _: Type[BaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,\n    file_secret_settings: PydanticBaseSettingsSource,\n) -&gt; Tuple[PydanticBaseSettingsSource, ...]:\n    \"\"\"Customize the settings sources used by pydantic-settings.\n\n    The order of the sources is important.\n    The first source has the highest priority.\n\n    Parameters:\n        cls: The class of the settings.\n        init_settings: The settings from the initialization.\n        env_settings: The settings from the environment.\n        dotenv_settings: The settings from the dotenv file.\n        file_secret_settings: The settings from the secret file.\n\n    Returns:\n        The customized settings sources.\n    \"\"\"\n    return (\n        file_secret_settings,\n        env_settings,\n        dotenv_settings,\n        init_settings,\n    )\n</code></pre>"},{"location":"reference/gerd/models/qa/#gerd.models.qa.QAConfig.settings_customise_sources(cls)","title":"<code>cls</code>","text":""},{"location":"reference/gerd/models/qa/#gerd.models.qa.QAConfig.settings_customise_sources(init_settings)","title":"<code>init_settings</code>","text":""},{"location":"reference/gerd/models/qa/#gerd.models.qa.QAConfig.settings_customise_sources(env_settings)","title":"<code>env_settings</code>","text":""},{"location":"reference/gerd/models/qa/#gerd.models.qa.QAConfig.settings_customise_sources(dotenv_settings)","title":"<code>dotenv_settings</code>","text":""},{"location":"reference/gerd/models/qa/#gerd.models.qa.QAConfig.settings_customise_sources(file_secret_settings)","title":"<code>file_secret_settings</code>","text":""},{"location":"reference/gerd/models/qa/#gerd.models.qa.QAFeaturesConfig","title":"QAFeaturesConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the QA-specific features.</p> <p>Attributes:</p> Name Type Description <code>analyze</code> <code>AnalyzeConfig</code> <p>Configuration to extract letter of discharge information from the text.</p> <code>analyze_mult_prompts</code> <code>AnalyzeConfig</code> <p>Configuration to extract predefined infos with multiple prompts from the text.</p> <code>return_source</code> <code>bool</code> <p>Whether to return the source in the response.</p>"},{"location":"reference/gerd/models/qa/#gerd.models.qa.QAFeaturesConfig.analyze","title":"analyze  <code>instance-attribute</code>","text":"<pre><code>analyze: AnalyzeConfig\n</code></pre> <p>Configuration to extract letter of discharge information from the text.</p>"},{"location":"reference/gerd/models/qa/#gerd.models.qa.QAFeaturesConfig.analyze_mult_prompts","title":"analyze_mult_prompts  <code>instance-attribute</code>","text":"<pre><code>analyze_mult_prompts: AnalyzeConfig\n</code></pre> <p>Configuration to extract predefined infos with multiple prompts from the text.</p>"},{"location":"reference/gerd/models/qa/#gerd.models.qa.QAFeaturesConfig.return_source","title":"return_source  <code>instance-attribute</code>","text":"<pre><code>return_source: bool\n</code></pre> <p>Whether to return the source in the response.</p>"},{"location":"reference/gerd/models/server/","title":"gerd.models.server","text":""},{"location":"reference/gerd/models/server/#gerd.models.server","title":"gerd.models.server","text":"<p>Server configuration model for REST backends.</p> <p>Classes:</p> Name Description <code>ServerConfig</code> <p>Server configuration model for REST backends.</p>"},{"location":"reference/gerd/models/server/#gerd.models.server.ServerConfig","title":"ServerConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Server configuration model for REST backends.</p> <p>Attributes:</p> Name Type Description <code>api_prefix</code> <code>str</code> <p>The prefix of the API.</p> <code>host</code> <code>str</code> <p>The host of the server.</p> <code>port</code> <code>int</code> <p>The port of the server.</p>"},{"location":"reference/gerd/models/server/#gerd.models.server.ServerConfig.api_prefix","title":"api_prefix  <code>instance-attribute</code>","text":"<pre><code>api_prefix: str\n</code></pre> <p>The prefix of the API.</p>"},{"location":"reference/gerd/models/server/#gerd.models.server.ServerConfig.host","title":"host  <code>instance-attribute</code>","text":"<pre><code>host: str\n</code></pre> <p>The host of the server.</p>"},{"location":"reference/gerd/models/server/#gerd.models.server.ServerConfig.port","title":"port  <code>instance-attribute</code>","text":"<pre><code>port: int\n</code></pre> <p>The port of the server.</p>"},{"location":"reference/gerd/qa/","title":"gerd.qa","text":""},{"location":"reference/gerd/qa/#gerd.qa","title":"gerd.qa","text":"<p>Services and utilities for retrieval augmented generation (RAG).</p> <p>Modules:</p> Name Description <code>qa_service</code> <p>Implements the QAService class.</p>"},{"location":"reference/gerd/qa/qa_service/","title":"gerd.qa.qa_service","text":""},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service","title":"gerd.qa.qa_service","text":"<p>Implements the QAService class.</p> <p>The question and answer service is used to query a language model with questions related to a specific context. The context is usually a set of documents that are loaded into a vector store.</p> <p>Classes:</p> Name Description <code>QAService</code> <p>The question and answer service class.</p>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService","title":"QAService","text":"<pre><code>QAService(config: QAConfig)\n</code></pre> <p>The question and answer service class.</p> <p>The service is initialized with a configuration.</p> <p>Depending on the configuration, the service will create a new in-memory vector store or load an existing one from a file.</p> <p>Parameters:</p> Name Type Description Default <code>QAConfig</code> <p>The configuration for the QA service</p> required <p>Methods:</p> Name Description <code>add_file</code> <p>Add a document to the vectorstore.</p> <code>analyze_mult_prompts_query</code> <p>Reads a set of data from doc.</p> <code>analyze_query</code> <p>Read a set of data from a set of documents.</p> <code>db_embedding</code> <p>Converts a question to an embedding.</p> <code>db_query</code> <p>Queries the vector store with a question.</p> <code>get_prompt_config</code> <p>Returns the prompt config for the given mode.</p> <code>query</code> <p>Pass a question to the language model.</p> <code>remove_file</code> <p>Removes a document from the vectorstore.</p> <code>set_prompt_config</code> <p>Sets the prompt config for the given mode.</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def __init__(self, config: QAConfig) -&gt; None:\n    \"\"\"The service is initialized with a configuration.\n\n    Depending on the configuration, the service will create a new in-memory\n    vector store or load an existing one from a file.\n\n    Parameters:\n        config: The configuration for the QA service\n    \"\"\"\n    self.config = config\n    self._llm = gerd_loader.load_model_from_config(config.model)\n    self._vectorstore: Optional[FAISS] = None\n    self._database: Optional[Rag] = None\n    if (\n        config.embedding.db_path\n        and Path(config.embedding.db_path, \"index.faiss\").exists()\n    ):\n        _LOGGER.info(\n            \"Load existing vector store from '%s'.\", config.embedding.db_path\n        )\n        self._vectorstore = load_faiss(\n            Path(config.embedding.db_path, \"index.faiss\"),\n            config.embedding.model.name,\n            config.device,\n        )\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.add_file","title":"add_file","text":"<pre><code>add_file(file: QAFileUpload) -&gt; QAAnswer\n</code></pre> <p>Add a document to the vectorstore.</p> <p>Parameters:</p> Name Type Description Default <code>QAFileUpload</code> <p>The file to add to the vectorstore</p> required <p>Returns:</p> Type Description <code>QAAnswer</code> <p>an answer object with status 200 if successful</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def add_file(self, file: QAFileUpload) -&gt; QAAnswer:\n    \"\"\"Add a document to the vectorstore.\n\n    Parameters:\n        file: The file to add to the vectorstore\n\n    Returns:\n        an answer object with status 200 if successful\n    \"\"\"\n    documents: Optional[List[Document]] = None\n    file_path = Path(file.name)\n    try:\n        with NamedTemporaryFile(\n            dir=\".\", suffix=file_path.suffix, delete=False\n        ) as f:\n            f.write(file.data)\n            f.flush()\n        file_type = FileTypes(file_path.suffix[1:])\n        loader: BaseLoader\n        if file_type == FileTypes.TEXT:\n            loader = TextLoader(f.name)\n        elif file_type == FileTypes.PDF:\n            loader = PyPDFLoader(f.name)\n        documents = loader.load()\n        # source must be overriden to not leak upload information\n        # about the temp file which are rather useless anyway\n        for doc in documents:\n            doc.metadata[\"source\"] = file_path.name\n    except BaseException as err:\n        _LOGGER.error(err)\n    finally:\n        if f:\n            unlink(f.name)\n    if not documents:\n        _LOGGER.warning(\"No document was loaded!\")\n        return QAAnswer(error_msg=\"No document was loaded!\", status=500)\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=self.config.embedding.chunk_size,\n        chunk_overlap=self.config.embedding.chunk_overlap,\n    )\n    texts = text_splitter.split_documents(documents)\n    for i, text in enumerate(texts):\n        text.id = f\"{file_path.name}_{i}\"\n    if self._vectorstore is None:\n        _LOGGER.info(\"Create new vector store from document.\")\n        self._vectorstore = create_faiss(\n            texts, self.config.embedding.model.name, self.config.device\n        )\n    else:\n        _LOGGER.info(\"Adding document to existing vector store.\")\n        tmp = create_faiss(\n            texts, self.config.embedding.model.name, self.config.device\n        )\n        self._vectorstore.merge_from(tmp)\n    if self.config.embedding.db_path:\n        self._vectorstore.save_local(self.config.embedding.db_path)\n    return QAAnswer()\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.add_file(file)","title":"<code>file</code>","text":""},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.analyze_mult_prompts_query","title":"analyze_mult_prompts_query","text":"<pre><code>analyze_mult_prompts_query() -&gt; QAAnalyzeAnswer\n</code></pre> <p>Reads a set of data from doc.</p> <p>Loads the data via multiple prompts by asking for each data field separately.</p> <p>Data     - patient_name     - patient_date_of_birth     - attending_doctors     - recording_date     - release_date</p> <p>Returns:</p> Type Description <code>QAAnalyzeAnswer</code> <p>The answer from the language model</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def analyze_mult_prompts_query(self) -&gt; QAAnalyzeAnswer:\n    \"\"\"Reads a set of data from doc.\n\n    Loads the data via multiple prompts by asking for each data field\n    separately.\n\n    *Data*\n        - patient_name\n        - patient_date_of_birth\n        - attending_doctors\n        - recording_date\n        - release_date\n\n    Returns:\n        The answer from the language model\n    \"\"\"\n    if not self._vectorstore:\n        msg = \"No vector store initialized! Upload documents first.\"\n        _LOGGER.error(msg)\n        return QAAnalyzeAnswer(status=404, error_msg=msg)\n\n    config = self.config.features.analyze_mult_prompts\n\n    # check if prompt contains needed fields\n    qa_analyze_mult_prompts = config.model.prompt_config\n    if (\n        \"context\" not in qa_analyze_mult_prompts.parameters\n        or \"question\" not in qa_analyze_mult_prompts.parameters\n    ):\n        msg = \"Prompt does not include '{context}' or '{question}' variable.\"\n        _LOGGER.error(msg)\n        return QAAnalyzeAnswer(status=404, error_msg=msg)\n\n    # questions to search model and vectorstore\n    questions_model_dict: Dict[str, str] = {\n        \"Wie hei\u00dft der Patient?\": \"wir berichten \u00fcber unseren Patient oder Btr. oder Patient, wh, geboren oder  Patient, * 0.00.0000,\",  # noqa: E501\n        \"Wann hat der Patient Geburstag?\": \"wir berichten \u00fcber unseren Patient oder Btr. oder Patient, wh, geboren oder  Patient, * 0.00.0000,\",  # noqa: E501\n        \"Wie hei\u00dft der Arzt?\": \"Mit freundlichen kollegialen Gr\u00fc\u00dfen, Prof, Dr\",\n        \"Wann wurde der Patient bei uns aufgenommen?\": (\n            \"wir berichten \u00fcber unseren Patient oder Btr. oder Patient, wh, geboren\"\n        ),\n        \"Wann wurde der Patient bei uns entlassen?\": (\n            \"wir berichten \u00fcber unseren Patient oder Btr. oder Patient, wh, geboren\"\n        ),\n    }\n    fields: Dict[str, str] = {\n        \"Wie hei\u00dft der Patient?\": \"patient_name\",\n        \"Wann hat der Patient Geburstag?\": \"patient_date_of_birth\",\n        \"Wie hei\u00dft der Arzt?\": \"attending_doctors\",\n        \"Wann wurde der Patient bei uns aufgenommen?\": \"recording_date\",\n        \"Wann wurde der Patient bei uns entlassen?\": \"release_date\",\n    }\n    questions_dict: Dict[str, List[DocumentSource]] = {}\n    answer_dict: Dict[str, Any] = {}\n    responses: str = \"\"\n\n    # load context from vectorstore for each question\n    for question_m, question_v in questions_model_dict.items():\n        questions_dict, formatted_prompt = self._create_analyze_mult_prompt(\n            question_m, question_v, qa_analyze_mult_prompts.text\n        )\n\n        # query the model for each question\n        response = self._llm.generate(formatted_prompt)\n\n        # format the response\n        if response is not None:\n            response = self._clean_response(response)\n\n            # if enabled, collect response\n            if self.config.features.return_source:\n                responses = responses + \"; \" + question_m + \": \" + response\n        # format the response\n        answer_dict[fields[question_m]] = self._format_response_analyze_mult_prompt(\n            response, fields[question_m]\n        )\n\n        _LOGGER.info(\n            \"\\n===== Modelresult ====\\n\\n%s\\n\\n====================\", response\n        )\n\n    answer = QAAnalyzeAnswer(**answer_dict)\n\n    # if enabled, pass source data to answer\n    if self.config.features.return_source:\n        answer.response = responses\n        for question in questions_dict:\n            answer.sources = answer.sources + questions_dict[question]\n\n    _LOGGER.warning(\"\\n==== Answer ====\\n\\n%s\\n===============\", answer)\n    return answer\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.analyze_query","title":"analyze_query","text":"<pre><code>analyze_query() -&gt; QAAnalyzeAnswer\n</code></pre> <p>Read a set of data from a set of documents.</p> <p>Loads the data via single prompt.</p> <p>Data     - patient_name     - patient_date_of_birth     - attending_doctors     - recording_date     - release_date</p> <p>Returns:</p> Type Description <code>QAAnalyzeAnswer</code> <p>The answer from the language model</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def analyze_query(self) -&gt; QAAnalyzeAnswer:\n    \"\"\"Read a set of data from a set of documents.\n\n    Loads the data via single prompt.\n\n    *Data*\n        - patient_name\n        - patient_date_of_birth\n        - attending_doctors\n        - recording_date\n        - release_date\n\n    Returns:\n        The answer from the language model\n    \"\"\"\n    if not self._vectorstore:\n        msg = \"No vector store initialized! Upload documents first.\"\n        _LOGGER.error(msg)\n        return QAAnalyzeAnswer(status=404, error_msg=msg)\n\n    config = self.config.features.analyze\n\n    # questions to search model and vectorstore\n    questions_model_dict: Dict[str, str] = {\n        \"Wie hei\u00dft der Patient?\": \"wir berichten \u00fcber unseren Patient oder Btr. oder Patient, wh, geboren oder  Patient, * 0.00.0000,\",  # noqa E501\n        \"Wann hat der Patient Geburstag?\": \"wir berichten \u00fcber unseren Patient oder Btr. oder Patient, wh, geboren oder  Patient, * 0.00.0000,\",  # noqa E501\n        \"Wie hei\u00dft der Arzt?\": \"Mit freundlichen kollegialen Gr\u00fc\u00dfen, Prof, Dr\",\n        \"Wann wurde der Patient bei uns aufgenommen?\": (\n            \"wir berichten \u00fcber unseren Patient oder Btr. oder Patient, wh, geboren\"\n        ),\n        \"Wann wurde der Patient bei uns entlassen?\": (\n            \"wir berichten \u00fcber unseren Patient oder Btr. oder Patient, wh, geboren\"\n        ),\n    }\n\n    # map model questions to jsonfields\n    fields: Dict[str, str] = {\n        \"Wie hei\u00dft der Patient?\": \"patient_name\",\n        \"Wann hat der Patient Geburstag?\": \"patient_date_of_birth\",\n        \"Wie hei\u00dft der Arzt?\": \"attending_doctors\",\n        \"Wann wurde der Patient bei uns aufgenommen?\": \"recording_date\",\n        \"Wann wurde der Patient bei uns entlassen?\": \"release_date\",\n    }\n\n    questions_dict: Dict[str, List[DocumentSource]] = {}\n    parameters: Dict[str, str] = {}\n    question_counter: int = 0\n\n    qa_analyze_prompt = config.model.prompt_config\n    # check if prompt contains needed fields\n    for i in range(0, len(questions_model_dict)):\n        if (\n            \"context\" + str(i) not in qa_analyze_prompt.parameters\n            or \"question\" + str(i) not in qa_analyze_prompt.parameters\n        ):\n            msg = (\n                \"Prompt does not include '{context\"\n                + str(i)\n                + \"}' or '{question\"\n                + str(i)\n                + \"}' variable.\"\n            )\n            _LOGGER.error(msg)\n            return QAAnalyzeAnswer(status=404, error_msg=msg)\n\n    # load context from vectorstore for each question\n    for question_m, question_v in questions_model_dict.items():\n        questions_dict[question_m] = list(\n            self.db_query(QAQuestion(question=question_v, max_sources=3))\n        )\n\n        parameters[\"context\" + str(question_counter)] = \" \".join(\n            doc.content for doc in questions_dict[question_m]\n        )\n        parameters[\"question\" + str(question_counter)] = question_m\n        parameters[\"field\" + str(question_counter)] = fields[question_m]\n\n        question_counter = question_counter + 1\n\n    formatted_prompt = qa_analyze_prompt.text.format(**parameters)\n\n    # query the model\n    response = self._llm.generate(formatted_prompt)\n\n    if response is not None:\n        response = self._clean_response(response)\n\n    _LOGGER.info(\"\\n===== Modelresult ====\\n\\n%s\\n\\n====================\", response)\n\n    # convert json to QAAnalyzerAnswerclass\n    answer = self._format_response_analyze(response)\n\n    # if enabled, pass source data to answer\n    if self.config.features.return_source:\n        answer.response = response\n        answer.prompt = formatted_prompt\n        for question in questions_dict:\n            answer.sources = answer.sources + questions_dict[question]\n        _LOGGER.info(\n            \"\\n===== Sources ====\\n\\n%s\\n\\n====================\", answer.sources\n        )\n\n    _LOGGER.warning(\"\\n==== Answer ====\\n\\n%s\\n===============\", answer)\n    return answer\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.db_embedding","title":"db_embedding","text":"<pre><code>db_embedding(question: QAQuestion) -&gt; List[float]\n</code></pre> <p>Converts a question to an embedding.</p> <p>The embedding to be used is defined by the vector store or more specifically by the configured parameters passed to initialize the vector store.</p> <p>Parameters:</p> Name Type Description Default <code>QAQuestion</code> <p>The question to convert to an embedding.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>The embedding of the question</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def db_embedding(self, question: QAQuestion) -&gt; List[float]:\n    \"\"\"Converts a question to an embedding.\n\n    The embedding to be used is defined by the vector store or more specifically\n    by the configured parameters passed to initialize the vector store.\n\n    Parameters:\n        question: The question to convert to an embedding.\n\n    Returns:\n        The embedding of the question\n    \"\"\"\n    if self._vectorstore is None or self._vectorstore.embeddings is None:\n        return []\n    return self._vectorstore.embeddings.embed_documents([question.question])[0]\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.db_embedding(question)","title":"<code>question</code>","text":""},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.db_query","title":"db_query","text":"<pre><code>db_query(question: QAQuestion) -&gt; List[DocumentSource]\n</code></pre> <p>Queries the vector store with a question.</p> <p>The number of sources that are returned is defined by the max_sources parameter of the service's configuration.</p> <p>Parameters:</p> Name Type Description Default <code>QAQuestion</code> <p>The question to query the vector store with.</p> required <p>Returns:</p> Type Description <code>List[DocumentSource]</code> <p>A list of document sources</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def db_query(self, question: QAQuestion) -&gt; List[DocumentSource]:\n    \"\"\"Queries the vector store with a question.\n\n    The number of sources that are returned is defined by the max_sources parameter\n    of the service's configuration.\n\n    Parameters:\n        question: The question to query the vector store with.\n\n    Returns:\n        A list of document sources\n    \"\"\"\n    if not self._vectorstore:\n        return []\n    return [\n        DocumentSource(\n            query=question.question,\n            content=doc.page_content,\n            name=doc.metadata.get(\"source\", \"unknown\"),\n            page=doc.metadata.get(\"page\", 1),\n        )\n        for doc in self._vectorstore.search(\n            question.question,\n            search_type=question.search_strategy,\n            k=question.max_sources,\n        )\n    ]\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.db_query(question)","title":"<code>question</code>","text":""},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.get_prompt_config","title":"get_prompt_config","text":"<pre><code>get_prompt_config(qa_mode: QAModesEnum) -&gt; PromptConfig\n</code></pre> <p>Returns the prompt config for the given mode.</p> <p>Parameters:</p> Name Type Description Default <code>QAModesEnum</code> <p>The mode to get the prompt config for</p> required <p>Returns:</p> Type Description <code>PromptConfig</code> <p>The prompt config for the given mode</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def get_prompt_config(self, qa_mode: QAModesEnum) -&gt; PromptConfig:\n    \"\"\"Returns the prompt config for the given mode.\n\n    Parameters:\n        qa_mode: The mode to get the prompt config for\n\n    Returns:\n        The prompt config for the given mode\n    \"\"\"\n    if qa_mode == QAModesEnum.SEARCH:\n        return self.config.model.prompt_config\n    elif qa_mode == QAModesEnum.ANALYZE:\n        return self.config.features.analyze.model.prompt_config\n    elif qa_mode == QAModesEnum.ANALYZE_MULT_PROMPTS:\n        return self.config.features.analyze_mult_prompts.model.prompt_config\n    return PromptConfig()\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.get_prompt_config(qa_mode)","title":"<code>qa_mode</code>","text":""},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.query","title":"query","text":"<pre><code>query(question: QAQuestion) -&gt; QAAnswer\n</code></pre> <p>Pass a question to the language model.</p> <p>The language model will generate an answer based on the question and the context derived from the vector store.</p> <p>Parameters:</p> Name Type Description Default <code>QAQuestion</code> <p>The question to be answered</p> required <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer from the language model</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def query(self, question: QAQuestion) -&gt; QAAnswer:\n    \"\"\"Pass a question to the language model.\n\n    The language model will generate an answer based on the question and\n    the context derived from the vector store.\n\n    Parameters:\n        question: The question to be answered\n\n    Returns:\n        The answer from the language model\n    \"\"\"\n    if not self._database:\n        if not self._vectorstore:\n            return QAAnswer(error_msg=\"No database available!\", status=404)\n        self._database = Rag(\n            self._llm,\n            self.config.model,\n            self.config.model.prompt_config,\n            self._vectorstore,\n            self.config.features.return_source,\n        )\n    return self._database.query(question)\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.query(question)","title":"<code>question</code>","text":""},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.remove_file","title":"remove_file","text":"<pre><code>remove_file(file_name: str) -&gt; QAAnswer\n</code></pre> <p>Removes a document from the vectorstore.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the file to remove</p> required <p>Returns:</p> Type Description <code>QAAnswer</code> <p>an answer object with status 200 if successful</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def remove_file(self, file_name: str) -&gt; QAAnswer:\n    \"\"\"Removes a document from the vectorstore.\n\n    Parameters:\n        file_name: The name of the file to remove\n\n    Returns:\n        an answer object with status 200 if successful\n    \"\"\"\n    if not self._vectorstore:\n        return QAAnswer(error_msg=\"No vector store initialized!\", status=404)\n    self._vectorstore.delete(\n        [\n            id\n            for id in self._vectorstore.index_to_docstore_id.values()\n            if id.startswith(file_name)\n        ]\n    )\n    return QAAnswer(status=200)\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.remove_file(file_name)","title":"<code>file_name</code>","text":""},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.set_prompt_config","title":"set_prompt_config","text":"<pre><code>set_prompt_config(config: PromptConfig, qa_mode: QAModesEnum) -&gt; QAAnswer\n</code></pre> <p>Sets the prompt config for the given mode.</p> <p>Parameters:</p> Name Type Description Default <code>PromptConfig</code> <p>The prompt config to set</p> required <code>QAModesEnum</code> <p>The mode to set the prompt config for</p> required <p>Returns:</p> Type Description <code>QAAnswer</code> <p>an answer object with status 200 if successful</p> Source code in <code>gerd/qa/qa_service.py</code> <pre><code>def set_prompt_config(self, config: PromptConfig, qa_mode: QAModesEnum) -&gt; QAAnswer:\n    \"\"\"Sets the prompt config for the given mode.\n\n    Parameters:\n        config: The prompt config to set\n        qa_mode: The mode to set the prompt config for\n\n    Returns:\n        an answer object with status 200 if successful\n    \"\"\"\n    answer = QAAnswer()\n    if qa_mode == QAModesEnum.SEARCH:\n        self.config.model.prompt_config = config\n        if \"context\" not in config.parameters:\n            answer.error_msg = (\n                \"Prompt does not include '{context}' variable. \"\n                \"No context will be added. \"\n            )\n        if \"question\" not in config.parameters:\n            answer.error_msg += (\n                \"Prompt does not include '{question}' variable. \"\n                \"Questions will not be passed to the model.\"\n            )\n    elif qa_mode == QAModesEnum.ANALYZE:\n        self.config.features.analyze.model.prompt_config = config\n    elif qa_mode == QAModesEnum.ANALYZE_MULT_PROMPTS:\n        self.config.features.analyze_mult_prompts.model.prompt_config = config\n    return answer\n</code></pre>"},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.set_prompt_config(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/qa/qa_service/#gerd.qa.qa_service.QAService.set_prompt_config(qa_mode)","title":"<code>qa_mode</code>","text":""},{"location":"reference/gerd/rag/","title":"gerd.rag","text":""},{"location":"reference/gerd/rag/#gerd.rag","title":"gerd.rag","text":"<p>Retrieval-Augmented Generation (RAG) backend.</p> <p>This module provides the RAG backend for the GERD system which is currently based on FAISS.</p> <p>Classes:</p> Name Description <code>Rag</code> <p>The RAG backend for GERD.</p> <p>Functions:</p> Name Description <code>create_faiss</code> <p>Create a new FAISS store from a list of documents.</p> <code>load_faiss</code> <p>Load a FAISS store from a disk path.</p>"},{"location":"reference/gerd/rag/#gerd.rag.Rag","title":"Rag","text":"<pre><code>Rag(model: LLM, model_config: ModelConfig, prompt: PromptConfig, store: FAISS, return_source: bool)\n</code></pre> <p>The RAG backend for GERD.</p> <p>The RAG backend will check for a context parameter in the prompt.</p> <p>If the context parameter is not included, a warning will be logged. Without the context parameter, no context will be added to the query.</p> <p>Parameters:</p> Name Type Description Default <code>LLM</code> <p>The LLM model to use</p> required <code>ModelConfig</code> <p>The model configuration</p> required <code>PromptConfig</code> <p>The prompt configuration</p> required <code>FAISS</code> <p>The FAISS store to use</p> required <code>bool</code> <p>Whether to return the source documents</p> required <p>Methods:</p> Name Description <code>query</code> <p>Query the RAG backend with a question.</p> Source code in <code>gerd/rag.py</code> <pre><code>def __init__(\n    self,\n    model: LLM,\n    model_config: ModelConfig,\n    prompt: PromptConfig,\n    store: FAISS,\n    return_source: bool,\n) -&gt; None:\n    \"\"\"The RAG backend will check for a context parameter in the prompt.\n\n    If the context parameter is not included, a warning will be logged.\n    Without the context parameter, no context will be added to the query.\n\n    Parameters:\n        model: The LLM model to use\n        model_config: The model configuration\n        prompt: The prompt configuration\n        store: The FAISS store to use\n        return_source: Whether to return the source documents\n    \"\"\"\n    self.model = model\n    self.model_config = model_config\n    self.prompt = prompt\n    self.store = store\n    self.return_source = return_source\n\n    if \"context\" not in prompt.parameters:\n        _LOGGER.warning(\n            \"Prompt does not include '{context}' variable! \"\n            \"No context will be added to the query.\"\n        )\n</code></pre>"},{"location":"reference/gerd/rag/#gerd.rag.Rag(model)","title":"<code>model</code>","text":""},{"location":"reference/gerd/rag/#gerd.rag.Rag(model_config)","title":"<code>model_config</code>","text":""},{"location":"reference/gerd/rag/#gerd.rag.Rag(prompt)","title":"<code>prompt</code>","text":""},{"location":"reference/gerd/rag/#gerd.rag.Rag(store)","title":"<code>store</code>","text":""},{"location":"reference/gerd/rag/#gerd.rag.Rag(return_source)","title":"<code>return_source</code>","text":""},{"location":"reference/gerd/rag/#gerd.rag.Rag.query","title":"query","text":"<pre><code>query(question: QAQuestion) -&gt; QAAnswer\n</code></pre> <p>Query the RAG backend with a question.</p> <p>Parameters:</p> Name Type Description Default <code>QAQuestion</code> <p>The question to ask</p> required <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer to the question including the sources</p> Source code in <code>gerd/rag.py</code> <pre><code>def query(self, question: QAQuestion) -&gt; QAAnswer:\n    \"\"\"Query the RAG backend with a question.\n\n    Parameters:\n        question: The question to ask\n\n    Returns:\n        The answer to the question including the sources\n    \"\"\"\n    docs = self.store.search(\n        question.question,\n        search_type=question.search_strategy,\n        k=question.max_sources,\n    )\n    context = \"\\n\".join(doc.page_content for doc in docs)\n    resolved = self.prompt.text.format(context=context, question=question.question)\n    _, response = self.model.create_chat_completion(\n        [{\"role\": \"user\", \"content\": resolved}]\n    )\n    answer = QAAnswer(response=response)\n    if self.return_source:\n        for doc in docs:\n            answer.sources.append(\n                DocumentSource(\n                    query=question.question,\n                    content=doc.page_content,\n                    name=doc.metadata.get(\"source\", \"unknown\"),\n                    page=doc.metadata.get(\"page\", 1),\n                )\n            )\n    return answer\n</code></pre>"},{"location":"reference/gerd/rag/#gerd.rag.Rag.query(question)","title":"<code>question</code>","text":""},{"location":"reference/gerd/rag/#gerd.rag.create_faiss","title":"create_faiss","text":"<pre><code>create_faiss(documents: list[Document], model_name: str, device: str) -&gt; FAISS\n</code></pre> <p>Create a new FAISS store from a list of documents.</p> <p>Parameters:</p> Name Type Description Default <code>list[Document]</code> <p>The list of documents to index</p> required <code>str</code> <p>The name of the Hugging Face model to for the embeddings</p> required <code>str</code> <p>The device to use for the model</p> required <p>Returns:</p> Type Description <code>FAISS</code> <p>The newly created FAISS store</p> Source code in <code>gerd/rag.py</code> <pre><code>def create_faiss(documents: list[Document], model_name: str, device: str) -&gt; FAISS:\n    \"\"\"Create a new FAISS store from a list of documents.\n\n    Parameters:\n        documents: The list of documents to index\n        model_name: The name of the Hugging Face model to for the embeddings\n        device: The device to use for the model\n\n    Returns:\n        The newly created FAISS store\n    \"\"\"\n    return FAISS.from_documents(\n        documents,\n        HuggingFaceEmbeddings(\n            model_name=model_name,\n            model_kwargs={\"device\": device},\n        ),\n    )\n</code></pre>"},{"location":"reference/gerd/rag/#gerd.rag.create_faiss(documents)","title":"<code>documents</code>","text":""},{"location":"reference/gerd/rag/#gerd.rag.create_faiss(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/gerd/rag/#gerd.rag.create_faiss(device)","title":"<code>device</code>","text":""},{"location":"reference/gerd/rag/#gerd.rag.load_faiss","title":"load_faiss","text":"<pre><code>load_faiss(dp_path: Path, model_name: str, device: str) -&gt; FAISS\n</code></pre> <p>Load a FAISS store from a disk path.</p> <p>Parameters:</p> Name Type Description Default <code>Path</code> <p>The path to the disk path</p> required <code>str</code> <p>The name of the Hugging Face model to for the embeddings</p> required <code>str</code> <p>The device to use for the model</p> required <p>Returns:</p> Type Description <code>FAISS</code> <p>The loaded FAISS store</p> Source code in <code>gerd/rag.py</code> <pre><code>def load_faiss(dp_path: Path, model_name: str, device: str) -&gt; FAISS:\n    \"\"\"Load a FAISS store from a disk path.\n\n    Parameters:\n        dp_path: The path to the disk path\n        model_name: The name of the Hugging Face model to for the embeddings\n        device: The device to use for the model\n\n    Returns:\n        The loaded FAISS store\n    \"\"\"\n    return FAISS.load_local(\n        dp_path.as_posix(),\n        HuggingFaceEmbeddings(\n            model_name=model_name,\n            model_kwargs={\"device\": device},\n        ),\n    )\n</code></pre>"},{"location":"reference/gerd/rag/#gerd.rag.load_faiss(dp_path)","title":"<code>dp_path</code>","text":""},{"location":"reference/gerd/rag/#gerd.rag.load_faiss(model_name)","title":"<code>model_name</code>","text":""},{"location":"reference/gerd/rag/#gerd.rag.load_faiss(device)","title":"<code>device</code>","text":""},{"location":"reference/gerd/training/","title":"gerd.training","text":""},{"location":"reference/gerd/training/#gerd.training","title":"gerd.training","text":"<p>Collections of training routines for GERD.</p> <p>Modules:</p> Name Description <code>data</code> <p>Data utilities for training and data processing.</p> <code>instruct</code> <p>Training module for instruction text sets.</p> <code>lora</code> <p>Configuration dataclasses for training LoRA models.</p> <code>trainer</code> <p>Training module for LoRA models.</p> <code>unstructured</code> <p>Training of LoRA models on unstructured text data.</p>"},{"location":"reference/gerd/training/data/","title":"gerd.training.data","text":""},{"location":"reference/gerd/training/data/#gerd.training.data","title":"gerd.training.data","text":"<p>Data utilities for training and data processing.</p> <p>Functions:</p> Name Description <code>despacyfy</code> <p>Removes spacy-specific tokens from a text.</p> <code>encode</code> <p>Encodes a text using a tokenizer.</p> <code>split_chunks</code> <p>Splits a list of encoded tokens into chunks of a given size.</p> <code>tokenize</code> <p>Converts a prompt into a tokenized input for a model.</p>"},{"location":"reference/gerd/training/data/#gerd.training.data.despacyfy","title":"despacyfy","text":"<pre><code>despacyfy(text: str) -&gt; str\n</code></pre> <p>Removes spacy-specific tokens from a text.</p> <p>For instance, -RRB- is replaced with ')', -LRB- with '(' and -UNK- with '*'.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The text to despacyfy.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The despacyfied text</p> Source code in <code>gerd/training/data.py</code> <pre><code>def despacyfy(text: str) -&gt; str:\n    \"\"\"Removes spacy-specific tokens from a text.\n\n    For instance, -RRB- is replaced with ')', -LRB- with '(' and -UNK- with '*'.\n\n    Parameters:\n        text: The text to despacyfy.\n\n    Returns:\n        The despacyfied text\n    \"\"\"\n    res = (\n        text.replace(\"-RRB-\", \")\")\n        .replace(\"-LRB-\", \"(\")\n        .replace(\"-UNK-\", \"*\")\n        .replace(\"( \", \"(\")\n        .replace(\" )\", \")\")\n        .replace(\"  \", \" \")\n    )\n    check = re.findall(r\"-(RRB|UNK|LRB)-\", res)\n    if len(check) != 0:\n        msg = f\"Did not expect to find {check} in\\n{res}.\"\n        raise RuntimeError(msg)\n    return res\n</code></pre>"},{"location":"reference/gerd/training/data/#gerd.training.data.despacyfy(text)","title":"<code>text</code>","text":""},{"location":"reference/gerd/training/data/#gerd.training.data.encode","title":"encode","text":"<pre><code>encode(text: str, add_bos_token: bool, tokenizer: PreTrainedTokenizer, cutoff_len: int) -&gt; List[int]\n</code></pre> <p>Encodes a text using a tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The text to encode</p> required <code>bool</code> <p>Whether to add the beginning of sentence token</p> required <code>PreTrainedTokenizer</code> <p>The tokenizer to use</p> required <code>int</code> <p>The maximum length of the encoded text</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>The text encoded as a list of tokenizer tokens</p> Source code in <code>gerd/training/data.py</code> <pre><code>def encode(\n    text: str, add_bos_token: bool, tokenizer: PreTrainedTokenizer, cutoff_len: int\n) -&gt; List[int]:\n    \"\"\"Encodes a text using a tokenizer.\n\n    Parameters:\n        text: The text to encode\n        add_bos_token: Whether to add the beginning of sentence token\n        tokenizer: The tokenizer to use\n        cutoff_len: The maximum length of the encoded text\n\n    Returns:\n        The text encoded as a list of tokenizer tokens\n    \"\"\"\n    result: list[int] = tokenizer.encode(text, truncation=True, max_length=cutoff_len)\n    # Check if the first two tokens are BOS\n    if len(result) &gt;= 2 and result[:2] == [\n        tokenizer.bos_token_id,\n        tokenizer.bos_token_id,\n    ]:\n        result = result[1:]\n\n    if not add_bos_token and result[0] == tokenizer.bos_token_id:\n        result = result[1:]\n    return result\n</code></pre>"},{"location":"reference/gerd/training/data/#gerd.training.data.encode(text)","title":"<code>text</code>","text":""},{"location":"reference/gerd/training/data/#gerd.training.data.encode(add_bos_token)","title":"<code>add_bos_token</code>","text":""},{"location":"reference/gerd/training/data/#gerd.training.data.encode(tokenizer)","title":"<code>tokenizer</code>","text":""},{"location":"reference/gerd/training/data/#gerd.training.data.encode(cutoff_len)","title":"<code>cutoff_len</code>","text":""},{"location":"reference/gerd/training/data/#gerd.training.data.split_chunks","title":"split_chunks","text":"<pre><code>split_chunks(arr: List[int], size: int, step: int) -&gt; Generator[List[int], None, None]\n</code></pre> <p>Splits a list of encoded tokens into chunks of a given size.</p> <p>Parameters:</p> Name Type Description Default <code>List[int]</code> <p>The list of encoded tokens.</p> required <code>int</code> <p>The size of the chunks.</p> required <code>int</code> <p>The step size for the chunks.</p> required <p>Returns:</p> Type Description <code>None</code> <p>A generator that yields the chunks</p> Source code in <code>gerd/training/data.py</code> <pre><code>def split_chunks(\n    arr: List[int], size: int, step: int\n) -&gt; Generator[List[int], None, None]:\n    \"\"\"Splits a list of encoded tokens into chunks of a given size.\n\n    Parameters:\n        arr: The list of encoded tokens.\n        size: The size of the chunks.\n        step: The step size for the chunks.\n\n    Returns:\n        A generator that yields the chunks\n    \"\"\"\n    for i in range(0, len(arr), step):\n        yield arr[i : i + size]\n</code></pre>"},{"location":"reference/gerd/training/data/#gerd.training.data.split_chunks(arr)","title":"<code>arr</code>","text":""},{"location":"reference/gerd/training/data/#gerd.training.data.split_chunks(size)","title":"<code>size</code>","text":""},{"location":"reference/gerd/training/data/#gerd.training.data.split_chunks(step)","title":"<code>step</code>","text":""},{"location":"reference/gerd/training/data/#gerd.training.data.tokenize","title":"tokenize","text":"<pre><code>tokenize(prompt: str, tokenizer: PreTrainedTokenizer, cutoff_len: int, append_eos_token: bool = False) -&gt; Dict[str, torch.Tensor | list[int]]\n</code></pre> <p>Converts a prompt into a tokenized input for a model.</p> <p>The methods returns the tokenized input as a dictionary with the keys \"input_ids\", \"labels\" and \"attention_mask\" where the input_ids are the tokenized input, the labels assign the same label ('1') to each token and the attention_mask masks out the padding tokens. Parameters:     prompt: The prompt to tokenize     tokenizer: The tokenizer to use     cutoff_len: The maximum length of the encoded text     append_eos_token: Whether to append an end of sentence token</p> <p>Returns:</p> Type Description <code>Dict[str, Tensor | list[int]]</code> <p>The tokenized input as a dictionary</p> Source code in <code>gerd/training/data.py</code> <pre><code>def tokenize(\n    prompt: str,\n    tokenizer: PreTrainedTokenizer,\n    cutoff_len: int,\n    append_eos_token: bool = False,\n) -&gt; Dict[str, torch.Tensor | list[int]]:\n    \"\"\"Converts a prompt into a tokenized input for a model.\n\n    The methods returns the tokenized input as a dictionary with the keys\n    \"input_ids\", \"labels\" and \"attention_mask\" where the input_ids are the\n    tokenized input, the labels assign the same label ('1') to each token\n    and the attention_mask masks out the padding tokens.\n    Parameters:\n        prompt: The prompt to tokenize\n        tokenizer: The tokenizer to use\n        cutoff_len: The maximum length of the encoded text\n        append_eos_token: Whether to append an end of sentence token\n\n    Returns:\n        The tokenized input as a dictionary\n    \"\"\"\n    input_ids = encode(prompt, True, tokenizer, cutoff_len)\n\n    if tokenizer.pad_token_id is None or tokenizer.padding_side is None:\n        msg = (\n            \"Tokenizing implies tokenizer.pad_token_id \"\n            \"and tokenizer.padding_side to be set!\"\n        )\n        raise AttributeError(msg)\n\n    if (\n        append_eos_token\n        and input_ids[-1] != tokenizer.eos_token_id\n        and len(input_ids) &lt; cutoff_len\n    ):\n        input_ids.append(tokenizer.eos_token_id)\n\n    input_ids = [tokenizer.pad_token_id] * (cutoff_len - len(input_ids)) + input_ids\n    labels = [1] * len(input_ids)\n\n    input_tensors = torch.tensor(input_ids)\n    return {\n        \"input_ids\": input_tensors,\n        \"labels\": labels,\n        \"attention_mask\": input_tensors.ne(tokenizer.pad_token_id),\n    }\n</code></pre>"},{"location":"reference/gerd/training/instruct/","title":"gerd.training.instruct","text":""},{"location":"reference/gerd/training/instruct/#gerd.training.instruct","title":"gerd.training.instruct","text":"<p>Training module for instruction text sets.</p> <p>In contrast to the <code>unstructured</code> training module, data must be prepared in a specific format to train LoRA models. Make sure to provide the training data in the correct format.</p> <p>Classes:</p> Name Description <code>InstructTrainingData</code> <p>Dataclass to hold training data for instruction text sets.</p> <code>InstructTrainingSample</code> <p>Dataclass to hold a training sample for instruction text sets.</p> <p>Functions:</p> Name Description <code>train_lora</code> <p>Train a LoRA model on instruction text sets.</p>"},{"location":"reference/gerd/training/instruct/#gerd.training.instruct.InstructTrainingData","title":"InstructTrainingData","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dataclass to hold training data for instruction text sets.</p> <p>A training data object consists of a list of training samples.</p> <p>Attributes:</p> Name Type Description <code>samples</code> <code>list[InstructTrainingSample]</code> <p>The list of training samples.</p>"},{"location":"reference/gerd/training/instruct/#gerd.training.instruct.InstructTrainingData.samples","title":"samples  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>samples: list[InstructTrainingSample] = []\n</code></pre> <p>The list of training samples.</p>"},{"location":"reference/gerd/training/instruct/#gerd.training.instruct.InstructTrainingSample","title":"InstructTrainingSample","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dataclass to hold a training sample for instruction text sets.</p> <p>A training sample consists of a list of chat messages.</p> <p>Attributes:</p> Name Type Description <code>messages</code> <code>list[ChatMessage]</code> <p>The list of chat messages.</p>"},{"location":"reference/gerd/training/instruct/#gerd.training.instruct.InstructTrainingSample.messages","title":"messages  <code>instance-attribute</code>","text":"<pre><code>messages: list[ChatMessage]\n</code></pre> <p>The list of chat messages.</p>"},{"location":"reference/gerd/training/instruct/#gerd.training.instruct.train_lora","title":"train_lora","text":"<pre><code>train_lora(config: str | LoraTrainingConfig, data: InstructTrainingData | None = None) -&gt; Trainer\n</code></pre> <p>Train a LoRA model on instruction text sets.</p> <p>Parameters:</p> Name Type Description Default <code>str | LoraTrainingConfig</code> <p>The configuration name or the configuration itself</p> required <code>InstructTrainingData | None</code> <p>The training data to train on, if None, the input_glob from the config is used</p> <code>None</code> <p>Returns:</p> Type Description <code>Trainer</code> <p>The trainer instance that is used for training</p> Source code in <code>gerd/training/instruct.py</code> <pre><code>def train_lora(\n    config: str | LoraTrainingConfig, data: InstructTrainingData | None = None\n) -&gt; Trainer:\n    \"\"\"Train a LoRA model on instruction text sets.\n\n    Parameters:\n        config: The configuration name or the configuration itself\n        data: The training data to train on, if None,\n            the input_glob from the config is used\n\n    Returns:\n        The trainer instance that is used for training\n    \"\"\"\n    # Disable parallelism to avoid issues with transformers\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n    lora_config = load_training_config(config) if isinstance(config, str) else config\n\n    if Path(lora_config.output_dir).joinpath(\"adapter_model.safetensors\").exists():\n        if lora_config.override_existing:\n            # check that we do not delete anything vital\n            if lora_config.output_dir == Path(\"/\"):\n                msg = \"Cannot delete root directory.\"\n                raise RuntimeError(msg)\n\n            _LOGGER.warning(\n                \"Overriding existing LoRA adapter in %s ...\", lora_config.output_dir\n            )\n            shutil.rmtree(lora_config.output_dir)\n        else:\n            msg = (\n                f\"LoRA target directory {lora_config.output_dir}\"\n                \" must not contain another lora adapter.\"\n            )\n            raise AssertionError(msg)\n\n    _LOGGER.info(\"Tokenizing training data ...\")\n\n    if data is None:\n        data = InstructTrainingData()\n        # currently, we expect the input glob to point to a local directory\n        # in the future, we might want to support other sources\n        glob_pattern = lora_config.input_glob.replace(\"file://\", \"\")\n        for file in Path().glob(glob_pattern):\n            if not file.is_file():\n                msg = \"Can only read files for now.\"\n                raise NotImplementedError(msg)\n            if file.suffix == \".json\":\n                with open(file, \"r\") as f:\n                    data.samples.extend(\n                        InstructTrainingData.model_validate_json(f.read()).samples\n                    )\n            elif file.suffix == \".yml\":\n                with open(file, \"r\") as f:\n                    obj = yaml.safe_load(f)\n                    data.samples.extend(\n                        InstructTrainingData.model_validate(obj).samples\n                    )\n            else:\n                msg = f\"Unsupported file format: {file.suffix}\"\n                raise NotImplementedError(msg)\n\n    train_data = Dataset.from_list(\n        [\n            lora_config.tokenizer(\n                lora_config.tokenizer.apply_chat_template(\n                    sample.messages, tokenize=False\n                )\n            )\n            for sample in data.samples\n        ]\n    )\n    _LOGGER.info(\"Decoding sample data ...\")\n    decoded_entries = []\n    for i in range(min(10, len(train_data))):\n        decoded_text = lora_config.tokenizer.decode(train_data[i][\"input_ids\"])\n        decoded_entries.append({\"value\": decoded_text})\n\n    log_dir = lora_config.output_dir / \"logs\"\n    log_dir.mkdir(exist_ok=True, parents=True)\n    _LOGGER.info(\"Writing sample to %s ...\", log_dir)\n    with open(Path(f\"{log_dir}/train_dataset_sample.json\"), \"w\") as json_file:\n        json.dump(decoded_entries, json_file, indent=4)\n\n    trainer = Trainer(config=lora_config)\n    trainer.setup_training(\n        train_data=train_data, train_template={\"template_type\": \"dataset\"}\n    )\n\n    trainer.train()\n    return trainer\n</code></pre>"},{"location":"reference/gerd/training/instruct/#gerd.training.instruct.train_lora(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/training/instruct/#gerd.training.instruct.train_lora(data)","title":"<code>data</code>","text":""},{"location":"reference/gerd/training/lora/","title":"gerd.training.lora","text":""},{"location":"reference/gerd/training/lora/#gerd.training.lora","title":"gerd.training.lora","text":"<p>Configuration dataclasses for training LoRA models.</p> <p>Classes:</p> Name Description <code>LLMModelProto</code> <p>Protocol for the LoRA model.</p> <code>LoraModules</code> <p>Configuration for the modules to be trained in LoRA models.</p> <code>LoraTrainingConfig</code> <p>Configuration for training LoRA models.</p> <code>TrainingFlags</code> <p>Training flags for LoRA models.</p> <p>Functions:</p> Name Description <code>load_training_config</code> <p>Load the LLM model configuration.</p>"},{"location":"reference/gerd/training/lora/#gerd.training.lora.LLMModelProto","title":"LLMModelProto","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for the LoRA model.</p> <p>A model model needs to implement the named_modules method for it to be used in LoRA Training.</p> <p>Methods:</p> Name Description <code>named_modules</code> <p>Get the named modules of the model.</p>"},{"location":"reference/gerd/training/lora/#gerd.training.lora.LLMModelProto.named_modules","title":"named_modules","text":"<pre><code>named_modules() -&gt; list[tuple[str, torch.nn.Module]]\n</code></pre> <p>Get the named modules of the model.</p> <p>Returns:</p> Type Description <code>list[tuple[str, Module]]</code> <p>The named modules.</p> Source code in <code>gerd/training/lora.py</code> <pre><code>def named_modules(self) -&gt; list[tuple[str, torch.nn.Module]]:\n    \"\"\"Get the named modules of the model.\n\n    Returns:\n        The named modules.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/gerd/training/lora/#gerd.training.lora.LoraModules","title":"LoraModules","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the modules to be trained in LoRA models.</p> <p>Methods:</p> Name Description <code>target_modules</code> <p>Get the target modules for the given model.</p>"},{"location":"reference/gerd/training/lora/#gerd.training.lora.LoraModules.target_modules","title":"target_modules","text":"<pre><code>target_modules(model: LLMModelProto) -&gt; List[str]\n</code></pre> <p>Get the target modules for the given model.</p> <p>Parameters:</p> Name Type Description Default <code>LLMModelProto</code> <p>The model to be trained.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>The list of target modules</p> Source code in <code>gerd/training/lora.py</code> <pre><code>def target_modules(self, model: LLMModelProto) -&gt; List[str]:\n    \"\"\"Get the target modules for the given model.\n\n    Parameters:\n        model: The model to be trained.\n\n    Returns:\n        The list of target modules\n    \"\"\"\n    avail = _find_target_modules(model)\n    return [\n        f\"{name}_proj\"\n        for name, enabled in self.model_dump().items()\n        if (enabled is True or (enabled is None and self.default is True))\n        and f\"{name}_proj\" in avail\n    ]\n</code></pre>"},{"location":"reference/gerd/training/lora/#gerd.training.lora.LoraModules.target_modules(model)","title":"<code>model</code>","text":""},{"location":"reference/gerd/training/lora/#gerd.training.lora.LoraTrainingConfig","title":"LoraTrainingConfig","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration for training LoRA models.</p> <p>Methods:</p> Name Description <code>model_post_init</code> <p>Post-initialization hook for the model.</p> <code>reset_tokenizer</code> <p>Resets the tokenizer.</p> <code>settings_customise_sources</code> <p>Customize the settings sources used by pydantic-settings.</p> <p>Attributes:</p> Name Type Description <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>Get the tokenizer for the model.</p>"},{"location":"reference/gerd/training/lora/#gerd.training.lora.LoraTrainingConfig.tokenizer","title":"tokenizer  <code>property</code>","text":"<pre><code>tokenizer: PreTrainedTokenizer\n</code></pre> <p>Get the tokenizer for the model.</p>"},{"location":"reference/gerd/training/lora/#gerd.training.lora.LoraTrainingConfig.model_post_init","title":"model_post_init","text":"<pre><code>model_post_init(_: Any) -&gt; None\n</code></pre> <p>Post-initialization hook for the model.</p> <p>This method currently checks whether cutoff is larger than overlap.</p> Source code in <code>gerd/training/lora.py</code> <pre><code>def model_post_init(self, _: Any) -&gt; None:  # noqa: ANN401\n    \"\"\"Post-initialization hook for the model.\n\n    This method currently checks whether cutoff is larger than overlap.\n    \"\"\"\n    if self.cutoff_len &lt;= self.overlap_len:\n        msg = (\n            \"Overlap must be smaller than cutoff\"\n            f\"({self.cutoff_len}) but is {self.overlap_len}\"\n        )\n        raise ValueError(msg)\n    self._tokenizer = None\n</code></pre>"},{"location":"reference/gerd/training/lora/#gerd.training.lora.LoraTrainingConfig.reset_tokenizer","title":"reset_tokenizer","text":"<pre><code>reset_tokenizer() -&gt; None\n</code></pre> <p>Resets the tokenizer.</p> <p>When a tokenizer has been used it needs to be reset before changig parameters to avoid issues with parallelism.</p> Source code in <code>gerd/training/lora.py</code> <pre><code>def reset_tokenizer(self) -&gt; None:\n    \"\"\"Resets the tokenizer.\n\n    When a tokenizer has been used it needs to be reset\n    before changig parameters to avoid issues with parallelism.\n    \"\"\"\n    self._tokenizer = transformers.AutoTokenizer.from_pretrained(\n        self.model.name, trust_remote_code=False, use_fast=True\n    )\n    if self._tokenizer.pad_token_id is None:\n        self._tokenizer.pad_token_id = self.pad_token_id\n    if self.padding_side:\n        self._tokenizer.padding_side = self.padding_side\n</code></pre>"},{"location":"reference/gerd/training/lora/#gerd.training.lora.LoraTrainingConfig.settings_customise_sources","title":"settings_customise_sources  <code>classmethod</code>","text":"<pre><code>settings_customise_sources(_: Type[BaseSettings], init_settings: PydanticBaseSettingsSource, env_settings: PydanticBaseSettingsSource, dotenv_settings: PydanticBaseSettingsSource, file_secret_settings: PydanticBaseSettingsSource) -&gt; tuple[PydanticBaseSettingsSource, ...]\n</code></pre> <p>Customize the settings sources used by pydantic-settings.</p> <p>The order of the sources is important. The first source has the highest priority.</p> <p>Parameters:</p> Name Type Description Default <p>The class of the settings.</p> required <code>PydanticBaseSettingsSource</code> <p>The settings from the initialization.</p> required <code>PydanticBaseSettingsSource</code> <p>The settings from the environment.</p> required <code>PydanticBaseSettingsSource</code> <p>The settings from the dotenv file.</p> required <code>PydanticBaseSettingsSource</code> <p>The settings from the secret file.</p> required <p>Returns:</p> Type Description <code>tuple[PydanticBaseSettingsSource, ...]</code> <p>The customized settings sources.</p> Source code in <code>gerd/training/lora.py</code> <pre><code>@classmethod\ndef settings_customise_sources(\n    cls,\n    _: Type[BaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,\n    file_secret_settings: PydanticBaseSettingsSource,\n) -&gt; tuple[PydanticBaseSettingsSource, ...]:\n    \"\"\"Customize the settings sources used by pydantic-settings.\n\n    The order of the sources is important.\n    The first source has the highest priority.\n\n    Parameters:\n        cls: The class of the settings.\n        init_settings: The settings from the initialization.\n        env_settings: The settings from the environment.\n        dotenv_settings: The settings from the dotenv file.\n        file_secret_settings: The settings from the secret file.\n\n    Returns:\n        The customized settings sources.\n    \"\"\"\n    return (\n        file_secret_settings,\n        env_settings,\n        dotenv_settings,\n        init_settings,\n    )\n</code></pre>"},{"location":"reference/gerd/training/lora/#gerd.training.lora.LoraTrainingConfig.settings_customise_sources(cls)","title":"<code>cls</code>","text":""},{"location":"reference/gerd/training/lora/#gerd.training.lora.LoraTrainingConfig.settings_customise_sources(init_settings)","title":"<code>init_settings</code>","text":""},{"location":"reference/gerd/training/lora/#gerd.training.lora.LoraTrainingConfig.settings_customise_sources(env_settings)","title":"<code>env_settings</code>","text":""},{"location":"reference/gerd/training/lora/#gerd.training.lora.LoraTrainingConfig.settings_customise_sources(dotenv_settings)","title":"<code>dotenv_settings</code>","text":""},{"location":"reference/gerd/training/lora/#gerd.training.lora.LoraTrainingConfig.settings_customise_sources(file_secret_settings)","title":"<code>file_secret_settings</code>","text":""},{"location":"reference/gerd/training/lora/#gerd.training.lora.TrainingFlags","title":"TrainingFlags","text":"<p>               Bases: <code>BaseModel</code></p> <p>Training flags for LoRA models.</p>"},{"location":"reference/gerd/training/lora/#gerd.training.lora.load_training_config","title":"load_training_config","text":"<pre><code>load_training_config(config: str) -&gt; LoraTrainingConfig\n</code></pre> <p>Load the LLM model configuration.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the configuration.</p> required <p>Returns:</p> Type Description <code>LoraTrainingConfig</code> <p>The model configuration.</p> Source code in <code>gerd/training/lora.py</code> <pre><code>def load_training_config(config: str) -&gt; LoraTrainingConfig:\n    \"\"\"Load the LLM model configuration.\n\n    Parameters:\n        config: The name of the configuration.\n\n    Returns:\n        The model configuration.\n    \"\"\"\n    config_path = (\n        Path(config)\n        if config.endswith(\"yml\")\n        else Path(PROJECT_DIR, \"config\", f\"{config}.yml\")\n    )\n    with config_path.open(\"r\", encoding=\"utf-8\") as f:\n        conf = LoraTrainingConfig.model_validate(safe_load(f))\n    return conf\n</code></pre>"},{"location":"reference/gerd/training/lora/#gerd.training.lora.load_training_config(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/training/trainer/","title":"gerd.training.trainer","text":""},{"location":"reference/gerd/training/trainer/#gerd.training.trainer","title":"gerd.training.trainer","text":"<p>Training module for LoRA models.</p> <p>Can be used to train LoRA models on structured or unstructured data.</p> <p>Classes:</p> Name Description <code>Callbacks</code> <p>Custom callbacks for the LoRA training.</p> <code>Tracked</code> <p>Dataclass to track the training progress.</p> <code>Trainer</code> <p>The LoRA trainer class.</p>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Callbacks","title":"Callbacks","text":"<pre><code>Callbacks(tracked: Tracked)\n</code></pre> <p>               Bases: <code>TrainerCallback</code></p> <p>Custom callbacks for the LoRA training.</p> <p>Initialize the callbacks based on tracking data config.</p> <p>Parameters:</p> Name Type Description Default <code>Tracked</code> <p>The tracking data</p> required <p>Methods:</p> Name Description <code>on_log</code> <p>Callback to log the training progress.</p> <code>on_save</code> <p>Saves the training log when the model is saved.</p> <code>on_step_begin</code> <p>Update the training progress.</p> <code>on_substep_end</code> <p>Update the training progress and check for interruption.</p> Source code in <code>gerd/training/trainer.py</code> <pre><code>def __init__(self, tracked: Tracked) -&gt; None:\n    \"\"\"Initialize the callbacks based on tracking data config.\n\n    Parameters:\n        tracked: The tracking data\n    \"\"\"\n    super().__init__()\n    self.tracked = tracked\n    self.gradient_accumulation_steps = (\n        tracked.config.batch_size // tracked.config.micro_batch_size\n    )\n    self.actual_save_steps = math.ceil(\n        tracked.config.save_steps / self.gradient_accumulation_steps\n    )\n</code></pre>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Callbacks(tracked)","title":"<code>tracked</code>","text":""},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Callbacks.on_log","title":"on_log","text":"<pre><code>on_log(_args: transformers.TrainingArguments, _state: transformers.TrainerState, control: transformers.TrainerControl, logs: Dict, **kwargs: int) -&gt; None\n</code></pre> <p>Callback to log the training progress.</p> <p>Parameters:</p> Name Type Description Default <code>TrainingArguments</code> <p>The training arguments (not used)</p> required <code>TrainerState</code> <p>The trainer state (not used)</p> required <code>TrainerControl</code> <p>The trainer control</p> required <code>Dict</code> <p>The training logs</p> required Source code in <code>gerd/training/trainer.py</code> <pre><code>def on_log(\n    self,\n    _args: transformers.TrainingArguments,  # noqa: ARG002\n    _state: transformers.TrainerState,  # noqa: ARG002\n    control: transformers.TrainerControl,\n    logs: Dict,\n    **kwargs: int,  # noqa: ARG002\n) -&gt; None:\n    \"\"\"Callback to log the training progress.\n\n    Parameters:\n        _args: The training arguments (not used)\n        _state: The trainer state (not used)\n        control: The trainer control\n        logs: The training logs\n    \"\"\"\n    self.tracked.train_log.update(logs)\n    self.tracked.train_log.update({\"current_steps\": self.tracked.current_steps})\n    if self.tracked.interrupted:\n        _LOGGER.info(\"Interrupted by user\")\n\n    # print(f\"Step: {self.tracked.current_steps}\", end=\"\")\n    if \"loss\" in logs:\n        loss = float(logs[\"loss\"])\n        if loss &lt;= self.tracked.config.stop_at_loss:\n            control.should_epoch_stop = True\n            control.should_training_stop = True\n            _LOGGER.info(\"Stop Loss %f reached.\", self.tracked.config.stop_at_loss)\n</code></pre>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Callbacks.on_log(_args)","title":"<code>_args</code>","text":""},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Callbacks.on_log(_state)","title":"<code>_state</code>","text":""},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Callbacks.on_log(control)","title":"<code>control</code>","text":""},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Callbacks.on_log(logs)","title":"<code>logs</code>","text":""},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Callbacks.on_save","title":"on_save","text":"<pre><code>on_save(_args: transformers.TrainingArguments, _state: transformers.TrainerState, _control: transformers.TrainerControl, **kwargs: int) -&gt; None\n</code></pre> <p>Saves the training log when the model is saved.</p> Source code in <code>gerd/training/trainer.py</code> <pre><code>def on_save(\n    self,\n    _args: transformers.TrainingArguments,  # noqa: ARG002\n    _state: transformers.TrainerState,  # noqa: ARG002\n    _control: transformers.TrainerControl,  # noqa: ARG002\n    **kwargs: int,  # noqa: ARG002\n) -&gt; None:\n    \"\"\"Saves the training log when the model is saved.\"\"\"\n    # Save log\n    with open(\n        f\"{self.tracked.config.output_dir}/{self.tracked.current_steps}-training_log.json\",\n        \"w\",\n        encoding=\"utf-8\",\n    ) as file:\n        json.dump(self.tracked.train_log, file, indent=2)\n</code></pre>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Callbacks.on_step_begin","title":"on_step_begin","text":"<pre><code>on_step_begin(_args: transformers.TrainingArguments, state: transformers.TrainerState, control: transformers.TrainerControl, **kwargs: int) -&gt; None\n</code></pre> <p>Update the training progress.</p> <p>This callback updates the current training steps and checks if the training was interrupted.</p> <p>Parameters:</p> Name Type Description Default <code>TrainingArguments</code> <p>The training arguments (not used)</p> required <code>TrainerState</code> <p>The trainer state</p> required <code>TrainerControl</code> <p>The trainer control</p> required Source code in <code>gerd/training/trainer.py</code> <pre><code>def on_step_begin(\n    self,\n    _args: transformers.TrainingArguments,  # noqa: ARG002\n    state: transformers.TrainerState,\n    control: transformers.TrainerControl,\n    **kwargs: int,  # noqa: ARG002\n) -&gt; None:\n    \"\"\"Update the training progress.\n\n    This callback updates the current training steps and\n    checks if the training was interrupted.\n\n    Parameters:\n        _args: The training arguments (not used)\n        state: The trainer state\n        control: The trainer control\n    \"\"\"\n    self.tracked.current_steps = (\n        state.global_step * self.gradient_accumulation_steps\n    )\n    self.tracked.max_steps = state.max_steps * self.gradient_accumulation_steps\n    if self.tracked.interrupted:\n        control.should_epoch_stop = True\n        control.should_training_stop = True\n</code></pre>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Callbacks.on_step_begin(_args)","title":"<code>_args</code>","text":""},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Callbacks.on_step_begin(state)","title":"<code>state</code>","text":""},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Callbacks.on_step_begin(control)","title":"<code>control</code>","text":""},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Callbacks.on_substep_end","title":"on_substep_end","text":"<pre><code>on_substep_end(_args: transformers.TrainingArguments, _state: transformers.TrainerState, control: transformers.TrainerControl, **kwargs: int) -&gt; None\n</code></pre> <p>Update the training progress and check for interruption.</p> <p>Parameters:</p> Name Type Description Default <code>TrainingArguments</code> <p>The training arguments (not used)</p> required <code>TrainerState</code> <p>The trainer state (not used)</p> required <code>TrainerControl</code> <p>The trainer control</p> required Source code in <code>gerd/training/trainer.py</code> <pre><code>def on_substep_end(\n    self,\n    _args: transformers.TrainingArguments,  # noqa: ARG002\n    _state: transformers.TrainerState,  # noqa: ARG002\n    control: transformers.TrainerControl,\n    **kwargs: int,  # noqa: ARG002\n) -&gt; None:\n    \"\"\"Update the training progress and check for interruption.\n\n    Parameters:\n        _args: The training arguments (not used)\n        _state: The trainer state (not used)\n        control: The trainer control\n    \"\"\"\n    self.tracked.current_steps += 1\n    if self.tracked.interrupted:\n        control.should_epoch_stop = True\n        control.should_training_stop = True\n</code></pre>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Callbacks.on_substep_end(_args)","title":"<code>_args</code>","text":""},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Callbacks.on_substep_end(_state)","title":"<code>_state</code>","text":""},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Callbacks.on_substep_end(control)","title":"<code>control</code>","text":""},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Tracked","title":"Tracked  <code>dataclass</code>","text":"<pre><code>Tracked(lora_model: PeftModel, config: LoraTrainingConfig, train_log: Dict = dict(), current_steps: int = 0, interrupted: bool = False, max_steps: int = 0, did_save: bool = False)\n</code></pre> <p>Dataclass to track the training progress.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>LoraTrainingConfig</code> <p>The training configuration.</p> <code>current_steps</code> <code>int</code> <p>The current training steps.</p> <code>did_save</code> <code>bool</code> <p>Whether the model was saved.</p> <code>interrupted</code> <code>bool</code> <p>Whether the training was interrupted.</p> <code>lora_model</code> <code>PeftModel</code> <p>The training model.</p> <code>max_steps</code> <code>int</code> <p>The maximum number of training steps.</p> <code>train_log</code> <code>Dict</code> <p>The training log.</p>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Tracked.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config: LoraTrainingConfig\n</code></pre> <p>The training configuration.</p>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Tracked.current_steps","title":"current_steps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>current_steps: int = 0\n</code></pre> <p>The current training steps.</p>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Tracked.did_save","title":"did_save  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>did_save: bool = False\n</code></pre> <p>Whether the model was saved.</p>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Tracked.interrupted","title":"interrupted  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>interrupted: bool = False\n</code></pre> <p>Whether the training was interrupted.</p>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Tracked.lora_model","title":"lora_model  <code>instance-attribute</code>","text":"<pre><code>lora_model: PeftModel\n</code></pre> <p>The training model.</p>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Tracked.max_steps","title":"max_steps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_steps: int = 0\n</code></pre> <p>The maximum number of training steps.</p>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Tracked.train_log","title":"train_log  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>train_log: Dict = field(default_factory=dict)\n</code></pre> <p>The training log.</p>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Trainer","title":"Trainer","text":"<pre><code>Trainer(config: LoraTrainingConfig, callback_cls: Optional[List[Type[transformers.TrainerCallback]]] = None)\n</code></pre> <p>The LoRA trainer class.</p> <p>This class is used to train LoRA models on structured or unstructured data. Since the training process is asynchronous, the trainer can be used to track or interrupt the training process.</p> <p>The LoRa traininer requires a configuration and optional list of callbacks.</p> <p>If no callbacks are provided, the default Callbacks class <code>Tracked</code> is used. Parameters:     config: The training configuration     callback_cls: The list of callbacks</p> <p>Methods:</p> Name Description <code>interrupt</code> <p>Interrupt the training process.</p> <code>save</code> <p>Save the model and log files to the path set in the trainer configuration.</p> <code>setup_training</code> <p>Setup the training process and initialize the transformer trainer.</p> <code>train</code> <p>Start the training process.</p> Source code in <code>gerd/training/trainer.py</code> <pre><code>def __init__(\n    self,\n    config: LoraTrainingConfig,\n    callback_cls: Optional[List[Type[transformers.TrainerCallback]]] = None,\n) -&gt; None:\n    \"\"\"The LoRa traininer requires a configuration and optional list of callbacks.\n\n    If no callbacks are provided,\n    the default Callbacks class [`Tracked`][gerd.training.trainer.Tracked] is used.\n    Parameters:\n        config: The training configuration\n        callback_cls: The list of callbacks\n    \"\"\"\n    self.config = config\n    self.base_model: transformers.PreTrainedModel = (\n        transformers.AutoModelForCausalLM.from_pretrained(\n            config.model.name,\n            load_in_4bit=config.flags.use_4bit,\n            load_in_8bit=config.flags.use_8bit,\n        )\n    )\n\n    training_config = LoraConfig(\n        lora_alpha=config.lora_alpha,\n        target_modules=config.modules.target_modules(self.base_model),\n        lora_dropout=config.lora_dropout,\n        bias=config.bias,\n        task_type=config.task_type,\n        r=config.r,\n    )\n\n    for param in self.base_model.parameters():\n        if param.requires_grad:\n            param.data = param.data.float()\n    if (\n        not hasattr(self.base_model, \"lm_head\")\n        or hasattr(self.base_model.lm_head, \"weight\")\n    ) and \"quantization_config\" in self.base_model.config.to_dict():\n        self.base_model = prepare_model_for_kbit_training(self.base_model)\n        _LOGGER.info(\"Quantization detected!\")\n\n    self.lora_model = get_peft_model(self.base_model, training_config)\n    if not hasattr(self.lora_model.config, \"use_cache\"):\n        msg = \"LoRA model config not have 'use_cache' attribute\"\n        raise AssertionError(msg)\n    self.lora_model.config.use_cache = False\n\n    self.tracked = Tracked(self.lora_model, self.config)\n    self.trainer = None\n    self.callbacks = (\n        [cls(self.tracked) for cls in callback_cls]\n        if callback_cls is not None\n        else [Callbacks(self.tracked)]\n    )\n\n    gradient_accumulation_steps = config.batch_size // config.micro_batch_size\n    actual_lr = float(config.learning_rate)\n\n    self.args = transformers.TrainingArguments(\n        per_device_train_batch_size=self.config.micro_batch_size,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        warmup_steps=math.ceil(config.warmup_steps / gradient_accumulation_steps),\n        num_train_epochs=config.epochs,\n        learning_rate=actual_lr,\n        fp16=not (config.flags.use_cpu or config.flags.use_bf16),\n        bf16=config.flags.use_bf16,\n        optim=config.optimizer,\n        logging_steps=2 if config.stop_at_loss &gt; 0 else 5,\n        eval_strategy=\"no\",\n        eval_steps=None,\n        save_strategy=\"no\",\n        output_dir=config.output_dir,\n        lr_scheduler_type=config.lr_scheduler,\n        load_best_model_at_end=False,\n        # TODO: Enable multi-device support\n        ddp_find_unused_parameters=None,\n        use_ipex=config.flags.use_ipex,\n        save_steps=config.save_steps,\n        use_cpu=config.flags.use_cpu,\n        # any of these two will set `torch_compile=True`\n        # torch_compile_backend=\"inductor\",\n        # torch_compile_mode=\"reduce-overhead\",\n    )\n</code></pre>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Trainer.interrupt","title":"interrupt","text":"<pre><code>interrupt() -&gt; None\n</code></pre> <p>Interrupt the training process.</p> Source code in <code>gerd/training/trainer.py</code> <pre><code>def interrupt(self) -&gt; None:\n    \"\"\"Interrupt the training process.\"\"\"\n    self.tracked.interrupted = True\n</code></pre>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Trainer.save","title":"save","text":"<pre><code>save() -&gt; None\n</code></pre> <p>Save the model and log files to the path set in the trainer configuration.</p> <p>When the <code>zip_output</code> flag is set, the output directory is zipped as well.</p> Source code in <code>gerd/training/trainer.py</code> <pre><code>def save(self) -&gt; None:\n    \"\"\"Save the model and log files to the path set in the trainer configuration.\n\n    When the `zip_output` flag is set, the output directory is zipped as well.\n    \"\"\"\n    if self.trainer is not None:\n        self.trainer.save_model(self.config.output_dir)\n        if self.config.zip_output:\n            shutil.make_archive(\n                base_name=self.config.output_dir.as_posix(),\n                root_dir=self.config.output_dir,\n                # base_dir=self.config.output_dir,\n                format=\"zip\",\n            )\n    else:\n        _LOGGER.warning(\"Trainer not initialized\")\n</code></pre>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Trainer.setup_training","title":"setup_training","text":"<pre><code>setup_training(train_data: Dataset, train_template: Dict, torch_compile: bool = False) -&gt; None\n</code></pre> <p>Setup the training process and initialize the transformer trainer.</p> <p>Parameters:</p> Name Type Description Default <code>Dataset</code> <p>The training data</p> required <code>Dict</code> <p>The training template</p> required <code>bool</code> <p>Whether to use torch compile</p> <code>False</code> Source code in <code>gerd/training/trainer.py</code> <pre><code>def setup_training(\n    self,\n    train_data: Dataset,\n    train_template: Dict,\n    torch_compile: bool = False,\n) -&gt; None:\n    \"\"\"Setup the training process and initialize the transformer trainer.\n\n    Parameters:\n        train_data: The training data\n        train_template: The training template\n        torch_compile: Whether to use torch compile\n    \"\"\"\n    _LOGGER.debug(\"Training parameter\\n============\\n %s\", self.args)\n    self.trainer = transformers.Trainer(\n        model=self.lora_model,\n        train_dataset=train_data,\n        eval_dataset=None,\n        args=self.args,\n        data_collator=transformers.DataCollatorForLanguageModeling(\n            self.config.tokenizer, mlm=False\n        ),\n        callbacks=self.callbacks,\n    )\n\n    # This must be done after Trainer init because otherwise\n    # the trainer cannot identify the relevant (as in 'trainable') parameters\n    # and will remove required information from the training data set.\n    # Whether it is useful to compile after reassignment.\n    if torch_compile and torch.__version__ &gt;= \"2\" and sys.platform != \"win32\":\n        self.lora_model = torch.compile(self.lora_model)  # type: ignore[assignment]\n\n    # == Save parameters for reuse ==\n    with open(\n        f\"{self.config.output_dir}/training_parameters.json\", \"w\", encoding=\"utf-8\"\n    ) as file:\n        file.write(self.config.model_dump_json(indent=2))\n\n    # == Save training prompt ==\n    with open(\n        f\"{self.config.output_dir}/training_prompt.json\", \"w\", encoding=\"utf-8\"\n    ) as file:\n        json.dump(train_template, file, indent=2)\n</code></pre>"},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Trainer.setup_training(train_data)","title":"<code>train_data</code>","text":""},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Trainer.setup_training(train_template)","title":"<code>train_template</code>","text":""},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Trainer.setup_training(torch_compile)","title":"<code>torch_compile</code>","text":""},{"location":"reference/gerd/training/trainer/#gerd.training.trainer.Trainer.train","title":"train","text":"<pre><code>train() -&gt; threading.Thread\n</code></pre> <p>Start the training process.</p> <p>Returns:</p> Type Description <code>Thread</code> <p>The training thread</p> Source code in <code>gerd/training/trainer.py</code> <pre><code>def train(self) -&gt; threading.Thread:\n    \"\"\"Start the training process.\n\n    Returns:\n        The training thread\n    \"\"\"\n    model_type = type(self.base_model).__name__\n    model_id = MODEL_CLASSES[model_type]\n\n    projections_string = \", \".join(\n        [\n            projection.replace(\"_proj\", \"\")\n            for projection in self.config.modules.target_modules(self.base_model)\n        ]\n    )\n\n    _LOGGER.info(\n        \"Training '%s' model using (%s) projections\", model_id, projections_string\n    )\n    # == Main run and monitor loop ==\n\n    log = self.tracked.train_log\n    log.update({\"base_model_config\": self.config.model.name})\n    log.update({\"base_model_class\": self.base_model.__class__.__name__})\n    log.update(\n        {\n            \"base_loaded_in_4bit\": getattr(\n                self.lora_model, \"is_loaded_in_4bit\", False\n            )\n        }\n    )\n    log.update(\n        {\n            \"base_loaded_in_8bit\": getattr(\n                self.lora_model, \"is_loaded_in_8bit\", False\n            )\n        }\n    )\n    log.update({\"projections\": projections_string})\n\n    if self.config.stop_at_loss &gt; 0:\n        _LOGGER.info(\n            \"Monitoring loss \\033[1;31;1m(Auto-Stop at: %f)\\033[0;37;0m\",\n            self.config.stop_at_loss,\n        )\n\n    def threaded_run() -&gt; None:\n        if self.trainer is None:\n            msg = \"Trainer not initialized\"\n            raise RuntimeError(msg)\n        self.trainer.train()\n        # Note: save in the thread in case the gradio thread breaks\n        # (eg browser closed)\n        self.save()\n        self.tracked.did_save = True\n        # Save log\n        with open(\n            f\"{self.config.output_dir}/training_log.json\", \"w\", encoding=\"utf-8\"\n        ) as file:\n            json.dump(self.tracked.train_log, file, indent=2)\n\n    self.thread = threading.Thread(target=threaded_run)\n    self.thread.start()\n    return self.thread\n</code></pre>"},{"location":"reference/gerd/training/unstructured/","title":"gerd.training.unstructured","text":""},{"location":"reference/gerd/training/unstructured/#gerd.training.unstructured","title":"gerd.training.unstructured","text":"<p>Training of LoRA models on unstructured text data.</p> <p>This module provides functions to train LoRA models to 'imitate' the style of a given text corpus.</p> <p>Functions:</p> Name Description <code>train_lora</code> <p>Train a LoRA model on unstructured text data.</p>"},{"location":"reference/gerd/training/unstructured/#gerd.training.unstructured.train_lora","title":"train_lora","text":"<pre><code>train_lora(config: str | LoraTrainingConfig, texts: list[str] | None = None) -&gt; Trainer\n</code></pre> <p>Train a LoRA model on unstructured text data.</p> <p>Parameters:</p> Name Type Description Default <code>str | LoraTrainingConfig</code> <p>The configuration name or the configuration itself</p> required <code>list[str] | None</code> <p>The list of texts to train on, if None, the input_glob from the config is used</p> <code>None</code> <p>Returns:</p> Type Description <code>Trainer</code> <p>The trainer instance that is used for training</p> Source code in <code>gerd/training/unstructured.py</code> <pre><code>def train_lora(\n    config: str | LoraTrainingConfig, texts: list[str] | None = None\n) -&gt; Trainer:\n    \"\"\"Train a LoRA model on unstructured text data.\n\n    Parameters:\n        config: The configuration name or the configuration itself\n        texts: The list of texts to train on, if None,\n            the input_glob from the config is used\n\n    Returns:\n        The trainer instance that is used for training\n    \"\"\"\n    # Disable parallelism to avoid issues with transformers\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n    lora_config = load_training_config(config) if isinstance(config, str) else config\n\n    if Path(lora_config.output_dir).joinpath(\"adapter_model.safetensors\").exists():\n        if lora_config.override_existing:\n            # check that we do not delete anything vital\n            if lora_config.output_dir == Path(\"/\"):\n                msg = \"Cannot delete root directory.\"\n                raise RuntimeError(msg)\n\n            _LOGGER.warning(\n                \"Overriding existing LoRA adapter in %s ...\", lora_config.output_dir\n            )\n            shutil.rmtree(lora_config.output_dir)\n        else:\n            msg = (\n                f\"LoRA target directory {lora_config.output_dir}\"\n                \" must not contain another lora adapter.\"\n            )\n            raise AssertionError(msg)\n\n    _LOGGER.info(\"Tokenizing training data ...\")\n\n    if not texts:\n        texts = []\n        # currently, we expect the input glob to point to a local directory\n        # in the future, we might want to support other sources\n        glob_pattern = lora_config.input_glob.replace(\"file://\", \"\")\n        for file in Path().glob(glob_pattern):\n            with open(file, \"r\") as f:\n                texts.append(f.read())\n\n    training_tokens: list[list[int]] = []\n    for text in texts:\n        if len(text) == 0:\n            continue\n        training_tokens.extend(\n            split_chunks(\n                lora_config.tokenizer.encode(text),\n                lora_config.cutoff_len,\n                lora_config.cutoff_len - lora_config.overlap_len,\n            )\n        )\n    text_chunks = [lora_config.tokenizer.decode(x) for x in training_tokens]\n    train_data = Dataset.from_list(\n        [\n            tokenize(x, lora_config.tokenizer, lora_config.cutoff_len)\n            for x in text_chunks\n        ]\n    )\n    _LOGGER.info(\"Decoding sample data ...\")\n    decoded_entries = []\n    for i in range(min(10, len(train_data))):\n        decoded_text = lora_config.tokenizer.decode(train_data[i][\"input_ids\"])\n        decoded_entries.append({\"value\": decoded_text})\n\n    log_dir = lora_config.output_dir / \"logs\"\n    log_dir.mkdir(exist_ok=True, parents=True)\n    _LOGGER.info(\"Writing sample to %s ...\", log_dir)\n    with open(Path(f\"{log_dir}/train_dataset_sample.json\"), \"w\") as json_file:\n        json.dump(decoded_entries, json_file, indent=4)\n\n    trainer = Trainer(config=lora_config)\n    trainer.setup_training(\n        train_data=train_data, train_template={\"template_type\": \"raw_text\"}\n    )\n\n    trainer.train()\n    return trainer\n</code></pre>"},{"location":"reference/gerd/training/unstructured/#gerd.training.unstructured.train_lora(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/training/unstructured/#gerd.training.unstructured.train_lora(texts)","title":"<code>texts</code>","text":""},{"location":"reference/gerd/transport/","title":"gerd.transport","text":""},{"location":"reference/gerd/transport/#gerd.transport","title":"gerd.transport","text":"<p>Module to define the transport protocol.</p> <p>The transport protocol is used to connect the backend and frontend services. Implemetations of the transport protocol can be found in the <code>gerd.backends</code> module.</p> <p>Classes:</p> Name Description <code>DocumentSource</code> <p>Dataclass to hold a document source.</p> <code>FileTypes</code> <p>Enum to hold all supported file types.</p> <code>GenResponse</code> <p>Dataclass to hold a response from the generation service.</p> <code>QAAnalyzeAnswer</code> <p>Dataclass to hold an answer from the predefined queries to the QA service.</p> <code>QAAnswer</code> <p>Dataclass to hold an answer from the QA service.</p> <code>QAFileUpload</code> <p>Dataclass to hold a file upload.</p> <code>QAModesEnum</code> <p>Enum to hold all supported QA modes.</p> <code>QAPromptConfig</code> <p>Prompt configuration for the QA service.</p> <code>QAQuestion</code> <p>Dataclass to hold a question for the QA service.</p> <code>Transport</code> <p>Transport protocol to connect backend and frontend services.</p>"},{"location":"reference/gerd/transport/#gerd.transport.DocumentSource","title":"DocumentSource","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dataclass to hold a document source.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str</code> <p>The content of the document.</p> <code>name</code> <code>str</code> <p>The name of the document.</p> <code>page</code> <code>int</code> <p>The page of the document.</p> <code>query</code> <code>str</code> <p>The query that was used to find the document.</p>"},{"location":"reference/gerd/transport/#gerd.transport.DocumentSource.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: str\n</code></pre> <p>The content of the document.</p>"},{"location":"reference/gerd/transport/#gerd.transport.DocumentSource.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the document.</p>"},{"location":"reference/gerd/transport/#gerd.transport.DocumentSource.page","title":"page  <code>instance-attribute</code>","text":"<pre><code>page: int\n</code></pre> <p>The page of the document.</p>"},{"location":"reference/gerd/transport/#gerd.transport.DocumentSource.query","title":"query  <code>instance-attribute</code>","text":"<pre><code>query: str\n</code></pre> <p>The query that was used to find the document.</p>"},{"location":"reference/gerd/transport/#gerd.transport.FileTypes","title":"FileTypes","text":"<p>               Bases: <code>Enum</code></p> <p>Enum to hold all supported file types.</p> <p>Attributes:</p> Name Type Description <code>PDF</code> <p>PDF file type.</p> <code>TEXT</code> <p>Text file type.</p>"},{"location":"reference/gerd/transport/#gerd.transport.FileTypes.PDF","title":"PDF  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PDF = 'pdf'\n</code></pre> <p>PDF file type.</p>"},{"location":"reference/gerd/transport/#gerd.transport.FileTypes.TEXT","title":"TEXT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TEXT = 'txt'\n</code></pre> <p>Text file type.</p>"},{"location":"reference/gerd/transport/#gerd.transport.GenResponse","title":"GenResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dataclass to hold a response from the generation service.</p> <p>Attributes:</p> Name Type Description <code>error_msg</code> <code>str</code> <p>The error message if the status code is not 200.</p> <code>prompt</code> <code>str | None</code> <p>The custom prompt that was used to generate the text.</p> <code>status</code> <code>int</code> <p>The status code of the response.</p> <code>text</code> <code>str</code> <p>The generated text if the status code is 200.</p>"},{"location":"reference/gerd/transport/#gerd.transport.GenResponse.error_msg","title":"error_msg  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>error_msg: str = ''\n</code></pre> <p>The error message if the status code is not 200.</p>"},{"location":"reference/gerd/transport/#gerd.transport.GenResponse.prompt","title":"prompt  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prompt: str | None = None\n</code></pre> <p>The custom prompt that was used to generate the text.</p>"},{"location":"reference/gerd/transport/#gerd.transport.GenResponse.status","title":"status  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status: int = 200\n</code></pre> <p>The status code of the response.</p>"},{"location":"reference/gerd/transport/#gerd.transport.GenResponse.text","title":"text  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>text: str = ''\n</code></pre> <p>The generated text if the status code is 200.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAAnalyzeAnswer","title":"QAAnalyzeAnswer","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dataclass to hold an answer from the predefined queries to the QA service.</p> <p>Attributes:</p> Name Type Description <code>error_msg</code> <code>str</code> <p>The error message of the answer if the status code is not 200.</p> <code>status</code> <code>int</code> <p>The status code of the answer.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAAnalyzeAnswer.error_msg","title":"error_msg  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>error_msg: str = ''\n</code></pre> <p>The error message of the answer if the status code is not 200.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAAnalyzeAnswer.status","title":"status  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status: int = 200\n</code></pre> <p>The status code of the answer.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAAnswer","title":"QAAnswer","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dataclass to hold an answer from the QA service.</p> <p>Attributes:</p> Name Type Description <code>error_msg</code> <code>str</code> <p>The error message of the answer if the status code is not 200.</p> <code>response</code> <code>str</code> <p>The response of the answer.</p> <code>sources</code> <code>List[DocumentSource]</code> <p>The sources of the answer.</p> <code>status</code> <code>int</code> <p>The status code of the answer.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAAnswer.error_msg","title":"error_msg  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>error_msg: str = ''\n</code></pre> <p>The error message of the answer if the status code is not 200.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAAnswer.response","title":"response  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>response: str = ''\n</code></pre> <p>The response of the answer.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAAnswer.sources","title":"sources  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sources: List[DocumentSource] = []\n</code></pre> <p>The sources of the answer.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAAnswer.status","title":"status  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status: int = 200\n</code></pre> <p>The status code of the answer.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAFileUpload","title":"QAFileUpload","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dataclass to hold a file upload.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>bytes</code> <p>The file data.</p> <code>name</code> <code>str</code> <p>The name of the file.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAFileUpload.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: bytes\n</code></pre> <p>The file data.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAFileUpload.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the file.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAModesEnum","title":"QAModesEnum","text":"<p>               Bases: <code>Enum</code></p> <p>Enum to hold all supported QA modes.</p> <p>Attributes:</p> Name Type Description <code>ANALYZE</code> <p>Analyze mode.</p> <code>ANALYZE_MULT_PROMPTS</code> <p>Analyze multiple prompts mode.</p> <code>NONE</code> <p>No mode.</p> <code>SEARCH</code> <p>Search mode.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAModesEnum.ANALYZE","title":"ANALYZE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ANALYZE = 2\n</code></pre> <p>Analyze mode.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAModesEnum.ANALYZE_MULT_PROMPTS","title":"ANALYZE_MULT_PROMPTS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ANALYZE_MULT_PROMPTS = 3\n</code></pre> <p>Analyze multiple prompts mode.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAModesEnum.NONE","title":"NONE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NONE = 0\n</code></pre> <p>No mode.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAModesEnum.SEARCH","title":"SEARCH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SEARCH = 1\n</code></pre> <p>Search mode.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAPromptConfig","title":"QAPromptConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Prompt configuration for the QA service.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>PromptConfig</code> <p>The prompt configuration.</p> <code>mode</code> <code>QAModesEnum</code> <p>The mode to set the prompt configuration for.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAPromptConfig.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config: PromptConfig\n</code></pre> <p>The prompt configuration.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAPromptConfig.mode","title":"mode  <code>instance-attribute</code>","text":"<pre><code>mode: QAModesEnum\n</code></pre> <p>The mode to set the prompt configuration for.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAQuestion","title":"QAQuestion","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dataclass to hold a question for the QA service.</p> <p>Attributes:</p> Name Type Description <code>max_sources</code> <code>int</code> <p>The maximum number of sources to return.</p> <code>question</code> <code>str</code> <p>The question to ask the QA service.</p> <code>search_strategy</code> <code>str</code> <p>The search strategy to use.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAQuestion.max_sources","title":"max_sources  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_sources: int = 3\n</code></pre> <p>The maximum number of sources to return.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAQuestion.question","title":"question  <code>instance-attribute</code>","text":"<pre><code>question: str\n</code></pre> <p>The question to ask the QA service.</p>"},{"location":"reference/gerd/transport/#gerd.transport.QAQuestion.search_strategy","title":"search_strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>search_strategy: str = 'similarity'\n</code></pre> <p>The search strategy to use.</p>"},{"location":"reference/gerd/transport/#gerd.transport.Transport","title":"Transport","text":"<p>               Bases: <code>Protocol</code></p> <p>Transport protocol to connect backend and frontend services.</p> <p>Transport should be implemented by a class that provides the necessary methods to interact with the backend.</p> <p>Methods:</p> Name Description <code>add_file</code> <p>Add a file to the vector store.</p> <code>analyze_mult_prompts_query</code> <p>Queries the vector store with a set of predefined queries.</p> <code>analyze_query</code> <p>Queries the vector store with a predefined query.</p> <code>db_embedding</code> <p>Converts a question to an embedding.</p> <code>db_query</code> <p>Queries the vector store with a question.</p> <code>generate</code> <p>Generates text with the generation service.</p> <code>get_gen_prompt</code> <p>Gets the prompt configuration for the generation service.</p> <code>get_qa_prompt</code> <p>Gets the prompt configuration for a mode of the QA service.</p> <code>qa_query</code> <p>Query the QA service with a question.</p> <code>remove_file</code> <p>Remove a file from the vector store.</p> <code>set_gen_prompt</code> <p>Sets the prompt configuration for the generation service.</p> <code>set_qa_prompt</code> <p>Sets the prompt configuration for the QA service.</p>"},{"location":"reference/gerd/transport/#gerd.transport.Transport.add_file","title":"add_file","text":"<pre><code>add_file(file: QAFileUpload) -&gt; QAAnswer\n</code></pre> <p>Add a file to the vector store.</p> <p>The returned answer has a status code of 200 if the file was added successfully. Parameters:     file: The file to add to the vector store.</p> <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer from the QA service</p> Source code in <code>gerd/transport.py</code> <pre><code>def add_file(self, file: QAFileUpload) -&gt; QAAnswer:\n    \"\"\"Add a file to the vector store.\n\n    The returned answer has a status code of 200 if the file was added successfully.\n    Parameters:\n        file: The file to add to the vector store.\n\n    Returns:\n        The answer from the QA service\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/gerd/transport/#gerd.transport.Transport.analyze_mult_prompts_query","title":"analyze_mult_prompts_query","text":"<pre><code>analyze_mult_prompts_query() -&gt; QAAnalyzeAnswer\n</code></pre> <p>Queries the vector store with a set of predefined queries.</p> <p>In contrast to <code>analyze_query</code>, this method queries the vector store with multiple prompts.</p> <p>Returns:</p> Type Description <code>QAAnalyzeAnswer</code> <p>The answer from the QA service.</p> Source code in <code>gerd/transport.py</code> <pre><code>def analyze_mult_prompts_query(self) -&gt; QAAnalyzeAnswer:\n    \"\"\"Queries the vector store with a set of predefined queries.\n\n    In contrast to [`analyze_query`][gerd.transport.Transport.analyze_query],\n    this method queries the vector store with multiple prompts.\n\n    Returns:\n        The answer from the QA service.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/gerd/transport/#gerd.transport.Transport.analyze_query","title":"analyze_query","text":"<pre><code>analyze_query() -&gt; QAAnalyzeAnswer\n</code></pre> <p>Queries the vector store with a predefined query.</p> <p>The query should return vital information gathered from letters of discharge.</p> <p>Returns:</p> Type Description <code>QAAnalyzeAnswer</code> <p>The answer from the QA service.</p> Source code in <code>gerd/transport.py</code> <pre><code>def analyze_query(self) -&gt; QAAnalyzeAnswer:\n    \"\"\"Queries the vector store with a predefined query.\n\n    The query should return vital information gathered\n    from letters of discharge.\n\n    Returns:\n        The answer from the QA service.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/gerd/transport/#gerd.transport.Transport.db_embedding","title":"db_embedding","text":"<pre><code>db_embedding(question: QAQuestion) -&gt; List[float]\n</code></pre> <p>Converts a question to an embedding.</p> <p>The embedding is defined by the vector store.</p> <p>Parameters:</p> Name Type Description Default <code>QAQuestion</code> <p>The question to convert to an embedding.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>The embedding of the question</p> Source code in <code>gerd/transport.py</code> <pre><code>def db_embedding(self, question: QAQuestion) -&gt; List[float]:\n    \"\"\"Converts a question to an embedding.\n\n    The embedding is defined by the vector store.\n\n    Parameters:\n        question: The question to convert to an embedding.\n\n    Returns:\n        The embedding of the question\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/gerd/transport/#gerd.transport.Transport.db_embedding(question)","title":"<code>question</code>","text":""},{"location":"reference/gerd/transport/#gerd.transport.Transport.db_query","title":"db_query","text":"<pre><code>db_query(question: QAQuestion) -&gt; List[DocumentSource]\n</code></pre> <p>Queries the vector store with a question.</p> <p>Parameters:</p> Name Type Description Default <code>QAQuestion</code> <p>The question to query the vector store with.</p> required <p>Returns:</p> Type Description <code>List[DocumentSource]</code> <p>A list of document sources</p> Source code in <code>gerd/transport.py</code> <pre><code>def db_query(self, question: QAQuestion) -&gt; List[DocumentSource]:\n    \"\"\"Queries the vector store with a question.\n\n    Parameters:\n        question: The question to query the vector store with.\n\n    Returns:\n        A list of document sources\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/gerd/transport/#gerd.transport.Transport.db_query(question)","title":"<code>question</code>","text":""},{"location":"reference/gerd/transport/#gerd.transport.Transport.generate","title":"generate","text":"<pre><code>generate(parameters: Dict[str, str]) -&gt; GenResponse\n</code></pre> <p>Generates text with the generation service.</p> <p>Parameters:</p> Name Type Description Default <code>Dict[str, str]</code> <p>The parameters to generate text with</p> required <p>Returns:</p> Type Description <code>GenResponse</code> <p>The generation result</p> Source code in <code>gerd/transport.py</code> <pre><code>def generate(self, parameters: Dict[str, str]) -&gt; GenResponse:\n    \"\"\"Generates text with the generation service.\n\n    Parameters:\n        parameters: The parameters to generate text with\n\n    Returns:\n        The generation result\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/gerd/transport/#gerd.transport.Transport.generate(parameters)","title":"<code>parameters</code>","text":""},{"location":"reference/gerd/transport/#gerd.transport.Transport.get_gen_prompt","title":"get_gen_prompt","text":"<pre><code>get_gen_prompt() -&gt; PromptConfig\n</code></pre> <p>Gets the prompt configuration for the generation service.</p> <p>Returns:</p> Type Description <code>PromptConfig</code> <p>The current prompt configuration</p> Source code in <code>gerd/transport.py</code> <pre><code>def get_gen_prompt(self) -&gt; PromptConfig:\n    \"\"\"Gets the prompt configuration for the generation service.\n\n    Returns:\n        The current prompt configuration\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/gerd/transport/#gerd.transport.Transport.get_qa_prompt","title":"get_qa_prompt","text":"<pre><code>get_qa_prompt(qa_mode: QAModesEnum) -&gt; PromptConfig\n</code></pre> <p>Gets the prompt configuration for a mode of the QA service.</p> <p>Parameters:</p> Name Type Description Default <code>QAModesEnum</code> <p>The mode to get the prompt configuration for</p> required <p>Returns:</p> Type Description <code>PromptConfig</code> <p>The prompt configuration for the QA service</p> Source code in <code>gerd/transport.py</code> <pre><code>def get_qa_prompt(self, qa_mode: QAModesEnum) -&gt; PromptConfig:\n    \"\"\"Gets the prompt configuration for a mode of the QA service.\n\n    Parameters:\n        qa_mode: The mode to get the prompt configuration for\n\n    Returns:\n        The prompt configuration for the QA service\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/gerd/transport/#gerd.transport.Transport.get_qa_prompt(qa_mode)","title":"<code>qa_mode</code>","text":""},{"location":"reference/gerd/transport/#gerd.transport.Transport.qa_query","title":"qa_query","text":"<pre><code>qa_query(query: QAQuestion) -&gt; QAAnswer\n</code></pre> <p>Query the QA service with a question.</p> <p>Parameters:</p> Name Type Description Default <code>QAQuestion</code> <p>The question to query the QA service with.</p> required <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer from the QA service.</p> Source code in <code>gerd/transport.py</code> <pre><code>def qa_query(self, query: QAQuestion) -&gt; QAAnswer:\n    \"\"\"Query the QA service with a question.\n\n    Parameters:\n        query: The question to query the QA service with.\n\n    Returns:\n       The answer from the QA service.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/gerd/transport/#gerd.transport.Transport.qa_query(query)","title":"<code>query</code>","text":""},{"location":"reference/gerd/transport/#gerd.transport.Transport.remove_file","title":"remove_file","text":"<pre><code>remove_file(file_name: str) -&gt; QAAnswer\n</code></pre> <p>Remove a file from the vector store.</p> <p>The returned answer has a status code of 200 if the file was removed successfully. Parameters:     file_name: The name of the file to remove from the vector store.</p> <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer from the QA service</p> Source code in <code>gerd/transport.py</code> <pre><code>def remove_file(self, file_name: str) -&gt; QAAnswer:\n    \"\"\"Remove a file from the vector store.\n\n    The returned answer has a status code of 200\n    if the file was removed successfully.\n    Parameters:\n        file_name: The name of the file to remove from the vector store.\n\n    Returns:\n        The answer from the QA service\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/gerd/transport/#gerd.transport.Transport.set_gen_prompt","title":"set_gen_prompt","text":"<pre><code>set_gen_prompt(config: PromptConfig) -&gt; PromptConfig\n</code></pre> <p>Sets the prompt configuration for the generation service.</p> <p>The prompt configuration that is returned should in most cases be the same as the one that was set. Parameters:     config: The prompt configuration to set</p> <p>Returns:</p> Type Description <code>PromptConfig</code> <p>The prompt configuration that was set</p> Source code in <code>gerd/transport.py</code> <pre><code>def set_gen_prompt(self, config: PromptConfig) -&gt; PromptConfig:\n    \"\"\"Sets the prompt configuration for the generation service.\n\n    The prompt configuration that is returned should in most cases\n    be the same as the one that was set.\n    Parameters:\n        config: The prompt configuration to set\n\n    Returns:\n        The prompt configuration that was set\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/gerd/transport/#gerd.transport.Transport.set_qa_prompt","title":"set_qa_prompt","text":"<pre><code>set_qa_prompt(config: PromptConfig, qa_mode: QAModesEnum) -&gt; QAAnswer\n</code></pre> <p>Sets the prompt configuration for the QA service.</p> <p>Since the QA service uses multiple prompt configurations, the mode should be specified. For more details, see the documentation of <code>QAService.set_prompt_config</code>.</p> <p>Parameters:</p> Name Type Description Default <code>PromptConfig</code> <p>The prompt configuration to set</p> required <code>QAModesEnum</code> <p>The mode to set the prompt configuration for</p> required <p>Returns:</p> Type Description <code>QAAnswer</code> <p>The answer from the QA service</p> Source code in <code>gerd/transport.py</code> <pre><code>def set_qa_prompt(self, config: PromptConfig, qa_mode: QAModesEnum) -&gt; QAAnswer:\n    \"\"\"Sets the prompt configuration for the QA service.\n\n    Since the QA service uses multiple prompt configurations,\n    the mode should be specified. For more details, see the documentation\n    of [`QAService.set_prompt_config`][gerd.qa.QAService.set_prompt_config].\n\n    Parameters:\n        config: The prompt configuration to set\n        qa_mode: The mode to set the prompt configuration for\n\n    Returns:\n        The answer from the QA service\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/gerd/transport/#gerd.transport.Transport.set_qa_prompt(config)","title":"<code>config</code>","text":""},{"location":"reference/gerd/transport/#gerd.transport.Transport.set_qa_prompt(qa_mode)","title":"<code>qa_mode</code>","text":""}]}